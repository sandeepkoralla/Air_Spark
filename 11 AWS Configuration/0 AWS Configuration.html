<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>AWS Configuration / AWS Configuration - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":2862,"name":"AWS Configuration / AWS Configuration","language":"scala","commands":[{"version":"CommandV1","origId":2863,"guid":"7e367613-f5f8-4c65-84e1-26d6c9b9a85a","subtype":"command","commandType":"auto","position":0.5,"command":"%md #**AWS Configuration**\nThis folder contains a number of helpful guides on how to customize your Databricks Configuration.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"64b74dbd-a1b4-437d-bc14-1d6c3ee1a82b"},{"version":"CommandV1","origId":2864,"guid":"699170de-dbf4-415f-84d0-2361e682c9a9","subtype":"command","commandType":"auto","position":1.0,"command":"%md\n#### Integration with Pre-existing Systems\n\nThis document covers network connectivity to and from different types of applications. This will cover questions related to network connectivity only. Please see respective guides for connecting third party applications such as Tableau, Zoomdata, etc under the Databricks Guide -> External Applications folder.  \nWe will continue to add content to this document as additional use cases come about. \n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"54550a36-1393-4de1-a0bd-d26c174a37ed"},{"version":"CommandV1","origId":2865,"guid":"8be76f2c-cf1d-48d1-b210-28e97fbdee62","subtype":"command","commandType":"auto","position":1.25,"command":"%md\n\n### Configuration Guides\n\n* [Describe My EC2](../11 AWS Configuration/1 Describe My EC2.html) - This shows you how to get information about the EC2 instances on your AWS account.\n* [Environment](../11 AWS Configuration/2 Environment.html) - This document will tell you about your Databricks Environment.\n* [Whitelist IP](../11 AWS Configuration/3 Whitelist IP.html) - Modify your Databricks security groups to grant access to specific IPAddresses.  \n* [Tag AWS Instances](../11 AWS Configuration/4 Tag AWS Instances.html) - Tag your Spark cluster instances to identify them for billing or other purposes.\n* [Init Scripts](../11 AWS Configuration/5 Init Scripts.html) - Create a script that will be run on each machine on your Spark cluster before Spark is brought up.\n* [Elastic IPAddress](../11 AWS Configuration/6 Elastic IPAddress.html) - Assign a static IPAddress to your Spark master.\n* [VPC Peering](../11 AWS Configuration/7 VPC Peering.html) - Set up VPC peering between your Databricks VPC and another VPC.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"8bc155ef-dc96-4e28-b681-5ab668c73611"},{"version":"CommandV1","origId":2866,"guid":"cdb4eb50-c402-4d15-b620-a45c49886e04","subtype":"command","commandType":"auto","position":2.0,"command":"%md \n| Description | Port | Outbound / Inbound | Solution | \n| -------     | :----: | :-----------------: | -------- |\n| Rest Api | 34563 | Inbound | File a support ticket to whitelist the IP address. If you are not running in the default Hybrid model, see additional information below. | \n| Tableau  | 10000 | Inbound | Tableau connects to port 10000 on the Spark Driver of the cluster. Follow the guide under _**Databricks Guide -> External Applications -> Tableau**_. |\n| Looker | 10000 | Inbound | Similar to Tableau access, Looker connects to the Spark Driver on port 10000. Find the installation guide under the _**External Applications**_ folder. | \n| External Kafka | 2181 | Outbound | Spark Streaming Receivers can connect to the Kafka source from any of the executors. Therefore, you need to whitelist all the IP addresses of the executors on the streaming cluster. Code example is provided below. | \n| External Kinesis | -- | Outbound | Similar to the external Kafka solution. | \n| GitHub Enterprise | -- | Outbound | GitHub Enterprise is not yet supported, but is currently on the near term roadmap. | ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"5f96ed48-a50f-4bcc-8da1-384910350716"},{"version":"CommandV1","origId":2867,"guid":"b8c3bee9-085e-46ab-8f77-4b5237a26646","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n#### Open Access to REST API\n\nThe REST API is handled by the web application on port 34563. To open access to a particular IP address, file a support ticket with the following information:  \n- IP Address to Whitelist (Or Range of IP address of your organization in the format X.X.X.X/24) \n- Shard ID, e.g. the URL you use to access the Databricks Environment. \n\nA few customers require that the web application be colocated in the same VPC as the cluster. If this is the case, you can follow the _**Databricks Guide -> AWS Configuration -> Whitelist IP**_ guide to whitelist the IP address and port. \n\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"43d1897f-b9b6-460c-a107-7bf8a1386e70"},{"version":"CommandV1","origId":2868,"guid":"605a3f1d-bf4c-4d2e-8ef7-148d4abae9b1","subtype":"command","commandType":"auto","position":4.0,"command":"%md\n\n### To access the AWS API in Python: Use the Boto library.\n* [Boto Documentation](http://docs.pythonboto.org/en/latest/)\n* [Boto Github Repo](https://github.com/boto/boto)\n\n### To access the AWS API in Scala: Use the AWS SDK for Java.\n* [AWS SDK for Java Home Page](http://aws.amazon.com/sdk-for-java/)\n* [AWS SDK for Java Developer Guide](http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/welcome.html)\n* [AWS SDK for Java API Documentation](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html)\n* [AWS Java SDK Github Repo](https://github.com/aws/aws-sdk-java)","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"9d1fe15a-4a98-4959-8217-a7d5f0f93b79"},{"version":"CommandV1","origId":2869,"guid":"07837dfd-60d4-4d4f-b31a-6c898c476c82","subtype":"command","commandType":"auto","position":6.0,"command":"%md\n#### Determine Public IP Address of Executors\nFor initial development and testing environments, you can whitelist a specific cluster. To whitelist the IP range for your shard, please contact **sales@databricks.com** for further assistance. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"089b6b45-7fc5-4f01-bdc0-bab373d1e245"},{"version":"CommandV1","origId":2870,"guid":"45734b95-f58e-47a7-b98d-c41fe86b3426","subtype":"command","commandType":"auto","position":7.0,"command":"import scala.sys.process._\n// Find the number of executors on the cluster\nval numWorkers = sc.getExecutorStorageStatus.length - 1\n\nval perNodeIPv4 = sc.parallelize(0 to numWorkers).map { _ =>\n  val ip = (\"wget -qO- http://169.254.169.254/latest/meta-data/public-ipv4\".!!).trim\n  (\"IP: \", ip)\n}.collect.distinct\n\nprintln(\"IP Addresses for executors:\")\nfor (i <- perNodeIPv4)\n  println(i)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">IP Addresses for executors: \n(IP: ,54.149.51.199)\n(IP: ,54.149.102.210)\nimport scala.sys.process._\nnumWorkers: Int = 8\nperNodeIPv4: Array[(String, String)] = Array((&quot;IP: &quot;,54.149.51.199), (&quot;IP: &quot;,54.149.102.210))\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:36: error: value ip is not a member of String\n         (&quot;IP: &quot; ip)\n                 ^\n&lt;console&gt;:37: error: value distinct is not a member of Array[Nothing]\n       }.collect.distinct\n                 ^\n</div>","error":null,"startTime":1.445269216524E12,"submitTime":1.445269216542E12,"finishTime":1.445269216941E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"c9a56457-68e4-404a-8881-e316b489edbb"}],"guid":"bed1fd8f-17b0-4f0a-81ed-53e3e910248b","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"a56be0ad-a192-42c3-8ee7-85a75ddfe8be","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>