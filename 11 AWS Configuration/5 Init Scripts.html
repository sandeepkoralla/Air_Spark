<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>AWS Configuration / Init Scripts - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":2915,"name":"AWS Configuration / Init Scripts","language":"python","commands":[{"version":"CommandV1","origId":2916,"guid":"329c62ca-6aef-4868-bcd6-49b62270b1a1","subtype":"command","commandType":"auto","position":1.0,"command":"%md\n\n# **Init Scripts**\nAn init script is a script that is run on each node on a Spark cluster upon startup.  You can use this feature to work around a broad number of issues.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"083813c2-f3a3-4aa5-a697-0067748a9176"},{"version":"CommandV1","origId":2917,"guid":"cffbe817-04c5-4e67-a8ac-9d0144d69f57","subtype":"command","commandType":"auto","position":2.0,"command":"%md\n### The Details\n* Shell scripts that run on every cluster node upon startup - and *before* the Spark Driver/Worker JVM starts\n* These are used for the following types of situations:\n * Modify the JVM System classpath in special cases such as using [JDBC Drivers](https://spark.apache.org/docs/1.3.1/sql-programming-guide.html#troubleshooting)\n * Set system properties and environment variables used by the JVM\n * Modify the default spark conf parameters explicitly\n* These apply to both notebook and job clusters\n* Global account-wide scripts are put here:  **dbfs:/databricks/init/**\n* Cluster-specific scripts are scoped to the cluster and put here:  **dbfs:/databricks/init/CLUSTER_NAME/**\n* Output from all init scripts are generated here: **dbfs:/databricks/init/output/**\n  * The output is placed into a folder with the cluster's name\n  * Global scripts will have the **_script_name_timestamp.log_** and cluster specific logs as **_cluster_name_script_name_timestamp.log_**\n* Any changes to the scripts will require a cluster restart\n* Only create the script once and it will always be picked up\n* Explicitly remove the script to deactivate it upon the next restart\n* Note:  It's best to avoid spaces in your cluster names since they're used in the script and output paths above\n\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"ca3379c6-bb2f-453c-b9ba-9a30d335906a"},{"version":"CommandV1","origId":2918,"guid":"e3505547-069d-4115-bbb6-ee45204c6cdc","subtype":"command","commandType":"auto","position":3.0,"command":"%md \n### Create **dbfs:/databricks/init/** if it doesn't exist","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"07f9f223-d7f9-441a-99bd-ab12a3a5ca4e"},{"version":"CommandV1","origId":2919,"guid":"cd06097d-cbc5-4bf6-9541-04efbb6d3ec3","subtype":"command","commandType":"auto","position":4.0,"command":"dbutils.fs.mkdirs(\"dbfs:/databricks/init/\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>True\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.443207232066E12,"submitTime":1.443207232005E12,"finishTime":1.44320723286E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"2b731343-8f8f-4d66-b41c-a08ea68aee55"},{"version":"CommandV1","origId":2920,"guid":"ab75626f-e378-4519-91f4-1ff9b02a88fd","subtype":"command","commandType":"auto","position":5.0,"command":"%md ### Display the list of existing global init scripts if they exist","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"3b54950a-bc0e-4353-a05e-1ac9dbed9bc8"},{"version":"CommandV1","origId":2921,"guid":"4d52f5e8-78da-4296-b215-bfa7f6baf272","subtype":"command","commandType":"auto","position":6.0,"command":"display(dbutils.fs.ls(\"dbfs:/databricks/init/\"))\n","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/databricks/init/$clusterName/","$clusterName/",0.0],["dbfs:/databricks/init/Cassandra/","Cassandra/",0.0],["dbfs:/databricks/init/CassandraTest/","CassandraTest/",0.0],["dbfs:/databricks/init/CassandraTest2/","CassandraTest2/",0.0],["dbfs:/databricks/init/FreglyStreaming/","FreglyStreaming/",0.0],["dbfs:/databricks/init/LZOTest/","LZOTest/",0.0],["dbfs:/databricks/init/LzoTest/","LzoTest/",0.0],["dbfs:/databricks/init/OLD-SCRIPTS/","OLD-SCRIPTS/",0.0],["dbfs:/databricks/init/Rlgarris/","Rlgarris/",0.0],["dbfs:/databricks/init/Rlgarris_Spot_Shared/","Rlgarris_Spot_Shared/",0.0],["dbfs:/databricks/init/Spot_Cluster_Shared_Rlgarris/","Spot_Cluster_Shared_Rlgarris/",0.0],["dbfs:/databricks/init/VidaTEst/","VidaTEst/",0.0],["dbfs:/databricks/init/YOUR_CLUSTER/","YOUR_CLUSTER/",0.0],["dbfs:/databricks/init/YOUR_CLUSTER_NAME/","YOUR_CLUSTER_NAME/",0.0],["dbfs:/databricks/init/YOUR_CLUSTER_NAME__AVOID_SPACES/","YOUR_CLUSTER_NAME__AVOID_SPACES/",0.0],["dbfs:/databricks/init/YOUR_SPARK_CLUSTER_NAME/","YOUR_SPARK_CLUSTER_NAME/",0.0],["dbfs:/databricks/init/a1/","a1/",0.0],["dbfs:/databricks/init/cluster-a/","cluster-a/",0.0],["dbfs:/databricks/init/creds.sh","creds.sh",157.0],["dbfs:/databricks/init/cust-eng-dev/","cust-eng-dev/",0.0],["dbfs:/databricks/init/delete/","delete/",0.0],["dbfs:/databricks/init/demo/","demo/",0.0],["dbfs:/databricks/init/henry/","henry/",0.0],["dbfs:/databricks/init/hhd/","hhd/",0.0],["dbfs:/databricks/init/kp/","kp/",0.0],["dbfs:/databricks/init/kp2/","kp2/",0.0],["dbfs:/databricks/init/ktp/","ktp/",0.0],["dbfs:/databricks/init/kyle/","kyle/",0.0],["dbfs:/databricks/init/kyleTesting/","kyleTesting/",0.0],["dbfs:/databricks/init/michael/","michael/",0.0],["dbfs:/databricks/init/output/","output/",0.0],["dbfs:/databricks/init/parviz/","parviz/",0.0],["dbfs:/databricks/init/parviz-test/","parviz-test/",0.0],["dbfs:/databricks/init/prakash_test/","prakash_test/",0.0],["dbfs:/databricks/init/raela/","raela/",0.0],["dbfs:/databricks/init/raela-test/","raela-test/",0.0],["dbfs:/databricks/init/test/","test/",0.0],["dbfs:/databricks/init/test2/","test2/",0.0],["dbfs:/databricks/init/tpcds-dataGen/","tpcds-dataGen/",0.0],["dbfs:/databricks/init/tpcds-dataGen-1/","tpcds-dataGen-1/",0.0],["dbfs:/databricks/init/tpcds-dataGen-2/","tpcds-dataGen-2/",0.0],["dbfs:/databricks/init/tpcds-dataGen-3/","tpcds-dataGen-3/",0.0],["dbfs:/databricks/init/vault/","vault/",0.0],["dbfs:/databricks/init/vidatest/","vidatest/",0.0],["dbfs:/databricks/init/yin/","yin/",0.0]],"arguments":{},"schema":[{"type":"string","name":"path"},{"type":"string","name":"name"},{"type":"bigint","name":"size"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":null,"startTime":1.443207240529E12,"submitTime":1.443207240456E12,"finishTime":1.443207243702E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"d07beb62-0524-4223-97cd-043acb9f837a"},{"version":"CommandV1","origId":2922,"guid":"7ea38b6a-f56c-4045-9731-2022b7e1ca64","subtype":"command","commandType":"auto","position":7.0,"command":"%md ### Create a global script to run on all clusters.\n* #### Example: Download the Postresql JDBC driver (used by Amazon Redshift) to the Databricks Cloud System classpath to /databricks/jars","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"8c812e2a-3adf-42af-92ea-7d743e9caa61"},{"version":"CommandV1","origId":2923,"guid":"a7d945ae-c203-4851-8857-3b3086764fdc","subtype":"command","commandType":"auto","position":8.0,"command":"dbutils.fs.put(\"/databricks/init/postgresql-install.sh\",\"\"\"\n#!/bin/bash \nwget --quiet -O /mnt/driver-daemon/jars/postgresql-9.3-1101-jdbc4.jar http://central.maven.org/maven2/org/postgresql/postgresql/9.3-1101-jdbc4/postgresql-9.3-1101-jdbc4.jar\nwget --quiet -O /mnt/jars/driver-daemon/postgresql-9.3-1101-jdbc4.jar http://central.maven.org/maven2/org/postgresql/postgresql/9.3-1101-jdbc4/postgresql-9.3-1101-jdbc4.jar\n\"\"\", True)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Wrote 360 bytes.\n<span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>True\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;true&apos; is not defined","error":"<div class=\"ansiout\">ERROR: An unexpected error occurred while tokenizing input\nThe following traceback may be corrupted or invalid\nThe error message is: (&apos;EOF in multi-line string&apos;, (1, 0))\n\n<span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-3-85eace2338af&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      3</span> wget <span class=\"ansiyellow\">-</span><span class=\"ansiyellow\">-</span>quiet <span class=\"ansiyellow\">-</span>O <span class=\"ansiyellow\">/</span>mnt<span class=\"ansiyellow\">/</span>driver<span class=\"ansiyellow\">-</span>daemon<span class=\"ansiyellow\">/</span>jars<span class=\"ansiyellow\">/</span>postgresql<span class=\"ansiyellow\">-</span><span class=\"ansicyan\">9.3</span><span class=\"ansiyellow\">-</span><span class=\"ansicyan\">1101</span><span class=\"ansiyellow\">-</span>jdbc4<span class=\"ansiyellow\">.</span>jar http<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\">//</span>central<span class=\"ansiyellow\">.</span>maven<span class=\"ansiyellow\">.</span>org<span class=\"ansiyellow\">/</span>maven2<span class=\"ansiyellow\">/</span>org<span class=\"ansiyellow\">/</span>postgresql<span class=\"ansiyellow\">/</span>postgresql<span class=\"ansiyellow\">/</span><span class=\"ansicyan\">9.3</span><span class=\"ansiyellow\">-</span><span class=\"ansicyan\">1101</span><span class=\"ansiyellow\">-</span>jdbc4<span class=\"ansiyellow\">/</span>postgresql<span class=\"ansiyellow\">-</span><span class=\"ansicyan\">9.3</span><span class=\"ansiyellow\">-</span><span class=\"ansicyan\">1101</span><span class=\"ansiyellow\">-</span>jdbc4<span class=\"ansiyellow\">.</span>jar<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> wget <span class=\"ansiyellow\">-</span><span class=\"ansiyellow\">-</span>quiet <span class=\"ansiyellow\">-</span>O <span class=\"ansiyellow\">/</span>mnt<span class=\"ansiyellow\">/</span>jars<span class=\"ansiyellow\">/</span>driver<span class=\"ansiyellow\">-</span>daemon<span class=\"ansiyellow\">/</span>postgresql<span class=\"ansiyellow\">-</span><span class=\"ansicyan\">9.3</span><span class=\"ansiyellow\">-</span><span class=\"ansicyan\">1101</span><span class=\"ansiyellow\">-</span>jdbc4<span class=\"ansiyellow\">.</span>jar http<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\">//</span>central<span class=\"ansiyellow\">.</span>maven<span class=\"ansiyellow\">.</span>org<span class=\"ansiyellow\">/</span>maven2<span class=\"ansiyellow\">/</span>org<span class=\"ansiyellow\">/</span>postgresql<span class=\"ansiyellow\">/</span>postgresql<span class=\"ansiyellow\">/</span><span class=\"ansicyan\">9.3</span><span class=\"ansiyellow\">-</span><span class=\"ansicyan\">1101</span><span class=\"ansiyellow\">-</span>jdbc4<span class=\"ansiyellow\">/</span>postgresql<span class=\"ansiyellow\">-</span><span class=\"ansicyan\">9.3</span><span class=\"ansiyellow\">-</span><span class=\"ansicyan\">1101</span><span class=\"ansiyellow\">-</span>jdbc4<span class=\"ansiyellow\">.</span>jar<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 5</span><span class=\"ansiyellow\"> &quot;&quot;&quot;, true)\n</span>\n<span class=\"ansired\">NameError</span>: name &apos;true&apos; is not defined\n</div>","startTime":1.443207282568E12,"submitTime":1.443207282494E12,"finishTime":1.443207282747E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"13a7c773-62ab-4fdc-b42b-ef8c82cb4a30"},{"version":"CommandV1","origId":2924,"guid":"76b58beb-6127-4e4a-98aa-ab1fb556c8c6","subtype":"command","commandType":"auto","position":9.0,"command":"%md ### Confirm that the global init script has been created.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"a3b02b17-ea2c-41ef-8f1d-5bdd1b8b750b"},{"version":"CommandV1","origId":2925,"guid":"3f31e5c0-47a2-45ff-9d4c-e6eb335cc00d","subtype":"command","commandType":"auto","position":10.0,"command":"display(dbutils.fs.ls(\"dbfs:/databricks/init/postgresql-install.sh\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/databricks/init/postgresql-install.sh","postgresql-install.sh",360.0]],"arguments":{},"schema":[{"type":"string","name":"path"},{"type":"string","name":"name"},{"type":"bigint","name":"size"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"java.io.FileNotFoundException: /databricks/init/postgresql-install.sh","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-4-f7e4982a12b7&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>display<span class=\"ansiyellow\">(</span>dbutils<span class=\"ansiyellow\">.</span>fs<span class=\"ansiyellow\">.</span>ls<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;dbfs:/databricks/init/postgresql-install.sh&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/tmp/1443207222923-0/dbutils.py</span> in <span class=\"ansicyan\">f_with_exception_handling</span><span class=\"ansiblue\">(*args, **kwargs)</span>\n<span class=\"ansigreen\">    112</span>                     <span class=\"ansigreen\">class</span> ExecutionError<span class=\"ansiyellow\">(</span>BaseException<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    113</span>                         <span class=\"ansigreen\">pass</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 114</span><span class=\"ansiyellow\">                     </span><span class=\"ansigreen\">raise</span> ExecutionError<span class=\"ansiyellow\">(</span>str<span class=\"ansiyellow\">(</span>e<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    115</span>             <span class=\"ansigreen\">return</span> f_with_exception_handling<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    116</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ExecutionError</span>: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.ls.\n: java.io.FileNotFoundException: /databricks/init/postgresql-install.sh\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:63)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:40)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:179)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:57)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n\n</div>","startTime":1.443207292502E12,"submitTime":1.443207292358E12,"finishTime":1.443207293081E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"8922ed45-6ddd-49e2-8015-986fbe0d5a64"},{"version":"CommandV1","origId":2926,"guid":"6ef8bb5d-025a-4882-a597-87929fdde9a8","subtype":"command","commandType":"auto","position":11.0,"command":"clusterName = \"YOUR_CLUSTER_NAME\"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.443207298803E12,"submitTime":1.443207298735E12,"finishTime":1.443207298883E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"08752827-c355-4db1-b012-f50a07419faf"},{"version":"CommandV1","origId":2927,"guid":"031c2744-ea6f-4329-b195-6edc45e342ab","subtype":"command","commandType":"auto","position":12.0,"command":"%md \n### Create **dbfs:/databricks/init/CLUSTER_NAME/** if it doesn't exist","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"75a1fa1b-4021-4376-b5ad-8546b05d6243"},{"version":"CommandV1","origId":2928,"guid":"ad9f8e39-981c-4acf-930c-4b574c98dca7","subtype":"command","commandType":"auto","position":13.0,"command":"dbutils.fs.mkdirs(\"dbfs:/databricks/init/%s/\" % clusterName)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>True\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.443207320119E12,"submitTime":1.443207320032E12,"finishTime":1.443207320754E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"549ed5b1-cb43-4e88-9165-e55ce974cef2"},{"version":"CommandV1","origId":2929,"guid":"c33f3734-86b4-4137-8861-711b1d300bc7","subtype":"command","commandType":"auto","position":14.0,"command":"dbutils.fs.put(\"dbfs:/databricks/init/%s/postgresql-install.sh\" % clusterName,\"\"\"\n#!/bin/bash \nwget --quiet -O /databricks/jars/postgresql-9.3-1101-jdbc4.jar http://central.maven.org/maven2/org/postgresql/postgresql/9.3-1101-jdbc4/postgresql-9.3-1101-jdbc4.jar\n\"\"\", True)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Wrote 149 bytes.\n<span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>True\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.443207353205E12,"submitTime":1.443207353113E12,"finishTime":1.443207353335E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"89d095f5-b827-4f2c-9e6a-1d251c42ae68"},{"version":"CommandV1","origId":2930,"guid":"14d05009-48d9-43ff-8620-06224a20527a","subtype":"command","commandType":"auto","position":14.5,"command":"%md ### Confirm that the cluster-specific init script has been created.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"7c2f8d12-e2a1-4be3-9824-8a21534ba1aa"},{"version":"CommandV1","origId":2931,"guid":"fbbd6fcc-aefa-4705-9b96-172bcbd77e99","subtype":"command","commandType":"auto","position":15.0,"command":"display(dbutils.fs.ls(\"dbfs:/databricks/init/%s/postgresql-install.sh\" % clusterName))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/databricks/init/YOUR_CLUSTER_NAME/setup-awscreds.sh","setup-awscreds.sh",149.0]],"arguments":{},"schema":[{"type":"string","name":"path"},{"type":"string","name":"name"},{"type":"bigint","name":"size"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"java.net.URISyntaxException: Malformed escape pair at index 22: dbfs:/databricks/init/%s/setup-awscreds.sh","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-10-d501454c9341&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>dbutils<span class=\"ansiyellow\">.</span>fs<span class=\"ansiyellow\">.</span>ls<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;dbfs:/databricks/init/%s/setup-awscreds.sh&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/tmp/1443207222923-0/dbutils.py</span> in <span class=\"ansicyan\">f_with_exception_handling</span><span class=\"ansiblue\">(*args, **kwargs)</span>\n<span class=\"ansigreen\">    112</span>                     <span class=\"ansigreen\">class</span> ExecutionError<span class=\"ansiyellow\">(</span>BaseException<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    113</span>                         <span class=\"ansigreen\">pass</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 114</span><span class=\"ansiyellow\">                     </span><span class=\"ansigreen\">raise</span> ExecutionError<span class=\"ansiyellow\">(</span>str<span class=\"ansiyellow\">(</span>e<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    115</span>             <span class=\"ansigreen\">return</span> f_with_exception_handling<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    116</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ExecutionError</span>: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.ls.\n: java.net.URISyntaxException: Malformed escape pair at index 22: dbfs:/databricks/init/%s/setup-awscreds.sh\n\tat java.net.URI$Parser.fail(URI.java:2848)\n\tat java.net.URI$Parser.scanEscape(URI.java:2978)\n\tat java.net.URI$Parser.scan(URI.java:3001)\n\tat java.net.URI$Parser.checkChars(URI.java:3019)\n\tat java.net.URI$Parser.parseHierarchical(URI.java:3105)\n\tat java.net.URI$Parser.parse(URI.java:3053)\n\tat java.net.URI.&lt;init&gt;(URI.java:588)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.getPath(DBUtilsCore.scala:154)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.getFS(DBUtilsCore.scala:165)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:57)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n\n</div>","startTime":1.443207406637E12,"submitTime":1.443207406558E12,"finishTime":1.44320740716E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"4113c1e7-e14c-4d52-8b01-751256aaebc2"},{"version":"CommandV1","origId":2932,"guid":"9b4996a3-7077-44f3-b999-2d7a32cb3ce6","subtype":"command","commandType":"auto","position":16.0,"command":"%md ### Clean up the example init scripts that were just added","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"c059ae03-4ef6-4d95-a57a-95ac56c2b42e"},{"version":"CommandV1","origId":2933,"guid":"6db6b1d3-6a14-49a5-8839-7c3da8a4e089","subtype":"command","commandType":"auto","position":17.0,"command":"dbutils.fs.rm(\"/databricks/init/postgresql-install.sh\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">13</span><span class=\"ansired\">]: </span>True\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.443207421657E12,"submitTime":1.44320742156E12,"finishTime":1.443207421835E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"8c66ba7f-fcc1-42a4-ba06-3783cef3661b"},{"version":"CommandV1","origId":2934,"guid":"845b0d64-298e-4170-8245-55e21f0b7371","subtype":"command","commandType":"auto","position":18.0,"command":"dbutils.fs.rm(\"dbfs:/databricks/init/%s/postgresql-install.sh\" % clusterName)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">14</span><span class=\"ansired\">]: </span>True\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.443207439325E12,"submitTime":1.443207439238E12,"finishTime":1.443207439504E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"8ce46ace-bb4d-4b13-b3af-31ea1c8fa8c5"},{"version":"CommandV1","origId":2935,"guid":"5cc98d61-9573-4218-982e-055caaba5af8","subtype":"command","commandType":"auto","position":19.0,"command":"%md ### Modify spark conf parameters","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"e3157ddd-d88a-4461-9445-4b106a4a0690"},{"version":"CommandV1","origId":2936,"guid":"dceebb08-04bf-4f32-90df-41fdc6317e98","subtype":"command","commandType":"auto","position":20.0,"command":"%md Example: Setup cassandra host in the spark conf to connect to a cassandra cluster","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"34a54902-b6af-44fd-bb01-a49ee67bd1d2"},{"version":"CommandV1","origId":2937,"guid":"6c175f27-02a8-4323-b731-12814fe99d72","subtype":"command","commandType":"auto","position":21.0,"command":"sparkClusterHostIP = \"192.168.0.1\"\ndbutils.fs.put(\"/databricks/init/%s/cassandra.sh\" % clusterName,\"\"\"\n#!/usr/bin/bash\necho '[driver].\"spark.cassandra.connection.host\" = \"%s\"' >> /home/ubuntu/databricks/common/conf/cassandra.conf\n\"\"\" % sparkClusterHostIP, True)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Wrote 137 bytes.\n<span class=\"ansired\">Out[</span><span class=\"ansired\">15</span><span class=\"ansired\">]: </span>True\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.443207634909E12,"submitTime":1.443207634812E12,"finishTime":1.44320763509E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"bf0fa324-daf3-4f52-bb52-5b07881b4e51"},{"version":"CommandV1","origId":2938,"guid":"06b1c429-523a-492d-b5f2-e64b488b27c0","subtype":"command","commandType":"auto","position":22.0,"command":"# Verify the contents\ndbutils.fs.head(\"/databricks/init/%s/cassandra.sh\" % clusterName)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">19</span><span class=\"ansired\">]: </span>u&apos;\\n#!/usr/bin/bash\\necho \\&apos;[driver].&quot;spark.cassandra.connection.host&quot; = &quot;192.168.0.1&quot;\\&apos; &gt;&gt; /home/ubuntu/databricks/common/conf/cassandra.conf\\n&apos;\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;dbfs&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-16-d4ef17459742&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>dbfs<span class=\"ansiyellow\">.</span>fs<span class=\"ansiyellow\">.</span>head<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;/databricks/init/%s/cassandra.sh&quot;</span> <span class=\"ansiyellow\">%</span> clusterName<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;dbfs&apos; is not defined\n</div>","startTime":1.443207729268E12,"submitTime":1.443207729024E12,"finishTime":1.443207729446E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"d6329535-adec-4651-9820-4041b89dedba"},{"version":"CommandV1","origId":2939,"guid":"9e39887e-8088-44db-ad3d-c2e4b1f21d0f","subtype":"command","commandType":"auto","position":23.0,"command":"%md ### Analysis of Init Script Logs\nLogs are stored under **dbfs:/databricks/init/output/__CLUSTER_NAME__/**  \nThe logs are timestamped and should allow you to analyze why the script was failing. \n* The output is placed into a folder with the cluster's name\n* Global scripts will have the **_script_name_timestamp.log_** and cluster specific logs as **_cluster_name_script_name_timestamp.log_**","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"62176841-7cb5-4e86-847d-d93dfcc96ef9"},{"version":"CommandV1","origId":2940,"guid":"28d86623-3899-442e-b064-530185ee6bb0","subtype":"command","commandType":"auto","position":24.0,"command":"# Find the desired init script logs\ndbutils.fs.ls(\"/databricks/init/output/%s/\" % clusterName)","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"7e0827bc-8bf0-40d2-a60e-08fd80fe6fb0"},{"version":"CommandV1","origId":2941,"guid":"81af8816-191a-4750-945c-12bc0ade5eaf","subtype":"command","commandType":"auto","position":25.0,"command":"# Print the contents of the log file found above. \ndbutils.fs.head(\"/databricks/init/output/%s/2015-10-12_20-34-42/initscript-install.sh_10.0.210.132.log\" % clusterName, 100000)","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"4bcc0835-1958-4807-a737-c4723c9b0afd"},{"version":"CommandV1","origId":2942,"guid":"89d8e74c-057c-4c48-b617-aad1e2fcfffa","subtype":"command","commandType":"auto","position":26.0,"command":"%md\n### Init Script Not Working\nCustomer's typically use the init scripts to install additional packages using Ubuntu's package manager. \nThere may be an issue when a customer attempts to use apt-get to install these packages and run into the following error:\n```\nstderr:\nE: You must put some 'source' URIs in your sources.list\nE: There are problems and -y was used without --force-yes\nE: Command line option 'y' [from -y] is not known.\nE: There are problems and -y was used without --force-yes\n```\n\nThe above error message shows up in the init script logs. To workaround around this, add the following `--force-yes` option to the command.  \nExample: \n```\napt-get -y --force-yes install libssl-dev\n```\n","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"3824fd46-1d44-4255-8b9a-7f4067ce67bc"}],"guid":"1e812a0b-0ce5-4b34-bd2e-ffc004501af4","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"78b47788-d387-49b6-8172-01523009625f","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>