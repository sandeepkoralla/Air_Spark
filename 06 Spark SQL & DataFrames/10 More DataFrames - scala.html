<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Spark SQL &amp; DataFrames / More DataFrames - scala - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":1457,"name":"Spark SQL & DataFrames / More DataFrames - scala","language":"scala","commands":[{"version":"CommandV1","origId":1458,"guid":"43e2df6d-f16d-4282-9666-71a1dece5831","subtype":"command","commandType":"auto","position":0.5,"command":"%md \nThis notebook is formatted as an FAQ with to describe common use cases and example usage using the available APIs.  \nThe scala documentation that describes the APIs are located [here](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame)\n\n**Q:** DataFrame UDF Performance. How could I get better performance?  \n**A:** If the functionality exists in the available built-in functions, using these will perform better. Example usage below. Specific documnetation is located [here](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$).  \nWe use the built-in functions and the _withColumn()_ API to add new columns. We could have also used _withColumnRenamed()_ to replace an existing column after the transformation.    \n**Note:** Import the libraries in the first cell\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"e1783c89-0947-418c-85c3-772144e00824"},{"version":"CommandV1","origId":1459,"guid":"75f3b92c-42e9-45c4-b744-c236b67a5f17","subtype":"command","commandType":"auto","position":0.75,"command":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.hadoop.io.LongWritable\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat\n\n// Build an example DataFrame dataset to work with. \ndbutils.fs.rm(\"/tmp/dataframe_sample.csv\", true)\ndbutils.fs.put(\"/tmp/dataframe_sample.csv\", \"\"\"\nid|end_date|start_date|location\n1|2015-10-14 00:00:00|2015-09-14 00:00:00|CA-SF\n2|2015-10-15 01:00:20|2015-08-14 00:00:00|CA-SD\n3|2015-10-16 02:30:00|2015-01-14 00:00:00|NY-NY\n4|2015-10-17 03:00:20|2015-02-14 00:00:00|NY-NY\n5|2015-10-18 04:30:00|2014-04-14 00:00:00|CA-LA\n\"\"\", true)\n\nval conf = new Configuration\nconf.set(\"textinputformat.record.delimiter\", \"\\n\")\nval rdd = sc.newAPIHadoopFile(\"/tmp/dataframe_sample.csv\", classOf[TextInputFormat], classOf[LongWritable], classOf[Text], conf).map(_._2.toString).filter(_.nonEmpty)\n\nval header = rdd.first()\n// Parse the header line \nval rdd_noheader = rdd.filter(x => !x.contains(\"id\"))\n// Convert the RDD[String] to an RDD[Rows]. Create an array using the delimiter and use Row.fromSeq() \nval row_rdd = rdd_noheader.map(x => x.split('|')).map(x => Row.fromSeq(x))\n\nval df_schema =\n  StructType(\n    header.split('|').map(fieldName => StructField(fieldName, StringType, true)))\n\nvar df = sqlContext.createDataFrame(row_rdd, df_schema)\ndf.printSchema","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Wrote 273 bytes.\nroot\n |-- id: string (nullable = true)\n |-- end_date: string (nullable = true)\n |-- start_date: string (nullable = true)\n |-- location: string (nullable = true)\n\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.hadoop.io.LongWritable\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat\nconf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml\nrdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[49] at filter at &lt;console&gt;:153\nheader: String = id|end_date|start_date|location\nrdd_noheader: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[50] at filter at &lt;console&gt;:157\nrow_rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[52] at map at &lt;console&gt;:159\ndf_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(end_date,StringType,true), StructField(start_date,StringType,true), StructField(location,StringType,true))\ndf: org.apache.spark.sql.DataFrame = [id: string, end_date: string, start_date: string, location: string]\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:77: error: overloaded method value createDataFrame with alternatives:\n  (rdd: org.apache.spark.api.java.JavaRDD[_],beanClass: Class[_])org.apache.spark.sql.DataFrame &lt;and&gt;\n  (rdd: org.apache.spark.rdd.RDD[_],beanClass: Class[_])org.apache.spark.sql.DataFrame &lt;and&gt;\n  (rowRDD: org.apache.spark.api.java.JavaRDD[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame &lt;and&gt;\n  (rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame\n cannot be applied to (org.apache.spark.rdd.RDD[String], org.apache.spark.sql.types.StructType)\nval df = sqlContext.createDataFrame(rdd_noheader, df_schema)\n                    ^\n</div>","error":null,"startTime":1.447451535276E12,"submitTime":1.447451536908E12,"finishTime":1.447451542526E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"ff9dc108-0bb3-46d9-a780-380d71adbbe4"},{"version":"CommandV1","origId":1460,"guid":"5af8f7c7-8990-4f3c-9a22-dad49c97b98c","subtype":"command","commandType":"auto","position":0.875,"command":"// Instead of registering a UDF, call the builtin functions to perform operations on the columns. \n// This will provide a performance improvement as the builtins are compiled and running in the platform's JVM. \n\n// Convert to a Date type\nval timestamp2datetype: (Column) => Column = (x) => { to_date(x) }\ndf = df.withColumn(\"date\", timestamp2datetype(col(\"end_date\")))\n\n// Parse out the date only\nval timestamp2date: (Column) => Column = (x) => { regexp_replace(x,\" (\\\\d+)[:](\\\\d+)[:](\\\\d+).*$\", \"\") } \ndf = df.withColumn(\"date_only\", timestamp2date(col(\"end_date\")))\n\n// Split a string and index a field\nval parse_city: (Column) => Column = (x) => { split(x, \"-\")(1) } \ndf = df.withColumn(\"city\", parse_city(col(\"location\")))\n\n// Perform a date diff function\nval dateDiff: (Column, Column) => Column = (x, y) => { datediff(to_date(y), to_date(x)) }\ndf = df.withColumn(\"date_diff\", dateDiff(col(\"start_date\"), col(\"end_date\")))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">timestamp2datetype: org.apache.spark.sql.Column =&gt; org.apache.spark.sql.Column = &lt;function1&gt;\ndf: org.apache.spark.sql.DataFrame = [id: string, end_date: string, start_date: string, location: string, date: date, date_only: string, city: string, date_diff: int]\ntimestamp2date: org.apache.spark.sql.Column =&gt; org.apache.spark.sql.Column = &lt;function1&gt;\ndf: org.apache.spark.sql.DataFrame = [id: string, end_date: string, start_date: string, location: string, date: date, date_only: string, city: string, date_diff: int]\nparse_city: org.apache.spark.sql.Column =&gt; org.apache.spark.sql.Column = &lt;function1&gt;\ndf: org.apache.spark.sql.DataFrame = [id: string, end_date: string, start_date: string, location: string, date: date, date_only: string, city: string, date_diff: int]\ndateDiff: (org.apache.spark.sql.Column, org.apache.spark.sql.Column) =&gt; org.apache.spark.sql.Column = &lt;function2&gt;\ndf: org.apache.spark.sql.DataFrame = [id: string, end_date: string, start_date: string, location: string, date: date, date_only: string, city: string, date_diff: int]\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:94: error: type mismatch;\n found   : Char('-')\n required: String\n       val parse_city: (Column) =&gt; Column = (x) =&gt; { split(x, '-')(1) } \n                                                              ^\n</div>","error":null,"startTime":1.447451542535E12,"submitTime":1.44745153814E12,"finishTime":1.447451543539E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"8dd83d21-4457-45ff-ba64-e930a0c06f79"},{"version":"CommandV1","origId":1461,"guid":"73589a97-f22a-455a-be91-4193ad703b9e","subtype":"command","commandType":"auto","position":1.875,"command":"df.registerTempTable(\"sample_df\")\ndisplay(sql(\"select * from sample_df\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["1","2015-10-14 00:00:00","2015-09-14 00:00:00","CA-SF","2015-10-14","2015-10-14","SF",30.0],["2","2015-10-15 01:00:20","2015-08-14 00:00:00","CA-SD","2015-10-15","2015-10-15","SD",62.0],["3","2015-10-16 02:30:00","2015-01-14 00:00:00","NY-NY","2015-10-16","2015-10-16","NY",275.0],["4","2015-10-17 03:00:20","2015-02-14 00:00:00","NY-NY","2015-10-17","2015-10-17","NY",245.0],["5","2015-10-18 04:30:00","2014-04-14 00:00:00","CA-LA","2015-10-18","2015-10-18","LA",552.0]],"arguments":{},"schema":[{"type":"string","name":"id"},{"type":"string","name":"end_date"},{"type":"string","name":"start_date"},{"type":"string","name":"location"},{"type":"date","name":"date"},{"type":"string","name":"date_only"},{"type":"string","name":"city"},{"type":"int","name":"date_diff"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":null,"startTime":1.447451543546E12,"submitTime":1.447451539231E12,"finishTime":1.447451545526E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"ad11e659-15a5-4eab-a7bb-9214154549be"},{"version":"CommandV1","origId":1462,"guid":"bccbfa3f-7948-45c2-90df-f54cd1b2736f","subtype":"command","commandType":"auto","position":2.875,"command":"%md\n**Q:** I want to convert the DataFrame back to json strings to send back to Kafka.  \n**A:** There is an underlying `toJSON()` function that returns an RDD of json strings using the column names and schema to product the json records. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"36c75f37-7226-41af-b547-f3343cacf03c"},{"version":"CommandV1","origId":1463,"guid":"7e800ff3-2a1a-4bc9-ac25-392316a3e8b3","subtype":"command","commandType":"auto","position":3.875,"command":"val rdd_json = df.toJSON\nrdd_json.take(2).foreach(println)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">{&quot;id&quot;:&quot;1&quot;,&quot;end_date&quot;:&quot;2015-10-14 00:00:00&quot;,&quot;start_date&quot;:&quot;2015-09-14 00:00:00&quot;,&quot;location&quot;:&quot;CA-SF&quot;,&quot;date&quot;:&quot;2015-10-14&quot;,&quot;date_only&quot;:&quot;2015-10-14&quot;,&quot;city&quot;:&quot;SF&quot;,&quot;date_diff&quot;:30}\n{&quot;id&quot;:&quot;2&quot;,&quot;end_date&quot;:&quot;2015-10-15 01:00:20&quot;,&quot;start_date&quot;:&quot;2015-08-14 00:00:00&quot;,&quot;location&quot;:&quot;CA-SD&quot;,&quot;date&quot;:&quot;2015-10-15&quot;,&quot;date_only&quot;:&quot;2015-10-15&quot;,&quot;city&quot;:&quot;SD&quot;,&quot;date_diff&quot;:62}\nrdd_json: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[59] at toJSON at &lt;console&gt;:143\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:83: error: not found: value println_\n              rdd_json.take(2).foreach(println_)\n                                       ^\n</div>","error":null,"startTime":1.447451545626E12,"submitTime":1.447451541585E12,"finishTime":1.447451546315E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"d2c2b92b-001a-4b42-b3ed-9baf6a6286de"},{"version":"CommandV1","origId":1464,"guid":"04a9efaa-73e7-4422-bc4f-16e1fbd33a56","subtype":"command","commandType":"auto","position":4.875,"command":"%md \n**Q:** My UDF takes a parameter including the column to operate on. How do I pass this parameter?  \n**A:** There is a function available called lit() that creates a static column. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"08272d49-b03a-4f1e-976b-d57eda065ac2"},{"version":"CommandV1","origId":1465,"guid":"c10a059c-bf8c-44ba-9bdb-1d9ee8540aa2","subtype":"command","commandType":"auto","position":5.875,"command":"val add_n = udf((x: Integer, y: Integer) => x + y)\n\n// We register a UDF that adds a column to the DataFrame, and we cast the id column to an Integer type. \ndf = df.withColumn(\"id_offset\", add_n(lit(1000), col(\"id\").cast(\"int\")))\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[["1","2015-10-14 00:00:00","2015-09-14 00:00:00","CA-SF","2015-10-14","2015-10-14","SF",30.0,1001.0],["2","2015-10-15 01:00:20","2015-08-14 00:00:00","CA-SD","2015-10-15","2015-10-15","SD",62.0,1002.0],["3","2015-10-16 02:30:00","2015-01-14 00:00:00","NY-NY","2015-10-16","2015-10-16","NY",275.0,1003.0],["4","2015-10-17 03:00:20","2015-02-14 00:00:00","NY-NY","2015-10-17","2015-10-17","NY",245.0,1004.0],["5","2015-10-18 04:30:00","2014-04-14 00:00:00","CA-LA","2015-10-18","2015-10-18","LA",552.0,1005.0]],"arguments":{},"schema":[{"type":"string","name":"id"},{"type":"string","name":"end_date"},{"type":"string","name":"start_date"},{"type":"string","name":"location"},{"type":"date","name":"date"},{"type":"string","name":"date_only"},{"type":"string","name":"city"},{"type":"int","name":"date_diff"},{"type":"int","name":"id_offset"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"java.lang.UnsupportedOperationException: Schema for type org.apache.spark.sql.Column is not supported","error":"<div class=\"ansiout\">\tat org.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaReflection.scala:153)\n\tat org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:29)\n\tat org.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaReflection.scala:64)\n\tat org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:29)\n\tat org.apache.spark.sql.functions$.udf(functions.scala:2254)</div>","startTime":1.447451546325E12,"submitTime":1.447451543358E12,"finishTime":1.44745154715E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"c3c537b6-89f5-408d-a9f6-1e49a6e0ac02"},{"version":"CommandV1","origId":1466,"guid":"9f0632d7-7e2b-40bb-b3c0-65dc41f564f0","subtype":"command","commandType":"auto","position":6.375,"command":"val last_n_days = udf((x: Integer, y: Integer) => {\n  if (x < y) true else false\n})\n\n//last_n_days = udf(lambda x, y: True if x < y else False, BooleanType())\n\nval df_filtered = df.filter(last_n_days(col(\"date_diff\"), lit(90)))\ndisplay(df_filtered)","commandVersion":0,"state":"finished","results":{"type":"table","data":[["1","2015-10-14 00:00:00","2015-09-14 00:00:00","CA-SF","2015-10-14","2015-10-14","SF",30.0,1001.0],["2","2015-10-15 01:00:20","2015-08-14 00:00:00","CA-SD","2015-10-15","2015-10-15","SD",62.0,1002.0]],"arguments":{},"schema":[{"type":"string","name":"id"},{"type":"string","name":"end_date"},{"type":"string","name":"start_date"},{"type":"string","name":"location"},{"type":"date","name":"date"},{"type":"string","name":"date_only"},{"type":"string","name":"city"},{"type":"int","name":"date_diff"},{"type":"int","name":"id_offset"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:149: error: return outside method definition\n                  return true\n                  ^\n</div>","error":null,"startTime":1.447451737014E12,"submitTime":1.447451738635E12,"finishTime":1.447451737739E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"24678b87-f778-4fe3-a1f2-37191a8496ef"},{"version":"CommandV1","origId":1467,"guid":"e7026de0-0c68-4b9d-b6a0-f75c586e6a45","subtype":"command","commandType":"auto","position":6.875,"command":"%md \n**Q:** I have a table in the hive metastore and I'd like to access to table as a DataFrame. What's the best way to define this?  \n**A:** There's multiple ways to define a DataFrame from a registered table. Syntax show below.  \nCall `table(tableName)` or select and filter specific columns using an SQL query. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"e2b33764-03be-4cf4-9ed3-10e22462c200"},{"version":"CommandV1","origId":1468,"guid":"3b8a1990-785e-4f37-ac34-9d874b695654","subtype":"command","commandType":"auto","position":7.875,"command":"// Both return DataFrame types\nval df_1 = table(\"sample_df\")\nval df_2 = sqlContext.sql(\"select * from sample_df\") ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">df_1: org.apache.spark.sql.DataFrame = [id: string, end_date: string, start_date: string, location: string, date: date, date_only: string, city: string, date_diff: int]\ndf_2: org.apache.spark.sql.DataFrame = [id: string, end_date: string, start_date: string, location: string, date: date, date_only: string, city: string, date_diff: int]\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.447370165148E12,"submitTime":1.447370164471E12,"finishTime":1.447370165373E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"9087fc33-7246-475c-931c-beb4dbd0887a"},{"version":"CommandV1","origId":1469,"guid":"ce9f822b-4298-48d7-9e9a-65d255bac606","subtype":"command","commandType":"auto","position":8.875,"command":"%md\n**Q:** I'd like to clear all the cached tables on the current cluster.  \n**A:** There's an API available to do this. This can be done at a global level or per table. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"9b685f02-408d-4514-9e59-32e6ac82b0b1"},{"version":"CommandV1","origId":1470,"guid":"39041b39-d7ed-4d37-85d9-20228d953615","subtype":"command","commandType":"auto","position":9.875,"command":"sqlContext.clearCache()\nsqlContext.cacheTable(\"sample_df\")\nsqlContext.uncacheTable(\"sample_df\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.447370170496E12,"submitTime":1.447370169766E12,"finishTime":1.447370170766E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"ff9c8cff-e6f7-4d6f-b33e-224a0a64f65e"},{"version":"CommandV1","origId":1471,"guid":"4f01daac-2cdf-4d05-8a51-a731daa1627f","subtype":"command","commandType":"auto","position":10.875,"command":"%md \n**Q:** I'd like to compute aggregates on columns. What's the best way to do this?  \n**A:** There's a new API available named `agg(*exprs)` that takes a list of column names and expressions for the type of aggregation you'd like to compute. Documentation can be found [here](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame). You can leverage the built-in functions that were mentioned above as part of the expressions for each column. \n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"eaac7622-8133-4931-b91c-d0352f7a7b48"},{"version":"CommandV1","origId":1472,"guid":"ab7b2cf0-7ab5-43d8-9b80-5cecf303b505","subtype":"command","commandType":"auto","position":11.875,"command":"// Provide the min, count, and avg and groupBy the location column. Diplay the results \nvar agg_df = df.groupBy(\"location\").agg(min(\"id\"), count(\"id\"), avg(\"date_diff\"))\ndisplay(agg_df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[["CA-LA","5",1.0,552.0],["NY-NY","3",2.0,260.0],["CA-SD","2",1.0,62.0],["CA-SF","1",1.0,30.0]],"arguments":{},"schema":[{"type":"string","name":"location"},{"type":"string","name":"min(id)"},{"type":"bigint","name":"count(id)"},{"type":"double","name":"avg(date_diff)"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":null,"startTime":1.447370278142E12,"submitTime":1.447370277484E12,"finishTime":1.447370280356E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"8b482aa9-0422-4ff9-95b3-b8e4de48dbdc"},{"version":"CommandV1","origId":1473,"guid":"ebc5ae49-a382-418b-ac00-df6ee8cc3435","subtype":"command","commandType":"auto","position":12.875,"command":"%md \n**Q:** I'd like to write out the DataFrames to Parquet, but would like to partition on a particular column.  \n**A:** You can use the following APIs to accomplish this. Ensure the code does not create too many partition columns with the datasets otherwise the overhead of the metadata can cause queries to slow down significantly. If there is a SQL table back by this directory, users will need to call `refresh table _tableName_` to update the metadata prior to the query. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"99237bc5-71a3-4f39-b28f-f0473be430cd"},{"version":"CommandV1","origId":1474,"guid":"e0ea9536-1455-4309-87e9-1baacb24b635","subtype":"command","commandType":"auto","position":13.875,"command":"df = df.withColumn(\"end_month\", month(col(\"end_date\")))\ndf = df.withColumn(\"end_year\", year(col(\"end_date\")))\ndbutils.fs.rm(\"/tmp/sample_table\", true)\ndf.write.partitionBy(\"end_year\", \"end_month\").parquet(\"/tmp/sample_table\")\ndisplay(dbutils.fs.ls(\"/tmp/sample_table\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/tmp/sample_table/_SUCCESS","_SUCCESS",0.0],["dbfs:/tmp/sample_table/_common_metadata","_common_metadata",893.0],["dbfs:/tmp/sample_table/_metadata","_metadata",2228.0],["dbfs:/tmp/sample_table/end_year=2015/","end_year=2015/",0.0]],"arguments":{},"schema":[{"type":"string","name":"path"},{"type":"string","name":"name"},{"type":"bigint","name":"size"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"org.apache.spark.sql.AnalysisException: path dbfs:/tmp/sample_table already exists.;","error":"<div class=\"ansiout\">\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:76)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)\n\tat org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:933)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:933)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:197)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:137)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:304)</div>","startTime":1.447370456722E12,"submitTime":1.447370456047E12,"finishTime":1.447370460431E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"6b23cb6e-5e49-47a6-bb7a-b991caa2c515"},{"version":"CommandV1","origId":1475,"guid":"2f7adcbf-dfd9-4d1b-a966-9bd5fb8b4ee1","subtype":"command","commandType":"auto","position":14.875,"command":"%md \n**Q:** How do I properly handle cases where I want to filter out NULL data?  \n**A:** You can use _filter()_ and provide similar syntax as you would with a SQL query. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"ca8442c1-0865-45a0-98bd-6de752c4a641"},{"version":"CommandV1","origId":1476,"guid":"60375d45-7904-4772-b8e9-c0b853be29ba","subtype":"command","commandType":"auto","position":15.875,"command":"val null_item_schema = StructType(Array(StructField(\"col1\", StringType, true),\n                               StructField(\"col2\", IntegerType, true)))\n\nval null_dataset = sc.parallelize(Array((\"test\", 1 ), (null, 2))).map(x => Row.fromTuple(x))\nval null_df = sqlContext.createDataFrame(null_dataset, null_item_schema)\ndisplay(null_df.filter(\"col1 IS NOT NULL\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["test",1.0]],"arguments":{},"schema":[{"type":"string","name":"col1"},{"type":"int","name":"col2"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:67: error: type mismatch;\n found   : (String, Int)\n required: Seq[Any]\n              val null_dataset = sc.parallelize(Array((&quot;test&quot;, 1 ), (null, 2))).map(x =&gt; Row.fromSeq(x))\n                                                                                                     ^\n&lt;console&gt;:68: error: overloaded method value createDataFrame with alternatives:\n  (rdd: org.apache.spark.api.java.JavaRDD[_],beanClass: Class[_])org.apache.spark.sql.DataFrame &lt;and&gt;\n  (rdd: org.apache.spark.rdd.RDD[_],beanClass: Class[_])org.apache.spark.sql.DataFrame &lt;and&gt;\n  (rowRDD: org.apache.spark.api.java.JavaRDD[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame &lt;and&gt;\n  (rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame\n cannot be applied to (org.apache.spark.rdd.RDD[Nothing], org.apache.spark.sql.types.StructType)\n              val null_df = sqlContext.createDataFrame(null_dataset, null_item_schema)\n                                       ^\n</div>","error":null,"startTime":1.447377741082E12,"submitTime":1.447377740335E12,"finishTime":1.447377741626E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"65321511-c8cb-4ca7-8a3d-13761478e193"},{"version":"CommandV1","origId":1477,"guid":"7d503675-55db-4288-895b-5ef0320155d3","subtype":"command","commandType":"auto","position":16.875,"command":"%md \n**Q:** How do I infer the schema using the spark-csv or spark-avro libraries?  \n**A:** Documented on the GitHub projects [spark-csv](https://github.com/databricks/spark-csv), there is an inferSchema option flag. Providing a header would allow the columns to be named appropriately. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"d84f87a5-ff67-48de-b499-07353452c7d1"},{"version":"CommandV1","origId":1478,"guid":"e0b4fe64-0995-4a0d-9d39-dae8ae168760","subtype":"command","commandType":"auto","position":17.875,"command":"val adult_df = sqlContext.read.\n    format(\"com.databricks.spark.csv\").\n    option(\"header\", \"false\").\n    option(\"inferSchema\", \"true\").load(\"dbfs:/databricks-datasets/adult/adult.data\")\nadult_df.printSchema()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- C0: integer (nullable = true)\n |-- C1: string (nullable = true)\n |-- C2: double (nullable = true)\n |-- C3: string (nullable = true)\n |-- C4: double (nullable = true)\n |-- C5: string (nullable = true)\n |-- C6: string (nullable = true)\n |-- C7: string (nullable = true)\n |-- C8: string (nullable = true)\n |-- C9: string (nullable = true)\n |-- C10: double (nullable = true)\n |-- C11: double (nullable = true)\n |-- C12: double (nullable = true)\n |-- C13: string (nullable = true)\n |-- C14: string (nullable = true)\n\nadult_df: org.apache.spark.sql.DataFrame = [C0: int, C1: string, C2: double, C3: string, C4: double, C5: string, C6: string, C7: string, C8: string, C9: string, C10: double, C11: double, C12: double, C13: string, C14: string]\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:62: error: value \\ is not a member of org.apache.spark.sql.DataFrameReader\n              val animal_df = sqlContext.read.\\\n                                              ^\n&lt;console&gt;:63: error: value \\ is not a member of String\n                  format(&quot;com.databricks.spark.csv&quot;).\\\n                                                     ^\n&lt;console&gt;:64: error: not found: value option\n                  option(&quot;header&quot;, &quot;false&quot;).\\\n                  ^\n&lt;console&gt;:65: error: not found: value option\n                  option(&quot;inferSchema&quot;, &quot;true&quot;).load(&quot;dbfs:/databricks-datasets/adult/adult.data&quot;)\n                  ^\n</div>","error":null,"startTime":1.447377871417E12,"submitTime":1.447377870666E12,"finishTime":1.447377872177E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"5f9243fb-4ad7-4b48-ae31-993996c52d5e"},{"version":"CommandV1","origId":1479,"guid":"d5c1473a-77a2-4dd6-8704-3b7f33926196","subtype":"command","commandType":"auto","position":18.875,"command":"%md \n**Q:** You have a delimited string dataset that you want to convert to their datatypes. How would you accomplish this?  \n**A:** Use the RDD APIs to filter out the malformed rows and map the values to the appropriate types. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"3f3bafa5-535f-40eb-ac26-9ce91678eaec"},{"version":"CommandV1","origId":1480,"guid":"ed2ac6d0-5621-4ca3-9ea0-af021eb08c02","subtype":"command","commandType":"auto","position":19.375,"command":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.hadoop.io.LongWritable\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat\n\nval conf = new Configuration\nconf.set(\"textinputformat.record.delimiter\", \"\\n\")\n\nval rdd_orders = sc.newAPIHadoopFile(\"/databricks-datasets/tpch/data-001/orders/orders.tbl\", classOf[TextInputFormat], classOf[LongWritable], classOf[Text], conf).map(_._2.toString).filter(_.nonEmpty)\n\n// Filter records that do not contain the correct number of delimiters\nval bad_records = rdd_orders.filter(r => r.split('|').length < 9 )\n\n// Convert the string to an RDD of Rows to create a DataFrame. \nval row_rdd = rdd_orders.map(x => x.split('|')).map(x => Row.fromSeq(x))\nval orders_schema = StructType ( Array(StructField(\"OrderKey\", StringType, true),\nStructField(\"CustKey\", StringType, true),\nStructField(\"OrderStatus\", StringType, true),\nStructField(\"TotalPrice\", StringType, true),\nStructField(\"OrderDate\", StringType, true),\nStructField(\"OrderPriority\", StringType, true),\nStructField(\"Clerk\", StringType, true),\nStructField(\"ShipPriority\", StringType, true),\nStructField(\"Comment\", StringType, true),\nStructField(\"skip\", StringType, true)) )\n\nval df_str_orders = sqlContext.createDataFrame(row_rdd, orders_schema)\n\n// Convert the String types to appropriate types by casting the types\nval df_orders = df_str_orders.withColumn(\"ok_tmp\", col(\"OrderKey\").cast(IntegerType)).\n                    withColumn(\"custk_tmp\", col(\"CustKey\").cast(IntegerType)).\n                    withColumn(\"ship_tmp\", col(\"ShipPriority\").cast(IntegerType)).\n                    drop(\"OrderKey\").\n                    drop(\"CustKey\").\n                    drop(\"ShipPriority\").\n                    withColumnRenamed(\"ok_tmp\", \"OrderKey\").\n                    withColumnRenamed(\"custk_tmp\", \"CustKey\").\n                    withColumnRenamed(\"ship_tmp\", \"ShipPriority\")\n\ndf_orders.printSchema","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- OrderStatus: string (nullable = true)\n |-- TotalPrice: string (nullable = true)\n |-- OrderDate: string (nullable = true)\n |-- OrderPriority: string (nullable = true)\n |-- Clerk: string (nullable = true)\n |-- Comment: string (nullable = true)\n |-- skip: string (nullable = true)\n |-- OrderKey: integer (nullable = true)\n |-- CustKey: integer (nullable = true)\n |-- ShipPriority: integer (nullable = true)\n\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.hadoop.io.LongWritable\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat\nconf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml\nrdd_orders: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[42] at filter at &lt;console&gt;:126\nbad_records: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[43] at filter at &lt;console&gt;:129\nrow_rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[45] at map at &lt;console&gt;:132\norders_schema: org.apache.spark.sql.types.StructType = StructType(StructField(OrderKey,StringType,true), StructField(CustKey,StringType,true), StructField(OrderStatus,StringType,true), StructField(TotalPrice,StringType,true), StructField(OrderDate,StringType,true), StructField(OrderPriority,StringType,true), StructField(Clerk,StringType,true), StructField(ShipPriority,StringType,true), StructField(Comment,StringType,true), StructField(skip,StringType,true))\ndf_str_orders: org.apache.spark.sql.DataFrame = [OrderKey: string, CustKey: string, OrderStatus: string, TotalPrice: string, OrderDate: string, OrderPriority: string, Clerk: string, ShipPriority: string, Comment: string, skip: string]\ndf_orders: org.apache.spark.sql.DataFrame = [OrderStatus: string, TotalPrice: string, OrderDate: string, OrderPriority: string, Clerk: string, Comment: string, skip: string, OrderKey: int, CustKey: int, ShipPriority: int]\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:33: error: not found: value conf\n       val rdd_orders = sc.newAPIHadoopFile(&quot;/databricks-datasets/tpch/data-001/orders/orders.tbl&quot;, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], conf).map(_._2.toString).filter(_.nonEmpty)\n                                                                                                                                                                    ^\n</div>","error":null,"startTime":1.447451030938E12,"submitTime":1.447451032482E12,"finishTime":1.447451031652E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"50252382-17d7-4d3a-a1a2-1f271d0b3264"}],"guid":"f402b6f0-fc27-41b3-9e91-f09aa8fad4bd","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"d40199c2-4c25-4201-bbce-dabe2160a052","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>