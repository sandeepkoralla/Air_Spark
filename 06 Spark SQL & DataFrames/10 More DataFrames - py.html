<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Spark SQL &amp; DataFrames / More DataFrames - py - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":1521,"name":"Spark SQL & DataFrames / More DataFrames - py","language":"python","commands":[{"version":"CommandV1","origId":1522,"guid":"a6e51668-8c0c-45b9-9248-30f81a0bb7c8","subtype":"command","commandType":"auto","position":1.0,"command":"%md \nThis notebook is formatted as an FAQ with to describe common use cases and example usage using the available APIs.  \nThe PySpark Documentation that describes the APIs are located [here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n\n**Q:** DataFrame UDF Performance. How could I get better performance?  \n**A:** If the functionality exists in the available built-in functions, using these will perform better. Example usage below. Specific documnetation is located [here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions).  \nWe use the built-in functions and the _withColumn()_ API to add new columns. We could have also used _withColumnRenamed()_ to replace an existing column after the transformation.    \n**Note:** Import the modules in the first cell\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"acf2dc44-983a-453a-9dd9-974b95e52326"},{"version":"CommandV1","origId":1523,"guid":"ab8ad5a9-83d0-4ab2-b2e2-e09d62550e46","subtype":"command","commandType":"auto","position":2.0,"command":"from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\n\n# Build an example DataFrame dataset to work with. \ndbutils.fs.rm(\"/tmp/dataframe_sample.csv\", True)\ndbutils.fs.put(\"/tmp/dataframe_sample.csv\", \"\"\"\nid|end_date|start_date|location\n1|2015-10-14 00:00:00|2015-09-14 00:00:00|CA-SF\n2|2015-10-15 01:00:20|2015-08-14 00:00:00|CA-SD\n3|2015-10-16 02:30:00|2015-01-14 00:00:00|NY-NY\n4|2015-10-17 03:00:20|2015-02-14 00:00:00|NY-NY\n5|2015-10-18 04:30:00|2014-04-14 00:00:00|CA-SD\n\"\"\", True)\n\nrdd = sc.newAPIHadoopFile(\"/tmp/dataframe_sample.csv\", \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\n            \"org.apache.hadoop.io.LongWritable\", \"org.apache.hadoop.io.Text\",\n            conf={\"textinputformat.record.delimiter\": \"|\"}).map(lambda l:l[1]).filter(lambda x: x).map(lambda y: y.split('|'))\n\nheader = rdd.first()\nrdd_noheader = rdd.filter(lambda x:x !=header) \n\n# You can create the appropriate schema by matching the header names and building the StructFields inside the create_schema function\ndef create_schema(header):\n  new_schema_items = []\n  for s in header:\n    new_schema_items.append(StructField(s, StringType(), True))\n  return StructType(new_schema_items)\n\ndf_schema = create_schema(header)\n\ndf = sqlContext.createDataFrame(rdd_noheader, df_schema)\ndf.printSchema()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Wrote 273 bytes.\nroot\n |-- id: string (nullable = true)\n |-- end_date: string (nullable = true)\n |-- start_date: string (nullable = true)\n |-- location: string (nullable = true)\n\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;data&apos; is not defined","error":"<div class=\"ansiout\">Wrote 273 bytes.\n<span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-40-fad1fd50564d&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     18</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     19</span> header <span class=\"ansiyellow\">=</span> rdd<span class=\"ansiyellow\">.</span>first<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 20</span><span class=\"ansiyellow\"> </span>data <span class=\"ansiyellow\">=</span> data<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span>x <span class=\"ansiyellow\">!=</span>header<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     21</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     22</span> <span class=\"ansired\"># You can create the appropriate schema by matching the header names and building the StructFields inside the create_schema function</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;data&apos; is not defined\n</div>","startTime":1.447731927414E12,"submitTime":1.447731927011E12,"finishTime":1.447731938768E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"vida","iPythonMetadata":null,"nuid":"1b5fd6bf-a699-4eb3-baed-15bc049fb41c"},{"version":"CommandV1","origId":1524,"guid":"f46884e5-bdd0-4097-95db-880f9bd5594f","subtype":"command","commandType":"auto","position":3.0,"command":"# Instead of registering a UDF, call the builtin functions to perform operations on the columns. \n# This will provide a performance improvement as the builtins are compiled and running in the platform's JVM. \n\n# Convert to a Date type\ntimestamp2datetype = lambda x: F.to_date(x)\ndf = df.withColumn('date', timestamp2datetype(df.end_date))\n\n# Parse out the date only\ntimestamp2date = lambda x: F.regexp_replace(x,' (\\d+)[:](\\d+)[:](\\d+).*$', '')\ndf = df.withColumn('date_only', timestamp2date(df.end_date))\n\n# Split a string and index a field\nparse_city = lambda x: F.split(x, '-')[1]\ndf = df.withColumn('city', parse_city(df.location))\n\n# Perform a date diff function\ndateDiff = lambda x, y: F.datediff(F.to_date(y), F.to_date(x))\ndf = df.withColumn('date_diff', dateDiff(df.start_date, df.end_date))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;df&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-23-9c72630481cb&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansired\"># Convert to a Date type</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> timestamp2datetype <span class=\"ansiyellow\">=</span> <span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> F<span class=\"ansiyellow\">.</span>to_date<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 6</span><span class=\"ansiyellow\"> </span>df <span class=\"ansiyellow\">=</span> df<span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;date&apos;</span><span class=\"ansiyellow\">,</span> timestamp2datetype<span class=\"ansiyellow\">(</span>df<span class=\"ansiyellow\">.</span>end_date<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      7</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      8</span> <span class=\"ansired\"># Parse out the date only</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;df&apos; is not defined\n</div>","startTime":1.447731938777E12,"submitTime":1.447731937377E12,"finishTime":1.447731939062E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"vida","iPythonMetadata":null,"nuid":"67b28a2a-b30e-400f-b7e1-fd0493b3d61f"},{"version":"CommandV1","origId":1525,"guid":"9a4d26f3-e1b6-49e0-90be-0976d6729b69","subtype":"command","commandType":"auto","position":4.0,"command":"df.registerTempTable(\"sample_df\")\ndisplay(sql(\"select * from sample_df\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["1","2015-10-14 00:00:00","2015-09-14 00:00:00","CA-SF","2015-10-14","2015-10-14","SF",30.0],["2","2015-10-15 01:00:20","2015-08-14 00:00:00","CA-SD","2015-10-15","2015-10-15","SD",62.0],["3","2015-10-16 02:30:00","2015-01-14 00:00:00","NY-NY","2015-10-16","2015-10-16","NY",275.0],["4","2015-10-17 03:00:20","2015-02-14 00:00:00","NY-NY","2015-10-17","2015-10-17","NY",245.0],["5","2015-10-18 04:30:00","2014-04-14 00:00:00","CA-SD","2015-10-18","2015-10-18","SD",552.0]],"arguments":{},"schema":[{"type":"string","name":"id"},{"type":"string","name":"end_date"},{"type":"string","name":"start_date"},{"type":"string","name":"location"},{"type":"date","name":"date"},{"type":"string","name":"date_only"},{"type":"string","name":"city"},{"type":"int","name":"date_diff"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":null,"startTime":1.447731942691E12,"submitTime":1.447731942347E12,"finishTime":1.447731944581E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"vida","iPythonMetadata":null,"nuid":"6de05694-4be6-47ef-b011-aa4b311dca3e"},{"version":"CommandV1","origId":1526,"guid":"159c29b0-2311-4376-8c5d-4542167060af","subtype":"command","commandType":"auto","position":6.0,"command":"%md\n**Q:** I want to convert the DataFrame back to json strings to send back to Kafka.  \n**A:** There is an underlying `toJSON()` function that returns an RDD of json strings using the column names and schema to product the json records. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"db8e1fdf-74bc-4369-a239-4fd545995204"},{"version":"CommandV1","origId":1527,"guid":"621a8da7-8026-4dba-a6ba-7a6d21986afd","subtype":"command","commandType":"auto","position":7.0,"command":"rdd_json = df.toJSON()\nrdd_json.take(2)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>\n[u&apos;{&quot;id&quot;:&quot;1&quot;,&quot;end_date&quot;:&quot;2015-10-14 00:00:00&quot;,&quot;start_date&quot;:&quot;2015-09-14 00:00:00&quot;,&quot;location&quot;:&quot;CA-SF&quot;,&quot;date&quot;:&quot;2015-10-14&quot;,&quot;date_only&quot;:&quot;2015-10-14&quot;,&quot;city&quot;:&quot;SF&quot;,&quot;date_diff&quot;:30}&apos;,\n u&apos;{&quot;id&quot;:&quot;2&quot;,&quot;end_date&quot;:&quot;2015-10-15 01:00:20&quot;,&quot;start_date&quot;:&quot;2015-08-14 00:00:00&quot;,&quot;location&quot;:&quot;CA-SD&quot;,&quot;date&quot;:&quot;2015-10-15&quot;,&quot;date_only&quot;:&quot;2015-10-15&quot;,&quot;city&quot;:&quot;SD&quot;,&quot;date_diff&quot;:62}&apos;]\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;RDD&apos; object has no attribute &apos;toJSON&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-34-f2b7120f6f2f&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>rdd_json <span class=\"ansiyellow\">=</span> df<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">.</span>toJSON<span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;RDD&apos; object has no attribute &apos;toJSON&apos;\n</div>","startTime":1.447191477801E12,"submitTime":1.447191468037E12,"finishTime":1.44719147798E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"3f297828-a2ab-4821-b12d-b7e0b815351d"},{"version":"CommandV1","origId":1528,"guid":"b8f1563d-375d-4064-b146-191e8b533c38","subtype":"command","commandType":"auto","position":8.0,"command":"%md \n**Q:** My UDF takes a parameter including the column to operate on. How do I pass this parameter?  \n**A:** There is a function available called lit() that creates a static column. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.446772777109E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"c3aa461c-1ef6-48a9-ac51-cad8bcc4c1e1"},{"version":"CommandV1","origId":1529,"guid":"15a1ca5c-e98a-406d-aa02-09dcfda754d2","subtype":"command","commandType":"auto","position":9.0,"command":"from pyspark.sql import functions as F\n\nadd_n = udf(lambda x, y: x + y, IntegerType())\n\n# We register a UDF that adds a column to the DataFrame, and we cast the id column to an Integer type. \ndf = df.withColumn('id_offset', add_n(F.lit(1000), df.id.cast(IntegerType())))\ndisplay(df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[["1","2015-10-14 00:00:00","2015-09-14 00:00:00","CA-SF","2015-10-14","2015-10-14","SF",30.0,1001.0],["2","2015-10-15 01:00:20","2015-08-14 00:00:00","CA-SD","2015-10-15","2015-10-15","SD",62.0,1002.0],["3","2015-10-16 02:30:00","2015-01-14 00:00:00","NY-NY","2015-10-16","2015-10-16","NY",275.0,1003.0],["4","2015-10-17 03:00:20","2015-02-14 00:00:00","NY-NY","2015-10-17","2015-10-17","NY",245.0,1004.0],["5","2015-10-18 04:30:00","2014-04-14 00:00:00","CA-LA","2015-10-18","2015-10-18","LA",552.0,1005.0]],"arguments":{},"schema":[{"type":"string","name":"id"},{"type":"string","name":"end_date"},{"type":"string","name":"start_date"},{"type":"string","name":"location"},{"type":"date","name":"date"},{"type":"string","name":"date_only"},{"type":"string","name":"city"},{"type":"int","name":"date_diff"},{"type":"int","name":"id_offset"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;UserDefinedFunction&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-35-7871e1e102c9&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">from</span> pyspark<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">.</span>functions <span class=\"ansigreen\">import</span> <span class=\"ansiyellow\">*</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 3</span><span class=\"ansiyellow\"> </span>plusone <span class=\"ansiyellow\">=</span> UserDefinedFunction<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">,</span> y<span class=\"ansiyellow\">:</span> x <span class=\"ansiyellow\">+</span> y<span class=\"ansiyellow\">,</span> IntegerType<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> abc <span class=\"ansiyellow\">=</span> udf<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> x <span class=\"ansiyellow\">+</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;UserDefinedFunction&apos; is not defined\n</div>","startTime":1.447192971E12,"submitTime":1.447192970299E12,"finishTime":1.447192975033E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"2e24602b-7720-4c32-9a2b-f3b792ae71f6"},{"version":"CommandV1","origId":1530,"guid":"16911e03-a764-4f2f-87b3-0d72fa28921f","subtype":"command","commandType":"auto","position":10.0,"command":"last_n_days = udf(lambda x, y: True if x < y else False, BooleanType())\n\ndf_filtered = df.filter(last_n_days(df.date_diff, F.lit(90)))\ndisplay(df_filtered)","commandVersion":0,"state":"finished","results":{"type":"table","data":[["1","2015-10-14 00:00:00","2015-09-14 00:00:00","CA-SF","2015-10-14","2015-10-14","SF",30.0,1001.0],["2","2015-10-15 01:00:20","2015-08-14 00:00:00","CA-SD","2015-10-15","2015-10-15","SD",62.0,1002.0]],"arguments":{},"schema":[{"type":"string","name":"id"},{"type":"string","name":"end_date"},{"type":"string","name":"start_date"},{"type":"string","name":"location"},{"type":"date","name":"date"},{"type":"string","name":"date_only"},{"type":"string","name":"city"},{"type":"int","name":"date_diff"},{"type":"int","name":"id_offset"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"<span class=\"ansired\">AttributeError</span>: type object &apos;bool&apos; has no attribute &apos;json&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-61-8a2f7d507c30&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>last_n_days <span class=\"ansiyellow\">=</span> udf<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">,</span> y<span class=\"ansiyellow\">:</span> True <span class=\"ansigreen\">if</span> x <span class=\"ansiyellow\">&gt;</span> y <span class=\"ansigreen\">else</span> False<span class=\"ansiyellow\">,</span> bool<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> df_filtered <span class=\"ansiyellow\">=</span> df<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span>last_n_days<span class=\"ansiyellow\">(</span>df<span class=\"ansiyellow\">.</span>date_diff<span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>lit<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">90</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> display<span class=\"ansiyellow\">(</span>df_filtered<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/functions.pyc</span> in <span class=\"ansicyan\">udf</span><span class=\"ansiblue\">(f, returnType)</span>\n<span class=\"ansigreen\">   1449</span>     <span class=\"ansiyellow\">[</span>Row<span class=\"ansiyellow\">(</span>slen<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">5</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>slen<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1450</span>     &quot;&quot;&quot;\n<span class=\"ansigreen\">-&gt; 1451</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> UserDefinedFunction<span class=\"ansiyellow\">(</span>f<span class=\"ansiyellow\">,</span> returnType<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1452</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1453</span> blacklist <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;map&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;since&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;ignore_unicode_prefix&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/functions.pyc</span> in <span class=\"ansicyan\">__init__</span><span class=\"ansiblue\">(self, func, returnType, name)</span>\n<span class=\"ansigreen\">   1411</span>         self<span class=\"ansiyellow\">.</span>returnType <span class=\"ansiyellow\">=</span> returnType<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1412</span>         self<span class=\"ansiyellow\">.</span>_broadcast <span class=\"ansiyellow\">=</span> None<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1413</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_judf <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_create_judf<span class=\"ansiyellow\">(</span>name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1414</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1415</span>     <span class=\"ansigreen\">def</span> _create_judf<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/functions.pyc</span> in <span class=\"ansicyan\">_create_judf</span><span class=\"ansiblue\">(self, name)</span>\n<span class=\"ansigreen\">   1421</span>         pickled_command<span class=\"ansiyellow\">,</span> broadcast_vars<span class=\"ansiyellow\">,</span> env<span class=\"ansiyellow\">,</span> includes <span class=\"ansiyellow\">=</span> _prepare_for_python_RDD<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> command<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1422</span>         ssql_ctx <span class=\"ansiyellow\">=</span> sc<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>SQLContext<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">.</span>_jsc<span class=\"ansiyellow\">.</span>sc<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1423</span><span class=\"ansiyellow\">         </span>jdt <span class=\"ansiyellow\">=</span> ssql_ctx<span class=\"ansiyellow\">.</span>parseDataType<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>returnType<span class=\"ansiyellow\">.</span>json<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1424</span>         <span class=\"ansigreen\">if</span> name <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1425</span>             name <span class=\"ansiyellow\">=</span> f<span class=\"ansiyellow\">.</span>__name__ <span class=\"ansigreen\">if</span> hasattr<span class=\"ansiyellow\">(</span>f<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;__name__&apos;</span><span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">else</span> f<span class=\"ansiyellow\">.</span>__class__<span class=\"ansiyellow\">.</span>__name__<span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: type object &apos;bool&apos; has no attribute &apos;json&apos;\n</div>","startTime":1.44719147874E12,"submitTime":1.447191472147E12,"finishTime":1.447191479155E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"3eaf2149-d50c-4dd3-a04d-5ffdc82c0c2b"},{"version":"CommandV1","origId":1531,"guid":"3751075a-f092-46da-9ad9-0628aadfbd43","subtype":"command","commandType":"auto","position":12.0,"command":"%md \n**Q:** I have a table in the hive metastore and I'd like to access to table as a DataFrame. What's the best way to define this?  \n**A:** There's multiple ways to define a DataFrame from a registered table. Syntax show below.  \nCall `table(tableName)` or select and filter specific columns using an SQL query. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.447121414334E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"6127b2fe-baed-4504-b022-7e41f205098a"},{"version":"CommandV1","origId":1532,"guid":"b076d81a-798d-4257-bae9-a7e9a2fcc5d5","subtype":"command","commandType":"auto","position":13.0,"command":"# Both return DataFrame types\ndf_1 = table(\"sample_df\")\ndf_2 = sqlContext.sql(\"select * from sample_df\") ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.447191479159E12,"submitTime":1.44719147596E12,"finishTime":1.447191479236E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"26d78ac0-0fd0-4ea6-b9ba-331f222701cf"},{"version":"CommandV1","origId":1533,"guid":"c266e809-d35d-4fc5-926c-f44fbf7a7a89","subtype":"command","commandType":"auto","position":14.0,"command":"%md\n**Q:** I'd like to clear all the cached tables on the current cluster.  \n**A:** There's an API available to do this. This can be done at a global level or per table. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"b10d863d-fc26-4081-81c2-251db4075ce5"},{"version":"CommandV1","origId":1534,"guid":"3dd06249-5204-46b8-97a9-9667bc6ae840","subtype":"command","commandType":"auto","position":15.0,"command":"sqlContext.clearCache()\nsqlContext.cacheTable(\"sample_df\")\nsqlContext.uncacheTable(\"sample_df\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.447191491949E12,"submitTime":1.447191491359E12,"finishTime":1.447191492476E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"02e433aa-2fac-4221-bdd5-7ad5d2ebcd37"},{"version":"CommandV1","origId":1535,"guid":"43fa3ed9-b516-4a20-97d9-a304243e4242","subtype":"command","commandType":"auto","position":16.0,"command":"%md \n**Q:** I'd like to compute aggregates on columns. What's the best way to do this?  \n**A:** There's a new API available named `agg(*exprs)` that takes a list of column names and expressions for the type of aggregation you'd like to compute. Documentation can be found [here](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html). You can leverage the built-in functions that were mentioned above as part of the expressions for each column. \n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"5b79438a-a73e-4f39-941d-e74a44426b75"},{"version":"CommandV1","origId":1536,"guid":"31ee127e-ecb4-41a6-aece-601484f79716","subtype":"command","commandType":"auto","position":16.25,"command":"# Provide the min, count, and avg and groupBy the location column. Diplay the results \nagg_df = df.groupBy(\"location\").agg(F.min(\"id\"), F.count(\"id\"), F.avg(\"date_diff\"))\ndisplay(agg_df)","commandVersion":0,"state":"finished","results":{"type":"table","data":[["CA-LA","5",1.0,552.0],["NY-NY","3",2.0,260.0],["CA-SD","2",1.0,62.0],["CA-SF","1",1.0,30.0]],"arguments":{},"schema":[{"type":"string","name":"location"},{"type":"string","name":"min(id)"},{"type":"bigint","name":"count(id)"},{"type":"double","name":"avg(date_diff)"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;DataFrame&apos; object has no attribute &apos;grouBy&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-22-3f0bceff0004&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>agg_df <span class=\"ansiyellow\">=</span> df<span class=\"ansiyellow\">.</span>grouBy<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;location&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>agg<span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>min<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;id&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>avg<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;date_diff&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> display<span class=\"ansiyellow\">(</span>agg_df<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/dataframe.pyc</span> in <span class=\"ansicyan\">__getattr__</span><span class=\"ansiblue\">(self, name)</span>\n<span class=\"ansigreen\">    747</span>         <span class=\"ansigreen\">if</span> name <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">in</span> self<span class=\"ansiyellow\">.</span>columns<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    748</span>             raise AttributeError(\n<span class=\"ansigreen\">--&gt; 749</span><span class=\"ansiyellow\">                 &quot;&apos;%s&apos; object has no attribute &apos;%s&apos;&quot; % (self.__class__.__name__, name))\n</span><span class=\"ansigreen\">    750</span>         jc <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>apply<span class=\"ansiyellow\">(</span>name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    751</span>         <span class=\"ansigreen\">return</span> Column<span class=\"ansiyellow\">(</span>jc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;DataFrame&apos; object has no attribute &apos;grouBy&apos;\n</div>","startTime":1.447192066094E12,"submitTime":1.447192065527E12,"finishTime":1.447192069816E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"b5c97b48-dab1-4467-963a-49f46cd5244a"},{"version":"CommandV1","origId":1537,"guid":"df4efe83-436b-42d9-b3ae-9bd61f702658","subtype":"command","commandType":"auto","position":16.5,"command":"%md \n**Q:** I'd like to write out the DataFrames to Parquet, but would like to partition on a particular column.  \n**A:** You can use the following APIs to accomplish this. Ensure the code does not create too many partition columns with the datasets otherwise the overhead of the metadata can cause queries to slow down significantly. If there is a SQL table back by this directory, users will need to call `refresh table _tableName_` to update the metadata prior to the query. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"f2be2eff-a6fb-42cc-a0c1-0d3952af894b"},{"version":"CommandV1","origId":1538,"guid":"c98e9ed7-aabb-4a7e-a0bb-020efc8ab07a","subtype":"command","commandType":"auto","position":16.625,"command":"df = df.withColumn('end_month', F.month('end_date'))\ndf = df.withColumn('end_year', F.year('end_date'))\ndf.write.partitionBy(\"end_year\", \"end_month\").parquet(\"/tmp/sample_table\")\ndisplay(dbutils.fs.ls(\"/tmp/sample_table\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/tmp/sample_table/_SUCCESS","_SUCCESS",0.0],["dbfs:/tmp/sample_table/_common_metadata","_common_metadata",893.0],["dbfs:/tmp/sample_table/_metadata","_metadata",2228.0],["dbfs:/tmp/sample_table/end_year=2015/","end_year=2015/",0.0]],"arguments":{},"schema":[{"type":"string","name":"path"},{"type":"string","name":"name"},{"type":"bigint","name":"size"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"<span class=\"ansired\">AssertionError</span>: col should be Column","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-27-d3a0c263e067&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>df <span class=\"ansiyellow\">=</span> df<span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>month<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;end_date&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;month&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> df <span class=\"ansiyellow\">=</span> df<span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>year<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;end_date&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;year&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> display<span class=\"ansiyellow\">(</span>df<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansired\">#df.write.partitionBy(&quot;year&quot;, &quot;month&quot;).</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/dataframe.pyc</span> in <span class=\"ansicyan\">withColumn</span><span class=\"ansiblue\">(self, colName, col)</span>\n<span class=\"ansigreen\">   1216</span>         <span class=\"ansiyellow\">[</span>Row<span class=\"ansiyellow\">(</span>age<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;Alice&apos;</span><span class=\"ansiyellow\">,</span> age2<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">4</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>age<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">5</span><span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;Bob&apos;</span><span class=\"ansiyellow\">,</span> age2<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">7</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1217</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">-&gt; 1218</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">assert</span> isinstance<span class=\"ansiyellow\">(</span>col<span class=\"ansiyellow\">,</span> Column<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;col should be Column&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1219</span>         <span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span>colName<span class=\"ansiyellow\">,</span> col<span class=\"ansiyellow\">.</span>_jc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>sql_ctx<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1220</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AssertionError</span>: col should be Column\n</div>","startTime":1.44719397048E12,"submitTime":1.447193969919E12,"finishTime":1.447193975926E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"b4d1aecc-f6df-4abc-8406-d099165ceda3"},{"version":"CommandV1","origId":1539,"guid":"76bd4942-54e4-4577-b9a3-fa597a3860d1","subtype":"command","commandType":"auto","position":16.75,"command":"%md \n**Q:** How do I properly handle cases where I want to filter out NULL data?  \n**A:** You can use _filter()_ and provide similar syntax as you would with a SQL query. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"a87f26c9-a82b-41b1-a32f-3bd14a792f09"},{"version":"CommandV1","origId":1540,"guid":"510757c9-6832-4ba7-b464-31e30c1f4633","subtype":"command","commandType":"auto","position":16.875,"command":"null_item_schema = StructType([StructField(\"col1\", StringType(), True),\n                               StructField(\"col2\", IntegerType(), True)])\nnull_df = sqlContext.createDataFrame([(\"test\", 1), (None, 2)], null_item_schema)\ndisplay(null_df.filter(\"col1 IS NOT NULL\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["test",1.0]],"arguments":{},"schema":[{"type":"string","name":"col1"},{"type":"int","name":"col2"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"java.lang.RuntimeException: [1.9] failure: &#96;&#96;in&apos;&apos; expected but &#96;null&apos; found","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-8-decf344056c3&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> null_item_schema <span class=\"ansiyellow\">=</span> StructType<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span>StructField<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;col1&quot;</span><span class=\"ansiyellow\">,</span> StringType<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> True<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> StructField<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;col2&quot;</span><span class=\"ansiyellow\">,</span> IntegerType<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> True<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> null_df <span class=\"ansiyellow\">=</span> sqlContext<span class=\"ansiyellow\">.</span>createDataFrame<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;test&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>None<span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> null_item_schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 3</span><span class=\"ansiyellow\"> </span>null_df<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot; IS NOT NULL&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">filter</span><span class=\"ansiblue\">(self, condition)</span>\n<span class=\"ansigreen\">    895</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    896</span>         <span class=\"ansigreen\">if</span> isinstance<span class=\"ansiyellow\">(</span>condition<span class=\"ansiyellow\">,</span> basestring<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 897</span><span class=\"ansiyellow\">             </span>jdf <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span>condition<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    898</span>         <span class=\"ansigreen\">elif</span> isinstance<span class=\"ansiyellow\">(</span>condition<span class=\"ansiyellow\">,</span> Column<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    899</span>             jdf <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span>condition<span class=\"ansiyellow\">.</span>_jc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/python/local/lib/python2.7/site-packages/py4j/java_gateway.pyc</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">    536</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    537</span>         return_value = get_return_value(answer, self.gateway_client,\n<span class=\"ansigreen\">--&gt; 538</span><span class=\"ansiyellow\">                 self.target_id, self.name)\n</span><span class=\"ansigreen\">    539</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    540</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     43</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     44</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 45</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     46</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     47</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/python/local/lib/python2.7/site-packages/py4j/protocol.pyc</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    298</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    299</span>                     <span class=\"ansiblue\">&apos;An error occurred while calling {0}{1}{2}.\\n&apos;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 300</span><span class=\"ansiyellow\">                     format(target_id, &apos;.&apos;, name), value)\n</span><span class=\"ansigreen\">    301</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    302</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o306.filter.\n: java.lang.RuntimeException: [1.9] failure: &#96;&#96;in&apos;&apos; expected but &#96;null&apos; found\n\n IS NOT NULL\n        ^\n\tat scala.sys.package$.error(package.scala:27)\n\tat org.apache.spark.sql.catalyst.SqlParser$.parseExpression(SqlParser.scala:48)\n\tat org.apache.spark.sql.DataFrame.filter(DataFrame.scala:818)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n\n</div>","startTime":1.44720165857E12,"submitTime":1.447201659663E12,"finishTime":1.447201658856E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"d404592d-eb4d-410e-8ba8-fd7ad893803e"},{"version":"CommandV1","origId":1541,"guid":"db8a7ab8-a040-4ebf-aa50-1c9c4a5be27b","subtype":"command","commandType":"auto","position":17.0,"command":"%md \n**Q:** How do I infer the schema using the spark-csv or spark-avro libraries?  \n**A:** Documented on the GitHub projects [spark-csv](https://github.com/databricks/spark-csv), there is an inferSchema option flag. Providing a header would allow the columns to be named appropriately. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"5bb5d803-8445-46c3-a9b5-bdefe05ff482"},{"version":"CommandV1","origId":1542,"guid":"23cd76fb-9d88-4347-b7f8-0a64df2176c1","subtype":"command","commandType":"auto","position":17.5,"command":"adult_df = sqlContext.read.\\\n    format(\"com.databricks.spark.csv\").\\\n    option(\"header\", \"false\").\\\n    option(\"inferSchema\", \"true\").load(\"dbfs:/databricks-datasets/adult/adult.data\")\nadult_df.printSchema()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- C0: integer (nullable = true)\n |-- C1: string (nullable = true)\n |-- C2: double (nullable = true)\n |-- C3: string (nullable = true)\n |-- C4: double (nullable = true)\n |-- C5: string (nullable = true)\n |-- C6: string (nullable = true)\n |-- C7: string (nullable = true)\n |-- C8: string (nullable = true)\n |-- C9: string (nullable = true)\n |-- C10: double (nullable = true)\n |-- C11: double (nullable = true)\n |-- C12: double (nullable = true)\n |-- C13: string (nullable = true)\n |-- C14: string (nullable = true)\n\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">IndentationError</span><span class=\"ansired\">:</span> unexpected indent","error":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;ipython-input-19-4661b5b644c5&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">2</span>\n<span class=\"ansiyellow\">    .format(&quot;com.databricks.spark.csv&quot;)</span>\n<span class=\"ansigrey\">    ^</span>\n<span class=\"ansired\">IndentationError</span><span class=\"ansired\">:</span> unexpected indent\n\nIf you want to paste code into IPython, try the %paste and %cpaste magic functions.\n</div>","startTime":1.447377875435E12,"submitTime":1.447377877532E12,"finishTime":1.447377884597E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"3b59ebca-3344-4a36-a9fc-e34ace631b67"},{"version":"CommandV1","origId":1543,"guid":"7884b46a-2456-4458-98ca-5a2601396aab","subtype":"command","commandType":"auto","position":18.0,"command":"%md \n**Q:** You have a delimited string dataset that you want to convert to their datatypes. How would you accomplish this?  \n**A:** Use the RDD APIs to filter out the malformed rows and map the values to the appropriate types. We define a function that filters the items using regular expressions.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"ffc06807-a7fb-4ba2-ab59-3ca892b2e66e"},{"version":"CommandV1","origId":1544,"guid":"0821b1ad-5a90-4d27-845e-874add724cbe","subtype":"command","commandType":"auto","position":18.5,"command":"from pyspark.sql.types import *\nfrom datetime import datetime\nfrom decimal import Decimal\nimport re\n\nrdd = sc.newAPIHadoopFile(\"/databricks-datasets/tpch/data-001/orders/orders.tbl\", \n                          \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\n                          \"org.apache.hadoop.io.LongWritable\", \"org.apache.hadoop.io.Text\",\n                          conf={\"textinputformat.record.delimiter\": \"\\n\"}).map(lambda l:l[1])\n\n# Convert DF from an RDD and apply encoding to ascii from unicode. This will help with the TimestampType conversion in the schema. \ndef dataframe_type_conversion(elem):\n  is_date = re.match('(\\d+)[\\-](\\d+)[\\-](\\d+).*$', elem)\n  is_int = re.match('^\\d+$', elem)\n  is_dec = re.match('^[-]?\\d+\\.\\d+', elem)\n  if not elem:\n    return None\n  # If match is a date type\n  if (is_date is not None) and (len(elem) < 11):\n    return datetime.strptime(elem, \"%Y-%m-%d\")\n  # If match is int \n  elif (is_int is not None):\n    return int(elem)\n  elif (is_dec is not None):\n    return Decimal(elem)\n  else:\n    return elem\n\n# To filter out bad records, you could add a step to verify that there are the correct number of delimiters per record to filter them out\nbad_records = rdd.filter(lambda r: r.count('|') < 9 )\nnew_rdd = rdd.filter(lambda r: r.count('|') == 9).\\\n              map(lambda x: x.split(\"|\")).\\\n              map(lambda x: [dataframe_type_conversion(e.encode(\"ascii\", \"replace\")) for e in x])\n\n# Schema for lineitems of the TPCH dataset\norders_schema = StructType ( [StructField('OrderKey', IntegerType(), True),\nStructField('CustKey', IntegerType(), True),\nStructField('OrderStatus', StringType(), True),\nStructField('TotalPrice', StringType(), True),\nStructField('OrderDate', StringType(), True),\nStructField('OrderPriority', StringType(), True),\nStructField('Clerk', StringType(), True),\nStructField('ShipPriority', IntegerType(), True),\nStructField('Comment', StringType(), True),\nStructField('skip', StringType(), True)] )\n\ndf_orders = sqlContext.createDataFrame(new_rdd, orders_schema)\ndf_orders.printSchema()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- OrderKey: integer (nullable = true)\n |-- CustKey: integer (nullable = true)\n |-- OrderStatus: string (nullable = true)\n |-- TotalPrice: string (nullable = true)\n |-- OrderDate: string (nullable = true)\n |-- OrderPriority: string (nullable = true)\n |-- Clerk: string (nullable = true)\n |-- ShipPriority: integer (nullable = true)\n |-- Comment: string (nullable = true)\n |-- skip: string (nullable = true)\n\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">ValueError</span>: Length of object (10) does not match with length of fields (17)","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-15-b9a504169829&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     42</span> StructField(&apos;skip&apos;, StringType(), True)] )\n<span class=\"ansigreen\">     43</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 44</span><span class=\"ansiyellow\"> </span>df_lineitems <span class=\"ansiyellow\">=</span> sqlContext<span class=\"ansiyellow\">.</span>createDataFrame<span class=\"ansiyellow\">(</span>new_rdd<span class=\"ansiyellow\">,</span> lineitem_schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/context.pyc</span> in <span class=\"ansicyan\">createDataFrame</span><span class=\"ansiblue\">(self, data, schema, samplingRatio)</span>\n<span class=\"ansigreen\">    402</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    403</span>         <span class=\"ansigreen\">if</span> isinstance<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> RDD<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 404</span><span class=\"ansiyellow\">             </span>rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromRDD<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">,</span> samplingRatio<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    405</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    406</span>             rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromLocal<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/context.pyc</span> in <span class=\"ansicyan\">_createFromRDD</span><span class=\"ansiblue\">(self, rdd, schema, samplingRatio)</span>\n<span class=\"ansigreen\">    296</span>             rows <span class=\"ansiyellow\">=</span> rdd<span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">10</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    297</span>             <span class=\"ansigreen\">for</span> row <span class=\"ansigreen\">in</span> rows<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 298</span><span class=\"ansiyellow\">                 </span>_verify_type<span class=\"ansiyellow\">(</span>row<span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    299</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    300</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/types.pyc</span> in <span class=\"ansicyan\">_verify_type</span><span class=\"ansiblue\">(obj, dataType)</span>\n<span class=\"ansigreen\">   1150</span>         <span class=\"ansigreen\">if</span> len<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">!=</span> len<span class=\"ansiyellow\">(</span>dataType<span class=\"ansiyellow\">.</span>fields<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1151</span>             raise ValueError(&quot;Length of object (%d) does not match with &quot;\n<span class=\"ansigreen\">-&gt; 1152</span><span class=\"ansiyellow\">                              &quot;length of fields (%d)&quot; % (len(obj), len(dataType.fields)))\n</span><span class=\"ansigreen\">   1153</span>         <span class=\"ansigreen\">for</span> v<span class=\"ansiyellow\">,</span> f <span class=\"ansigreen\">in</span> zip<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">,</span> dataType<span class=\"ansiyellow\">.</span>fields<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1154</span>             _verify_type<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">,</span> f<span class=\"ansiyellow\">.</span>dataType<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ValueError</span>: Length of object (10) does not match with length of fields (17)\n</div>","startTime":1.447206687045E12,"submitTime":1.447206686876E12,"finishTime":1.447206687593E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"5130abb8-c90c-4beb-8b8c-9beb93ac807d"}],"guid":"20954647-767c-4391-bb68-345a70fd458a","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"2c0b0fc3-337f-4d86-9f4a-7c6ab90aee2e","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>