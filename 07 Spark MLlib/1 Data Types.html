<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Spark MLlib / Data Types - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":2222,"name":"Spark MLlib / Data Types","language":"python","commands":[{"version":"CommandV1","origId":2223,"guid":"f78ecfd6-397a-4d39-a5d4-fae3a4bdbf52","subtype":"command","commandType":"auto","position":1.0,"command":"%md # ML Data Types and Datasets\n\nMLlib algorithms use data types specialized for particular Machine Learning tasks.  This guide gives an overview of best practices and important types.\n\nFor more details, see the [Spark programming guide on Data Types](http://spark.apache.org/docs/latest/mllib-data-types.html).\n\n**Contents**\n* Best practices\n* Types for various tasks\n* Datasets\n* Linear algebra types","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.427766271505E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"5039e35a-faee-457b-9ec3-34927a9dde33"},{"version":"CommandV1","origId":2224,"guid":"cb06f2db-6fb3-4f7e-aaf3-7018073e78c1","subtype":"command","commandType":"auto","position":1.25,"command":"%md ## Best practices\n\nIn general, preprocessing should be done before converting to ML-specific types.  E.g., to use [LogisticRegression](http://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression), you should:\n1. Load data as an RDD (or DataFrame)\n2. Transform the RDD to preprocess the data\n  * For preprocessing, [Spark DataFrames](http://spark.apache.org/docs/latest/sql-programming-guide.html) are very helpful.\n  * Some ML-specific feature transformations are supported within MLlib itself and operate on Vector types.\n3. Create an RDD of feature vectors (RDD[Vector]) and an RDD of labels (RDD[float or Double])\n4. Zip the feature and label RDDs together and map them into an RDD[LabeledPoint]\n5. Pass that data to Logistic Regression\n\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.427764047982E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"5435229a-b54a-4f22-b4f6-bedcd737e3de"},{"version":"CommandV1","origId":2225,"guid":"57d76e84-d3a9-4132-ae53-55142cc524d0","subtype":"command","commandType":"auto","position":1.5,"command":"%md ## Types for various tasks\n\nThis section lists a few key types which are used for particular tasks.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.427764067757E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"da159121-c653-4482-b57b-398221eb8080"},{"version":"CommandV1","origId":2226,"guid":"dca83152-3b26-4441-824c-b9b21822de2a","subtype":"command","commandType":"auto","position":2.0,"command":"%md ### Feature vectors\n\n`Vector` is a real-valued vector, often used to store feature vectors.\n* `Vector` has dense and sparse specializations: `DenseVector` and `SparseVector`.\n* In Python, you can also use Numpy arrays, lists, or scipy.sparse column vectors.\n\n*See the \"Linear algebra types\" section below for links to API references.*","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.427765101154E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"32a61c6e-43c6-4019-8582-22f7da9caea2"},{"version":"CommandV1","origId":2227,"guid":"85bd6357-a81e-4e2d-8370-17a9e1b57ce1","subtype":"command","commandType":"auto","position":3.0,"command":"%md ### Prediction\n\n`LabeledPoint` stores a label + feature vector for classification and regression tasks.  It has 2 fields:\n  * `LabeledPoint.label`: integer or real-valued label to predict\n  * `LabeledPoint.features`: feature vector\n\nAPI references: [Python API](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint), [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.427764714991E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"82f354f0-222d-4b4c-b075-1dd44b87aed5"},{"version":"CommandV1","origId":2228,"guid":"ac612d74-2c0e-4278-aa01-d3cd5af826a2","subtype":"command","commandType":"auto","position":4.0,"command":"%md ### Recommendation (collaborative filtering)\n\n`Rating` stores one rating for recommendation (collaborative filtering).  It has 3 fields:\n* `Rating.user`: user who gave the rating\n* `Rating.product`: product or item which the user rated\n* `Rating.rating`: numerical rating given\n\nAPI references: [Python API](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.Rating), [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.recommendation.Rating)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.427764839718E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"613dea6a-51f1-484b-8c92-6610b93b53fc"},{"version":"CommandV1","origId":2229,"guid":"c493e058-51aa-4246-a5b6-0a33d8017765","subtype":"command","commandType":"auto","position":4.5,"command":"%md ## Datasets\n\nMLlib algorithms and models (in the `mllib` package) take `RDD`s as datasets.  E.g., `pyspark.mllib.classification.LogisticRegressionWithSGD` takes an `RDD` of `LabeledPoint`.\n\nIn order to take advantage of the *`DataFrame` API*, we recommend that users use `DataFrame` to preprocess data before creating an `RDD` to pass to `mllib` algorithms.\n\n#### Pipelines API\n\nThere is an experimental *\"Pipelines API\"* package available in the `ml` package.  This package provides a higher-level API for Machine Learning, including wrappers for some of the algorithms and models in the `mllib` package.  These wrappers take `DataFrame` as a dataset, rather than an `RDD`.\n\nFor more info on this experimental `ml` package, see the [ML package guide](http://spark.apache.org/docs/latest/ml-guide.html).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.427767023609E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"38ba2ccd-52fb-4283-b218-ddfb92502a54"},{"version":"CommandV1","origId":2230,"guid":"2197cecc-4bc9-4441-9f63-86ef1c2444bc","subtype":"command","commandType":"auto","position":5.0,"command":"%md ## Linear algebra types\n\nMLlib supports several types for linear algebra.  We list several types here but refer readers to the [Spark programming guide on Data Types](http://spark.apache.org/docs/latest/mllib-data-types.html) for more details.\n\nAPI references: [Python API](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.linalg), [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.package)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.427764993758E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"90d68250-910a-49d2-b33f-656d334096a7"},{"version":"CommandV1","origId":2231,"guid":"533cb69e-dc50-44cd-8b53-538d315f8671","subtype":"command","commandType":"auto","position":6.0,"command":"%md ### Local vector and matrix types\n\n#### Vectors\n\nMLlib supports dense and sparse vectors via the `Vector`, `DenseVector`, and `SparseVector` types.  In Python, you can also use Numpy arrays, lists, or scipy.sparse column vectors.\n\nThe `Vectors` object provides factory methods for constructing vectors.\n\nAPI references\n* `Vector`: [Python API](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vector), [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Vector)\n* `DenseVector`: [Python API](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseVector), [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.DenseVector)\n* `SparseVector`: [Python API](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector), [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SparseVector)\n* `Vectors` factory methods: [Python API](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors), [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Vectors$)\n\n#### Matrices\n\nMLlib supports dense and sparse matrices via the `Matrix`, `DenseMatrix`, and `SparseMatrix` types.  In Python, we recommend using Numpy and SciPy's types for local matrices.\n\nThe `Matrices` object provides factory methods for constructing matrices.\n\nAPI references\n* `Matrix`: abstract Matrix type\n* `DenseMatrix`: [Python API](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseMatrix), [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.DenseMatrix)\n* `SparseMatrix`: (no Python API; use `scipy.sparse` instead), [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SparseMatrix)\n* `Matrices` factory methods: [Python API](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrices), [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Matrices$)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.428028773542E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"joseph","iPythonMetadata":null,"nuid":"58a76aab-b329-4f78-8a89-80a73a70151c"},{"version":"CommandV1","origId":2232,"guid":"b61e5413-3e22-4f1a-94bf-1ebf1b1be846","subtype":"command","commandType":"auto","position":7.0,"command":"%md ### Distributed matrix types\n\nMLlib supports 4 types of distributed matrices, for different purposes.  Refer to the [MLlib Data Types Programming Guide](http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix) when choosing a type.\n\n*Python: These types are not yet supported in Python.  You must use them via Scala notebooks.*\n\nThese matrices all support sparse storage, either via `Vector` types or via coordinate-wise storage.\n\n`BlockMatrix`: block-structured matrix\n  * [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.BlockMatrix)\n\n`RowMatrix`: row-partitioned matrix\n  * [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix)\n\n`IndexedRowMatrix`: row-partitioned matrix with row indices\n  * [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix)\n\n`CoordinateMatrix`: coordinate-wise storage (row, col, value)\n  * [Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.CoordinateMatrix)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.42776614135E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"f3aef410-292c-4822-a0e7-2b368d34e0d2"}],"guid":"5f5547b4-4c13-41a9-9f73-41df9f3f707d","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"f15b9d11-e65b-4161-bec0-496399983486","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>