<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Accessing Data / Common File Formats / Avro - py - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":1198,"name":"Accessing Data / Common File Formats / Avro - py","language":"python","commands":[{"version":"CommandV1","origId":1199,"guid":"1fe68276-e1ca-4e41-920b-b1a515bf2b03","subtype":"command","commandType":"auto","position":0.5,"command":"%md\n\n# **Avro in Python**\nThis notebook describes how to read Avro files into Spark using PySpark.\n\nWe will use the example Avro files generated by the Scala Avro notebook. \nThe schema is the following:\n```\n{\"namespace\": \"example.avro\",\n \"type\": \"record\",\n \"name\": \"User\",\n \"fields\": [\n     {\"name\": \"name\", \"type\": \"string\"},\n     {\"name\": \"favorite_number\",  \"type\": [\"int\", \"null\"]},\n     {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n ]\n}\n```\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"8d0dd614-2ea3-4ead-b0f9-c0ccdcf8a826"},{"version":"CommandV1","origId":1200,"guid":"b6f4090d-4469-4f22-92d0-0cea751aa344","subtype":"command","commandType":"auto","position":0.875,"command":"%md ### **Setup:** Write a test Avro file.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"467eee3d-9219-490c-8a35-5df040890111"},{"version":"CommandV1","origId":1201,"guid":"24806a6f-e274-4784-8b12-ee624cd2b1ac","subtype":"command","commandType":"auto","position":1.0625,"command":"%md\n\nSee the scala version of this notebook and hit **Run All** to create the dbfs:/tmp/users/avro file.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"ba1d9ca7-20c4-4c56-9d7f-277b5e7ff69c"},{"version":"CommandV1","origId":1202,"guid":"1260ddbf-3712-40a1-8245-99888bb8b8d5","subtype":"command","commandType":"auto","position":1.25,"command":"%md \n%md ### **Option 1:** Use Spark Avro package to read and write Avro files.\n\nThe open source code for Spark Avro is here: [Spark-Avro GitHub](https://github.com/databricks/spark-avro)  \n\nThe example below covers examples reading directly from a single avro file, or from a directory of avro files that contain the same schema. This loads the avro file into a DataFrame. \nThe library assumes all the avro files contained within a directory contain the same schema. Loading a directory with multiple schemas will choose the first schema read to apply to the rest of the files. ","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.444664520617E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"6105d084-72e2-47ba-86bf-6bcbfdc94e83"},{"version":"CommandV1","origId":1203,"guid":"519bc56b-0d9d-49d3-b3d0-21cc768110d4","subtype":"command","commandType":"auto","position":1.5,"command":"df = sqlContext.read.format(\"com.databricks.spark.avro\").load(\"dbfs:/tmp/users.avro\")\ndf_partitioned = sqlContext.read.format(\"com.databricks.spark.avro\").load(\"dbfs:/tmp/test_dataset\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":"java.io.FileNotFoundException: The path (dbfs:/tmp/users.avro) is invalid.","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-1-9eff2fe0e3ad&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>df <span class=\"ansiyellow\">=</span> sqlContext<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>format<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;com.databricks.spark.avro&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>load<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;dbfs:/tmp/users.avro&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/readwriter.pyc</span> in <span class=\"ansicyan\">load</span><span class=\"ansiblue\">(self, path, format, schema, **options)</span>\n<span class=\"ansigreen\">    119</span>         self<span class=\"ansiyellow\">.</span>options<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">**</span>options<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    120</span>         <span class=\"ansigreen\">if</span> path <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 121</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_df<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jreader<span class=\"ansiyellow\">.</span>load<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    122</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    123</span>             <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_df<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jreader<span class=\"ansiyellow\">.</span>load<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">    536</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    537</span>         return_value = get_return_value(answer, self.gateway_client,\n<span class=\"ansigreen\">--&gt; 538</span><span class=\"ansiyellow\">                 self.target_id, self.name)\n</span><span class=\"ansigreen\">    539</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    540</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     34</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     35</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 36</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     37</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     38</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    298</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    299</span>                     <span class=\"ansiblue\">&apos;An error occurred while calling {0}{1}{2}.\\n&apos;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 300</span><span class=\"ansiyellow\">                     format(target_id, &apos;.&apos;, name), value)\n</span><span class=\"ansigreen\">    301</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    302</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o82.load.\n: java.io.FileNotFoundException: The path (dbfs:/tmp/users.avro) is invalid.\n\tat com.databricks.spark.avro.AvroRelation.newReader(AvroRelation.scala:216)\n\tat com.databricks.spark.avro.AvroRelation.avroSchema$lzycompute(AvroRelation.scala:54)\n\tat com.databricks.spark.avro.AvroRelation.avroSchema(AvroRelation.scala:53)\n\tat com.databricks.spark.avro.AvroRelation.dataSchema(AvroRelation.scala:67)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:561)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:560)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.&lt;init&gt;(LogicalRelation.scala:37)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:120)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:104)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n\n</div>","startTime":1.444843233435E12,"submitTime":1.444843232987E12,"finishTime":1.444843234015E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"a9253815-e39c-4c10-9f2f-07c05a789bb6"},{"version":"CommandV1","origId":1204,"guid":"af490ba7-167d-4f4d-bac7-f1ade7304c3f","subtype":"command","commandType":"auto","position":1.9375,"command":"%md \n#### Setting Configuration Options\nYou can Specify Avro compression options by setting the configurations inside the SparkSql context.  \nCompression Codec Options:  \n* uncompressed (default)\n* snappy \n* deflate\n\nDefault deflate level is -1. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"1ff06cbb-c47a-4f66-a452-1859abb6714c"},{"version":"CommandV1","origId":1205,"guid":"5e692e59-7d0b-46e1-94cf-bb6d393458e1","subtype":"command","commandType":"auto","position":1.96875,"command":"sqlContext.setConf(\"spark.sql.avro.compression.codec\", \"uncompressed\")\nsqlContext.setConf(\"spark.sql.avro.deflate.level\", \"-1\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.444666757159E12,"submitTime":1.444666756936E12,"finishTime":1.444666757193E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"0b06097c-affc-4fa9-abac-2d108faae5b0"},{"version":"CommandV1","origId":1206,"guid":"a08f8d05-c387-416f-8094-e77effea0ed7","subtype":"command","commandType":"auto","position":1.970703125,"command":"%md\nSpecify the record name and namespace using a dictionary for the mapping. \n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"b0f46429-8b1f-44d7-ba15-e8dc9dd5eb02"},{"version":"CommandV1","origId":1207,"guid":"7080b505-e0a6-4a8c-8090-029c404babc5","subtype":"command","commandType":"auto","position":1.9765625,"command":"name = \"AvroTest\"\nnamespace = \"com.databricks.spark.avro\"\nparameters = {\"recordName\" : name, \"recordNamespace\" : namespace}\n\ndf.write.format(\"com.databricks.spark.avro\").save(\"/tmp/mwc/testData\", options=parameters)","commandVersion":0,"state":"error","results":null,"errorSummary":"py4j.Py4JException: Method option([class java.lang.String, class java.util.HashMap]) does not exist","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-49-b87783ce9ebc&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      3</span> parameters <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">{</span><span class=\"ansiblue\">&quot;recordName&quot;</span> <span class=\"ansiyellow\">:</span> name<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;recordNamespace&quot;</span> <span class=\"ansiyellow\">:</span> namespace<span class=\"ansiyellow\">}</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 5</span><span class=\"ansiyellow\"> </span>df<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">.</span>format<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;com.databricks.spark.avro&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>save<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;/tmp/mwc/testData&quot;</span><span class=\"ansiyellow\">,</span> options<span class=\"ansiyellow\">=</span>parameters<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansigreen\">    322</span>         <span class=\"ansiyellow\">&gt;&gt;</span><span class=\"ansiyellow\">&gt;</span> df<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">.</span>mode<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;append&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>parquet<span class=\"ansiyellow\">(</span>os<span class=\"ansiyellow\">.</span>path<span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>tempfile<span class=\"ansiyellow\">.</span>mkdtemp<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;data&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    323</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 324</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>mode<span class=\"ansiyellow\">(</span>mode<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>options<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">**</span>options<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    325</span>         <span class=\"ansigreen\">if</span> partitionBy <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    326</span>             self<span class=\"ansiyellow\">.</span>partitionBy<span class=\"ansiyellow\">(</span>partitionBy<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansicyan\">options</span><span class=\"ansiblue\">(self, **options)</span>\n<span class=\"ansigreen\">    282</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    283</span>         <span class=\"ansigreen\">for</span> k <span class=\"ansigreen\">in</span> options<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 284</span><span class=\"ansiyellow\">             </span>self<span class=\"ansiyellow\">.</span>_jwrite <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jwrite<span class=\"ansiyellow\">.</span>option<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">,</span> options<span class=\"ansiyellow\">[</span>k<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    286</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">    536</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    537</span>         return_value = get_return_value(answer, self.gateway_client,\n<span class=\"ansigreen\">--&gt; 538</span><span class=\"ansiyellow\">                 self.target_id, self.name)\n</span><span class=\"ansigreen\">    539</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    540</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     34</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     35</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 36</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     37</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     38</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    302</span>                 raise Py4JError(\n<span class=\"ansigreen\">    303</span>                     <span class=\"ansiblue\">&apos;An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n&apos;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 304</span><span class=\"ansiyellow\">                     format(target_id, &apos;.&apos;, name, value))\n</span><span class=\"ansigreen\">    305</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    306</span>             raise Py4JError(\n\n<span class=\"ansired\">Py4JError</span>: An error occurred while calling o1013.option. Trace:\npy4j.Py4JException: Method option([class java.lang.String, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)\n\tat py4j.Gateway.invoke(Gateway.java:252)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n</div>","startTime":1.444679288991E12,"submitTime":1.444679288991E12,"finishTime":1.444679289358E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"780eb4cb-c742-4d07-bbce-c83d679ddcf0"},{"version":"CommandV1","origId":1208,"guid":"92c40511-d4a1-4a92-9a65-845dfb5fe77c","subtype":"command","commandType":"auto","position":1.984375,"command":"%md\n#### Writing Partitioned Datasets\nSupport for writing to specific partitions using columns defined in the DataFrame are supported. The syntax differs slightly from the Scala package. Databricks clusters have the Spark-Avro package on the cluster's classpath to allow users to directly import any functions to their workspace.  \nIn the example below, the path can be seen as the root table location similar to how a Hive partitioned table would look. The directory can be the name of your dataset or table. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"30756d0e-b044-4608-8df8-451a11a9535d"},{"version":"CommandV1","origId":1209,"guid":"01127b9a-574a-49f4-9104-fc7b1418c673","subtype":"command","commandType":"auto","position":1.9921875,"command":"df = sqlContext.createDataFrame([(2012, 8, \"Batman\", 9.8), (2012, 8, \"Hero\", 8.7), (2012, 7, \"Robot\", 5.5), (2011, 7, \"Git\", 2.0)], \n                                [\"year\", \"month\", \"title\", \"rating\"])\n# Write the DataFrame out using columns as partitions. \ndf.write.partitionBy(\"year\", \"month\").format(\"com.databricks.spark.avro\").save(\"/tmp/avro_partitioned\")\ndisplay(dbutils.fs.ls(\"/tmp/avro_partitioned/\"))\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;DataFrameWriter&apos; object has no attribute &apos;avro&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-39-5efdc49f2ae6&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> df <span class=\"ansiyellow\">=</span> sqlContext<span class=\"ansiyellow\">.</span>createDataFrame<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">(</span><span class=\"ansicyan\">2012</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">8</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;Batman&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">9.8</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansicyan\">2012</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">8</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;Hero&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">8.7</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansicyan\">2012</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">7</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;Robot&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">5.5</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansicyan\">2011</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">7</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;Git&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">2.0</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&quot;year&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;month&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;title&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;rating&quot;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 3</span><span class=\"ansiyellow\"> </span>df<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">.</span>partitionBy<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;year&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;month&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>avro<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;/tmp/mwc/output&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;DataFrameWriter&apos; object has no attribute &apos;avro&apos;\n</div>","startTime":1.444669368929E12,"submitTime":1.444669368773E12,"finishTime":1.44466936901E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"69f9ad29-681b-4f7f-9ab9-ec13064a70fd"},{"version":"CommandV1","origId":1210,"guid":"fa6e1f03-f031-4a6f-8d18-a91b81a31b21","subtype":"command","commandType":"auto","position":2.7451171875,"command":"%md Use a **CREATE TABLE** command to read in the data.\n\nEasily query the tables by specifying the path to the Avro files in the CREATE TABLE statement.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"153af1dd-09c3-4558-aec3-0d081232334a"},{"version":"CommandV1","origId":1211,"guid":"cb75ed32-e1f2-40bf-b89b-1d584d349d93","subtype":"command","commandType":"auto","position":3.12158203125,"command":"%sql \n\nDROP TABLE IF EXISTS avroTable1;\nDROP TABLE IF EXISTS avroPartitioned;\n\nCREATE TABLE avroTable1\nUSING com.databricks.spark.avro\nOPTIONS (path \"dbfs:/tmp/users.avro\");\n\nCREATE TABLE avroPartitioned\nUSING com.databricks.spark.avro\nOPTIONS (path \"dbfs:/tmp/test_dataset\");","commandVersion":0,"state":"finished","results":{"type":"table","data":[],"arguments":{},"schema":[],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":null,"startTime":1.444844815315E12,"submitTime":1.444844816452E12,"finishTime":1.444844816492E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"6be76a3a-b810-43e3-9626-f94bf456f6d2"},{"version":"CommandV1","origId":1212,"guid":"74bb9ac7-033e-4e3a-98d9-49206196d4e3","subtype":"command","commandType":"auto","position":3.309814453125,"command":"%sql select * from avroTable1","commandVersion":0,"state":"finished","results":{"type":"table","data":[["Alyssa",256.0,[]],["Ben",7.0,"red"]],"arguments":{},"schema":[{"type":"string","name":"name"},{"type":"int","name":"favorite_number"},{"type":"string","name":"favorite_color"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":null,"startTime":1.444844817654E12,"submitTime":1.444844818805E12,"finishTime":1.444844827343E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"f56feaed-128e-4c1d-8ad6-47e1ebd2e3bf"},{"version":"CommandV1","origId":1213,"guid":"0cfaf688-5978-44e3-9ca1-6923801eb9cf","subtype":"command","commandType":"auto","position":3.4039306640625,"command":"%sql select * from avroPartitioned where year = \"2012\";","commandVersion":0,"state":"finished","results":{"type":"table","data":[["Batman",9.8,2012.0,8.0],["Hero",8.7,2012.0,8.0],["Robot",5.5,2012.0,7.0]],"arguments":{},"schema":[{"type":"string","name":"title"},{"type":"double","name":"rating"},{"type":"int","name":"year"},{"type":"int","name":"month"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"Error in SQL statement: AnalysisException: expression 'title' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() if you don't care which value you get.;","error":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: expression 'title' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() if you don't care which value you get.;\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:44)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.apply(CheckAnalysis.scala:129)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.apply(CheckAnalysis.scala:129)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:129)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:914)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:132)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:725)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$5.apply(DriverLocal.scala:310)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$5.apply(DriverLocal.scala:290)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:290)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:162)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:485)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:482)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:384)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:195)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:325)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:162)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:485)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:482)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:384)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:195)\n\tat java.lang.Thread.run(Thread.java:745)\n","startTime":1.444844871952E12,"submitTime":1.444844873019E12,"finishTime":1.444844872572E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"dbbcf22f-e4c5-468d-9a9a-eab2a1d23cd3"},{"version":"CommandV1","origId":1214,"guid":"882fc967-79ff-4840-9370-f09d5bdc6874","subtype":"command","commandType":"auto","position":3.45098876953125,"command":"%md \nYou can easily load the Avro data sets back into a DataFrame using the sqlContext.load() method. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.444844878107E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"b12f0f09-676a-4305-b7e3-69bf687e28bc"},{"version":"CommandV1","origId":1215,"guid":"b8767ee2-9a58-4491-abc3-f7d7e7bc5185","subtype":"command","commandType":"auto","position":3.474517822265625,"command":"users = sqlContext.read.format(\"com.databricks.spark.avro\").load(\"/tmp/users.avro\")\nusers.collect()\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>\n[Row(name=u&apos;Alyssa&apos;, favorite_number=256, favorite_color=None),\n Row(name=u&apos;Ben&apos;, favorite_number=7, favorite_color=u&apos;red&apos;)]\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;DataFrameReader&apos; object has no attribute &apos;avro&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-7-4f7bd5442dd6&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>users <span class=\"ansiyellow\">=</span> sqlContext<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>avro<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;/tmp/users.avro&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> users<span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;DataFrameReader&apos; object has no attribute &apos;avro&apos;\n</div>","startTime":1.444843662164E12,"submitTime":1.444843661956E12,"finishTime":1.444843662591E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"c6332e70-ebba-4224-bc9b-8d34cb5440fc"}],"guid":"7c116aa2-804e-4736-901a-c00bea554f84","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"58ddadaa-391c-435c-9da7-6b9e5abf2f42","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>