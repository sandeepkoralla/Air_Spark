<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>External Applications / PanTera - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":3017,"name":"External Applications / PanTera","language":"python","commands":[{"version":"CommandV1","origId":3018,"guid":"b23026e9-43a1-4760-9dfd-ceeca39c6823","subtype":"command","commandType":"auto","position":1.0,"command":"%md\n\n# **PanTera**\n\nPanTera is a general visualization tool designed specifically for big data sets. It allows rapid, multi-scale exploration of massive data sets at very granular depths, whether it’s identifying broader trends at the macro level (think seeing which country has the most data points) or making nuanced observations the micro level (which intersection has the most). \n\nYou can see PanTera in action on the product website at [pantera.io](http://pantera.io).\n\n[Watch a demonstration of PanTera on Databricks](https://www.youtube.com/watch?v=sTxHpvIS3CQ&feature=youtu.be&t=2226)\n\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.433990773693E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"vida","iPythonMetadata":null,"nuid":"fa5b8632-71fd-4655-86b3-581ae201af7d"},{"version":"CommandV1","origId":3019,"guid":"e1ec71d2-a4ba-4a15-bdcc-747c004f8f82","subtype":"command","commandType":"auto","position":1.5,"command":"%md\n\n### How does PanTera integrate with Databricks?\n\nDatabricks is a platform-as-a-service and provides fully managed Spark clusters in the cloud. This management is done atop your existing cloud computing accounts and enables you to focus on using your infrastructure rather than administering it.\n\nPanTera is an application which excels when paired with Spark. As Databricks provides many useful tools and utilities that make working with big data, and leveraging the underlying power of Spark, much easier, it’s a natural fit for PanTera.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.442422591322E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"vida","iPythonMetadata":null,"nuid":"4df95ff8-6c43-4b53-bb12-908a02303df2"},{"version":"CommandV1","origId":3020,"guid":"e15d5423-5f13-47df-94db-9ed862799819","subtype":"command","commandType":"auto","position":2.5,"command":"%md\n\n### How do I get PanTera?\nPanTera is currently available to select Early Adopters who have expressed interest in helping to shape the future of the product. \n\nWe’d love to have you join. If you’re interested, please contact us at info@pantera.io, and we’ll help you get started.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"3384bfe5-2418-4e7c-bc97-32c9aacdfd69"},{"version":"CommandV1","origId":3021,"guid":"e399dda8-fed8-4224-887a-b3c9db8c04e9","subtype":"command","commandType":"auto","position":3.5,"command":"%md\n\n### What types of visualizations does PanTera support?\n\nAs of release 0.4.0, PanTera supports the following visualization types. \n\nIf you wish PanTera had another visualization that we don’t currently support, please contact us at help@pantera.io. We’re always evaluating feedback when determining which features to prioritize next.\n\n#### Geographic\n\nChart your data on a map, aggregating it by latitude/longitude using functions of your choice. Useful for revealing patterns and trends in geographic data.\n\n![Geographic](http://training.databricks.com/databricks_guide/pantera/geographic.png)\n\n#### Scatter Plot\n\nPlot your data, aggregating it by spatial coordinates using functions of your choice. Useful for revealing patterns and trends in general data.\n\n![Scatter Plot](http://training.databricks.com/databricks_guide/pantera/scatter_plot.png)\n\n#### Time Series\n\nGraph your temporal data and blend it with our curated set of open data sources. Useful for revealing patterns and trends in event-based datasets.\n\n![Time Series](http://training.databricks.com/databricks_guide/pantera/time_series.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"1ede4a7f-5da4-4ac3-8f70-c539c4e93e72"},{"version":"CommandV1","origId":3022,"guid":"4443a3bb-0783-458f-bb46-ea68b899f496","subtype":"command","commandType":"auto","position":4.5,"command":"%md\n\n### What kind of data can I plot?\n\nAs PanTera is a general visualization tool, it is capable of plotting many different types of data and supporting many different types of analyses.\n\nTechnically speaking, the product supports “Float”, “Double”, “Integer”, “Long”, and “Timestamp” columns as well as geolocated data that has Latitude and Longitude in decimal degrees.\n\nGenerally speaking, PanTera works best with continuous, high resolution data and variables, though the sparse mode tools do allow PanTera to work with lower resolution sets, as well.\n\nIf you need to work with a column type we don’t currently support, or your curious if your data is the right fit, please don’t hesitate to contact us at help@pantera.io.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"9c875a16-7a24-4689-a77f-540011a4f4c7"},{"version":"CommandV1","origId":3023,"guid":"623e20c7-ec92-4855-967b-a06a7ef54dc1","subtype":"command","commandType":"auto","position":5.0,"command":"%md\n\n### How do I get started?\nOnce you’ve joined the Early Adopters program, your Databricks account will be provisioned with PanTera. You will then see PanTera under the “Applications” section of your Databricks dashboard as pictured:\n\n![Databricks Application](http://training.databricks.com/databricks_guide/pantera/application.png)\n\nTo begin using PanTera, you’ll need some data. To bring your data into Databricks and ultimately PanTera, follow these steps:\n\n* Load your data into a table via the Databricks Tables interface or create a temporary table via the Databricks Notebook\n  * To bring your own table in…\n     * Click the “Tables” item on the Databricks menu (accessible via the “hamburger” icon on the top-left corner of the Databricks dashboard).\n     * Click “Create Table” to create a new table. You can then either connect to your existing database or upload a CSV.\n     * Proceed through the process of connecting and mapping your table.\n  * To use the Notebook\n     * Click the “Notebook” item from the Databricks menu (accessible via the “hamburger” icon on the top-left corner of the Databricks dashboard).\n     * Write your own SQL or code to create your data. You can see an example of what this might look like here.\n* Once your tables have been created, start PanTera.\n  * Click “Applications”.\n  * Click the “PanTera” logo.\n     * If there is no “Applications” menu item, or if PanTera is not visible within it, please contact help@pantera.io.\n  * Begin by creating a new analysis by clicking the “+ New Analysis” in the top-right corner.\n  \n  ![New Analysis](http://training.databricks.com/databricks_guide/pantera/new_analysis.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"d7897c12-ab7a-489a-bf31-1a40feb6b650"},{"version":"CommandV1","origId":3024,"guid":"f26af113-67c3-4a0b-9b7a-3a510d90356b","subtype":"command","commandType":"auto","position":5.25,"command":"%md\n\n### What is an analysis?\n\nAn analysis in PanTera is somewhat analogous to a workbook in Microsoft Excel. It is a collection of data pulled together into a single source which can then be worked with and visualized.\n\nIn PanTera’s case, each analysis involves creating a single visualization from n number of disparate or related tables. This visualization can then be interactively interrogated, with multiple layers and multiple overlaid columns being drawn from potentially multiple data sets.\n\nFrom a given analysis, you can view the data at multiple depths and draw summary statistics and sample rows from any given subselection. You can also easily subselect data in a given analysis and spin it off into a new temporary table and a new, separate analysis.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"57e52188-a0e9-4372-9795-1f7882e332a9"},{"version":"CommandV1","origId":3025,"guid":"e3e7d8c1-9714-4217-a579-baa08358dc0b","subtype":"command","commandType":"auto","position":5.375,"command":"%md\n\n### What is a layer?\n\nA layer is the interactive, visual representation of data points from your source table. For each table of data that you want to visualize, you will have a single layer. You can then create additional layers for different aspects of the same data source or different data sources altogether.\n\nAs an example, if you had a set of data showing Mobile and Desktop sales, you’d create a temporary table for each data set in Databricks and then a layer for each in PanTera. This would then allow you to easily analyze the differences and similarities between the two sets.\n\nDepending on the visualization you’ve chosen, the data will be plotted along an x or x-y plane (e.g., Longitude and Latitude in a geographic visualization or time in a time series visualization). \n\nThe coordinate or temporal columns identified in your data will determine the position of the point, and its color and intensity is controlled by the value column and aggregation function of your choice. \n\nFor example, in a geographic visualization, plotting latitude and longitude from a source table with the count() aggregation function will result in geolocated points, with a colour and intensity controlled by the number of source records falling at that point.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.442423110086E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"vida","iPythonMetadata":null,"nuid":"4137a045-a6f2-49cc-9e17-34f00d1b157f"},{"version":"CommandV1","origId":3026,"guid":"b4d2c2e9-f35e-4ac6-b5af-414e48d5049b","subtype":"command","commandType":"auto","position":5.5,"command":"%md\n\n### How do I work with layers?\n\nOnce you have an analysis up and running, you can add new layers by clicking the “+ Layer” button in the top-right corner of PanTera. \n\nFor each layer, you will be required to define the following:\n* A name\n* The data table you want to use\n* The x or x-y coordinate columns in your data table\n* The plot function and the value column for that function\n\nOnce you have added a layer, you’ll see a simple interface for working with that layer as pictured:\n\n![Layer](http://training.databricks.com/databricks_guide/pantera/layer.png)\n\nThere are a number of useful controls in place to help you tailor the layer to your analysis. \n\n#### Controls\n\nIn the top-right corner of a layer when hovering over it, you’ll see four controls. These are the basic controls for manipulating a layer:\n\n* Clicking ![Trash Icon](http://training.databricks.com/databricks_guide/pantera/trash.png) will delete the layer.\n* Clicking  ![Bounds Icon](http://training.databricks.com/databricks_guide/pantera/bounds.png)will zoom your visualization to the bounds of the data in the layer.\n* Clicking and holding ![Hamburger](http://training.databricks.com/databricks_guide/pantera/hamburger.png) will allow you to drag and reorder the layer. The layer’s position in the list will determine whether its data is drawn above or below the other layers’ data.\n* Clicking ![Toggle](http://training.databricks.com/databricks_guide/pantera/toggle.png) will toggle whether or not the layer’s data is drawn in the visualization.\n\n#### Filter\n\nThe filter slider allows you to clamp the maximum and minimum values within a layer to enhance the color contrast of source data which falls largely within a small range of the true maximum and minimum values.\nColor Ramp\n\nThe color ramp selector allows you to decide which range of colors will represent data values within a layer. Hot, cool, verdant and grayscale color ramps use a spectrum based on a single hue, while spectral and temperature use a range based on multiple hues. Monochromatic color ramps are great for overlaying and comparing multiple layers of data, while polychromatic ramps are perfect for maximizing contrast in a single layer.\n\n#### Scale\n\nThe layer scale determines how the value at each data point is mapped to a color in your selected color ramp. Linear and Logarithmic options are available.\n\n#### Coarseness\n\nThe layer coarseness slider is particularly useful when dealing with sparse data. Increasing coarseness will increase the size of data points to make them easier to spot. \n\n#### Opacity\n\nThe opacity slider allows you to scale the transparency of the data coloring. This is very useful for distinguishing overlap between layers when you have data sets with overlapping data.\n\n#### Sparse Mode\n\nThe sparse mode toggle is a very useful feature which allows you to balloon out the representation of any individual point into a cloud, expanding the representation of the point and its heatmap to make visualizing sparser data sets at the macro and micro level more user friendly.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"1b49897b-2601-4f75-af02-c9a7629c208d"},{"version":"CommandV1","origId":3027,"guid":"f66b9dca-a280-4f12-8f0c-2f277c271f57","subtype":"command","commandType":"auto","position":6.5,"command":"%md\n\n### What are the plot functions and how do they work?\n\nPlot functions are mathematical operations which can be run against an additional dimension of your data and produce the heat mapping effect on PanTera’s visualizations. Each of our visualizations has the ability to leverage these functions, which are described in more detail below.\n\nIf you wish PanTera had another plot function that we don’t currently support, please contact us at help@pantera.io. We’re always evaluating feedback when determining which features to prioritize next.\n\n#### Geographic and Scatter Plot Functions\n\n* count(); plot the number of records at a given (lon, lat) or (x,y)\n* mean(v); plot the mean of ALL values in some column v which appear at a given (lon, lat) or (x,y)\n* max(v); plot the max of ALL values in some column v which appear at a given (lon, lat) or (x,y)\n* min(v); plot the min of ALL values in some column v which appear at a given (lon, lat) or (x,y)\n* sum(v); plot the sum of ALL values in some column v which appear at a given (lon, lat) or (x,y)\n\n#### Time Series Functions\n\n* count(); plot the number of records at a given t, where t is a timestamp or millisecond value\n* mean(v); plot the mean of ALL values in some column v which appear at a given t\n* max(v); plot the max of ALL values in some column v which appear at a given t\n* min(v); plot the min of ALL values in some column v which appear at a given t\n* sum(v); plot the sum of ALL values in some column v which appear at a given t","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"2329cd31-4618-4cac-a874-a0e5932a9d5b"},{"version":"CommandV1","origId":3028,"guid":"91d619d0-f2d7-4aa8-ae10-752160b0598a","subtype":"command","commandType":"auto","position":7.5,"command":"%md\n\n### Troubleshooting\n#### PanTera has stalled or is not returning data. What’s wrong?\n\nPlease try the following:\n\n* Turning your layer off and on\n* Recreating your layer\n* Recreating your analysis\n* Restarting PanTera via the Databricks Apps tab (last resort, as this will interrupt other users and erase all existing analyses)\n\nIf you are still experiencing issues, please contact help@pantera.io.\n\n#### I’m getting the message “This is analysis is no longer available”.\n\nUnfortunately, there may be times when your analyses are corrupted or removed as part of a cluster restart. These include the following:\n\n1. After a cluster has been restarted.\n1. After a Databricks maintenance window.\n1. After a PanTera upgrade if you were using the software during the upgrade.\n\nThe only option in this case is to delete and recreate the analysis, which you can do by clicking the trash can icon on the analysis. \n\nThis is something we’re actively working on fixing before 1.0 release, and we appreciate your patience and support as we work to resolve this issue. If you any concerns or comments, please don’t hesitate to contact help@pantera.io.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"bb673776-8136-4859-b886-9fa012b02e63"},{"version":"CommandV1","origId":3029,"guid":"ffc272a9-5fcf-4deb-bcbb-da5e444aa88c","subtype":"command","commandType":"auto","position":8.5,"command":"%md\n\n### Infrastructure\n\n#### What size cluster do I need?\n\nPanTera works best when the cluster is relatively large. PanTera will run faster on larger clusters for the same-sized dataset. If you notice performance issues in PanTera, try making a larger cluster. \n\nFor reference, we find that provisioning 1GB cluster memory per 1M records results in PanTera requiring about 30s to render a view. Performance scales roughly linearly so provisioning 3GB memory for the same 1M records should reduce render time to about 10s.\n\nBe wary that simply adding more clusters will not necessarily speed the visualization generation. The size of the data impacts the speed of PanTera’s tiling far more than the compute behind the scenes as Spark’s caching is nondeterministic.\n\nThat said, you can control the partitioning of data on a Spark cluster and Spark does it by default, too. Advanced users will be able to use this to their advantage in speeding up the results for large datasets, but for the most part and in most cases these types of optimizations will not be necessary.\n\n#### Which Spark version should I use?\n\nSpark clusters spun up via Databricks can be provisioned with Spark 1.3 or 1.4. PanTera fully supports both versions, but we recommend using 1.4 if you can, as it contains improvements which can make PanTera run even faster.\n\n#### Can I run PanTera on the same cluster that I use for data collection or to power my application?\n\nRunning PanTera on on a cluster shared with other production tools or processes is not advised during the Early Adopter Program. Currently, our recommended best practice is to run PanTera on a non-critical development, data science or offline analytics cluster. \n\nDatabricks makes provisioning a temporary cluster fast and easy: just turn off your temporary cluster when you are finished working with PanTera.\n\n#### How do I restart a cluster?\n\nRestarting your cluster may be necessary to resolve issues you may be experiencing. To do so, follow these steps:\n\n1. Click the “Clusters” item on the Databricks menu (accessible via the “hamburger” icon on the top-left corner of the Databricks dashboard).\n1. Identify your cluster and scroll to the right\n1. Click the “Restart” button for the cluster you’d like to restart.\n\n#### What happens if I restart a cluster?\n\nRestarting a cluster does have some negative side effects in our current version:\n\n* On the Databricks side of things, any temporary tables associated with the analyses/cluster will also be lost.\n* All existing analyses utilizing that cluster will be lost if and only if the source (temporary) tables are available. \n\nIn the future, as we move forward from 0.4.0, we will be working to resolve the first issue. Databricks is aware of the implications of the second issue and is always investigating potential improvements.\n\nThank you for your patience as we work to make PanTera an awesome tool for your big data visualization needs.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"5cd2e866-dae3-4263-8c12-f4f4e4fb0293"}],"guid":"cddff9bf-ff63-4507-b517-b735d6f47206","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"ba378005-b734-47a0-9741-18f5e2a8f13d","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>