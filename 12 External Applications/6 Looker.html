<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>External Applications / Looker - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":3035,"name":"External Applications / Looker","language":"python","commands":[{"version":"CommandV1","origId":3036,"guid":"bf7b0a97-9306-468c-9fe3-8859b7d9126c","subtype":"command","commandType":"auto","position":1.0,"command":"%md # Connecting Looker to a Databricks Cloud Cluster","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"b6221f14-be5c-40c2-8845-d27dead3af67"},{"version":"CommandV1","origId":3037,"guid":"348f8bb5-1508-4bd6-b4d7-9f0fb75bf042","subtype":"command","commandType":"auto","position":2.0,"command":"%md #### Step 1: Get the public hostname of the Master/Driver","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"ac8b9c5f-05bb-4f5c-b87c-feaf5f73a4ee"},{"version":"CommandV1","origId":3038,"guid":"e6a01af1-7509-4f23-89f2-d454a86ac35d","subtype":"command","commandType":"auto","position":3.0,"command":"import urllib2\npublic_hostname = urllib2.urlopen('http://169.254.169.254/latest/meta-data/public-hostname').read()\npublic_hostname","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>&apos;ec2-52-88-195-207.us-west-2.compute.amazonaws.com&apos;\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.442172522984E12,"submitTime":1.442172522939E12,"finishTime":1.442172523036E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"9f9de4ca-a986-436c-9d31-36c96dd92262"},{"version":"CommandV1","origId":3039,"guid":"3d37f1d3-8fea-4da0-af00-33436f6f5285","subtype":"command","commandType":"auto","position":3.5,"command":"%md \n#### Step 2: Open port TCP/10000 for the IP address that Looker will be connecting from","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"ebfb12af-eeb6-4b99-b179-29345356f3a7"},{"version":"CommandV1","origId":3040,"guid":"fff39292-4a57-4c49-9fab-a3eac5c347b5","subtype":"command","commandType":"auto","position":4.0,"command":"%md\n\nAll potential Looker-hosted source IP addresses are listed on [this page](http://www.looker.com/docs/admin/looker-hosted/secure-db-connection). As of September 2015, they are:\n\nUnited States (default)\n* 54.208.10.167\n* 54.209.116.191\n* 52.1.5.228\n* 52.1.157.156\n\nAsia\n* 52.68.85.40\n* 52.68.108.109\n\nEurope\n* 52.16.163.151\n* 52.16.174.170\n\nIf you are hosting Looker on premise, then you will need to determine the source IP address and adjust the `IP_ADDRESSES_TO_ADD` variable below.\n\nThe following three cells were taken from *Databricks Guide* -> *10 AWS Configuration Guide* -> *3 Whitelist IP* and modified for your convenience.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"c094775d-0356-4015-9765-a05a3dc353ae"},{"version":"CommandV1","origId":3041,"guid":"1be1df16-0453-4e0a-938e-b08522818114","subtype":"command","commandType":"auto","position":5.0,"command":"# Setup the access keys and configuration variables \nACCESS_KEY = \"XXXXXXXXXXXX\"\nSECRET_KEY = \"XXXXXXXXXXXXXXXXXXXX\" # shouldn't have to encode or escape\nREGION = \"us-west-1\"  # \"us-west-1\", not \"us-west-1c\"\nIP_ADDRESSES_TO_ADD = [\"54.208.10.167/32\", \"54.209.116.191/32\", \"52.1.5.228/32\", \"52.1.157.156/32\", \"52.68.85.40/32\", \"52.68.108.109/32\", \"52.16.163.151/32\", \"52.16.174.170/32\"]\nPORTS_TO_ADD = [\"10000\"]\nPROTOCOLS_TO_ADD = [\"tcp\"]","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.442173694847E12,"submitTime":1.442173694783E12,"finishTime":1.442173694879E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"904b789b-b621-4e8d-af4a-44735ab1f844"},{"version":"CommandV1","origId":3042,"guid":"b86071d9-c626-4906-9341-d7dba77ec571","subtype":"command","commandType":"auto","position":6.0,"command":"# Find current security group\nimport boto.ec2\n\n# this will only find the first matching security group\ndef get_databricks_security_group():\n  conn = boto.ec2.connect_to_region(REGION, \n                                    aws_access_key_id=ACCESS_KEY,    \n                                    aws_secret_access_key=SECRET_KEY)\n  rs = conn.get_all_security_groups()\n  for r in rs:\n    if (r.name.find('dbc') == 0 or r.name.find('databricks') == 0) and (r.name.find('-ExternalServices') > 0 or r.name.find('-worker-unmanaged')):\n      return r  \n\ndatabricks_security_group = get_databricks_security_group()\nprint \"Found Security Group: \" + databricks_security_group.name","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Found Security Group: happy-panda-security\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;NoneType&apos; object has no attribute &apos;get_all_security_groups&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-5-af2ec121245a&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     13</span>       <span class=\"ansigreen\">return</span> r<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     14</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 15</span><span class=\"ansiyellow\"> </span>databricks_security_group <span class=\"ansiyellow\">=</span> get_databricks_security_group<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     16</span> <span class=\"ansigreen\">print</span> databricks_security_group<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-5-af2ec121245a&gt;</span> in <span class=\"ansicyan\">get_databricks_security_group</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      7</span>                                     aws_access_key_id<span class=\"ansiyellow\">=</span>ACCESS_KEY<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      8</span>                                     aws_secret_access_key=SECRET_KEY)\n<span class=\"ansigreen\">----&gt; 9</span><span class=\"ansiyellow\">   </span>rs <span class=\"ansiyellow\">=</span> conn<span class=\"ansiyellow\">.</span>get_all_security_groups<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     10</span>   <span class=\"ansigreen\">for</span> r <span class=\"ansigreen\">in</span> rs<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     11</span>     <span class=\"ansigreen\">print</span> r<span class=\"ansiyellow\">.</span>name<span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;NoneType&apos; object has no attribute &apos;get_all_security_groups&apos;\n</div>","startTime":1.442176062871E12,"submitTime":1.442176062817E12,"finishTime":1.442176063421E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"8ea583eb-8f82-4404-a69e-a3f037216cf1"},{"version":"CommandV1","origId":3043,"guid":"0ab41ebc-ae1f-4e5c-8c30-81e559f1a7ae","subtype":"command","commandType":"auto","position":7.0,"command":"# Update the security group\nfrom collections import defaultdict\n\ndef sg_rule_dict(security_group):\n  rule_dict = defaultdict(list)\n  rules = map(lambda x: (\"%s:%s-%s\" %(x.ip_protocol, x.from_port, x.to_port), x.grants), security_group.rules)\n  for rule in rules:\n    for grant in rule[1]:\n      rule_dict[rule[0]].append(grant.cidr_ip)\n  return rule_dict\n\nexisting_rules = sg_rule_dict(databricks_security_group)\n\nfor ip_address in IP_ADDRESSES_TO_ADD:\n  for port in PORTS_TO_ADD:\n     for protocol in PROTOCOLS_TO_ADD:\n       key = \"%s:%s-%s\" % (protocol, port, port)\n       if ip_address not in existing_rules[key]:\n         databricks_security_group.authorize(ip_protocol=protocol, \n                                             from_port=port, \n                                             to_port=port, \n                                             cidr_ip=ip_address)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.442174291301E12,"submitTime":1.442174291196E12,"finishTime":1.442174292522E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"3469e4e0-5f66-42f9-bb69-fff78c8eb6e2"},{"version":"CommandV1","origId":3044,"guid":"5c002d56-3ea3-4fa1-bc54-e5b7ca0b50b8","subtype":"command","commandType":"auto","position":8.0,"command":"# Verify updated security groups\nupdated_databricks_security_group = get_databricks_security_group()\n\n# Given a rule, check that the port and protocol matches \ndef check_port_and_proto(rule, port, protocol):\n  if (port == rule.to_port == rule.from_port) and proto == rule.ip_protocol:\n    return True\n  return False\n\n# Loop over ip address to verify a matching rule in updated security groups\nfor ip in IP_ADDRESSES_TO_ADD:\n  found_ip = False\n  list_of_ips = []\n  for rule in updated_databricks_security_group.rules:\n    # Flag to determine if we find a match for the current ip address\n    ip_matches = False\n    # rule.grants contains a list of ip address objects. convert to string to check inclusion easily\n    for i in rule.grants:\n      list_of_ips.append(str(i))\n    if ip in list_of_ips:\n      ip_matches = True\n      for port in PORTS_TO_ADD:\n        for proto in PROTOCOLS_TO_ADD:\n          if not check_port_and_proto(rule, port, proto):\n            print \"Missing rule. Ip: \" + ip + \"\\tPort: \" + port + \"\\tProtocol: \" + proto\n          else:\n            print \"Matching rule. Ip: \" + ip + \"\\tPort: \" + port + \"\\tProtocol: \" + proto\n  if not ip_matches:\n    print \"Cannot find rule with ip: \" + ip\n      \n            ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Matching rule. Ip: 54.208.10.167/32\tPort: 10000\tProtocol: tcp\nMatching rule. Ip: 54.209.116.191/32\tPort: 10000\tProtocol: tcp\nMatching rule. Ip: 52.1.5.228/32\tPort: 10000\tProtocol: tcp\nMatching rule. Ip: 52.1.157.156/32\tPort: 10000\tProtocol: tcp\nMatching rule. Ip: 52.68.85.40/32\tPort: 10000\tProtocol: tcp\nMatching rule. Ip: 52.68.108.109/32\tPort: 10000\tProtocol: tcp\nMatching rule. Ip: 52.16.163.151/32\tPort: 10000\tProtocol: tcp\nMatching rule. Ip: 52.16.174.170/32\tPort: 10000\tProtocol: tcp\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax","error":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;ipython-input-72-ddef1b860a45&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">27</span>\n<span class=\"ansiyellow\">    elif not ip_matches:</span>\n<span class=\"ansigrey\">       ^</span>\n<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax\n\n</div>","startTime":1.442178276467E12,"submitTime":1.442178276374E12,"finishTime":1.442178277208E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"mwc@databricks.com","iPythonMetadata":null,"nuid":"6840077c-3a02-4916-aa7d-2cbb46b485a9"},{"version":"CommandV1","origId":3045,"guid":"2a835036-7ca7-4ff6-9730-b41a6ac59281","subtype":"command","commandType":"auto","position":8.5,"command":"%md #### Step 3: Connect Looker to your Databricks Cloud Cluster","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"8836a33f-3d0f-45af-8a66-6f017c895cb8"},{"version":"CommandV1","origId":3046,"guid":"57e53b67-6254-4112-8e58-92c7b91566c8","subtype":"command","commandType":"auto","position":9.0,"command":"%md\n1. Go to Admin -> Connections -> New Database Connection\n2. Fill in the connection parameters\n  * Use the hostname of the Master/Driver as determined in step 1 for the Host\n  * Leave the port as 10000\n  * Use `default` as the database\n  * Use `hiveuser` as the Username with no Password\n  * Don't enable PDTs\n  * Leave Max Connections and Connection Pool Timeout at their defaults\n  * Leave Database Time Zone blank (assuming you are storing everything in UTC)\n  * Adjust Query Time Zone if you want to translate queries into other time zones\n  * Leave Additional Params blank\n3. Click Test These Settings to make sure that you have everything set properly\n4. Click Add Connection\n\n![ConnectionScreen](http://0af262c0-a-62cb3a1a-s-sites.googlegroups.com/site/toddnemet/Home/databricks-looker/connection-screen.png?attachauth=ANoY7cqLdySGw15Drz8J7oU3ok2NVhtIYXdzrgeCyCopG-3M-p9u38WAltoXYZ-fcFAA1Y4icqGYycVHeRWma4TEVU4a1IUJi14Z-jUlIy3AgPLISFwlPi3SNeElZxuEko9rnkWFgstl7ixRe1MfPLbAea8o7aHAxoV1r1PWHO3z8cTKGjMTYYcHYAijbxgSMO1mCT0ceck64FwpE8tZ0lEyUlpKERBiUqDhK3yCWDIvZ90FJIspks91uOShNi3hOYmz-XDVHNOL&attredirects=0)\n\nOnce you have added the connection, it will be shown like this along side connections to other data sources.\n\n![ConnectionTest](http://0af262c0-a-62cb3a1a-s-sites.googlegroups.com/site/toddnemet/Home/databricks-looker/connection-test.png?attachauth=ANoY7cq8bww0c_UwzpjvM8ivYFQFMTW_iU70iwfHjJXJPbCMI5H0kuvP94zYbtnDLLCUGBg9EElpQGidT6LAFPh5Hzr9xhpD-vO-qHmOXUmP155n_rEcp36NLPBip1ptymC3rEEPbr-uHis3ttkq24OyFUAg5zbF1JDfhNu8hPwrAIcrC6-Vf3b9tiQTIr6m9qn5-UGnbTLESaDzawu-AvfbQ98Gvh8a27nuGfMztEgb7DsxiwN_A66OcljhTJmsh41o29DRIU95&attredirects=0)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"a73be766-c9ec-4c8e-b15b-518c85a07d91"},{"version":"CommandV1","origId":3047,"guid":"148e4e0b-021e-43d3-979c-1d7880d8af69","subtype":"command","commandType":"auto","position":10.0,"command":"%md #### Step 4: Begin modeling your database by creating a project and running the generator","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"2d65b71a-b472-4551-8dc2-2faaa61f34e8"},{"version":"CommandV1","origId":3048,"guid":"8ee8589d-cc27-4681-92e0-fe434f141a31","subtype":"command","commandType":"auto","position":11.0,"command":"%md\n*Note that this step assumes that there are permanent tables stored in the default database of your cluster*\n\n1. If necessary get into \"Developer Mode\" by clicking the Dev button from **OFF** to **ON**\n2. Go to LookML -> Manage Projects\n3. Click on New LookML Project\n4. Configure the new project\n  * Give the project a name\n  * Select the Connection name that you used in Step 3\n  * Select All Tables\n  * Use `default` as the Schemas, unless there is another database to be modeled in the cluster\n5. Click Create Project\n\n![New-Project](https://sites.google.com/site/toddnemet/_/rsrc/1442021052303/Home/databricks-looker/new-project.png)\n\nAfter the project is created and the generator runs, you will see something like the following:\n\n![After-Generator](http://0af262c0-a-62cb3a1a-s-sites.googlegroups.com/site/toddnemet/Home/databricks-looker/after-generator.png?attachauth=ANoY7cp1EIo6ivoaXE5BZeVowbOj_K65O2dde_80hEFy8CPBkyU5RV8bNb0lYWZTk0KmW3tLQo_ku_zlF_PsVPpuzX2BG6XeRmdt2GXR6fFWsOMAlm9AYJ3muQxoqKAWqRgtWHz9DSfPNK7pU8af4hg3euKAEIxEAZ4-MGfgnz3wNv4wgGS12QnpOOpeOM3DpmgEaTFoEzVO32zICPAAV-85Dd4br8QTPxZMblOvkMlNUwlRlQbAyCTyWXGO1hgvnyKWwtaWFvq9&attredirects=0)\n\nYou will then find one model file and multiple view LookML files. The model file shows the tables in the schema and any join relations between them that were discovered, and the view files list each dimension (column) available for each table in the schema.\n\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"d935ae42-2d79-4909-88d9-025a3645644d"},{"version":"CommandV1","origId":3049,"guid":"1caf148c-d85c-4a48-9a71-841938e572e6","subtype":"command","commandType":"auto","position":12.0,"command":"%md #### What to do next?","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"c2ead71b-6e7f-4b6b-8ef4-bb98103624b6"},{"version":"CommandV1","origId":3050,"guid":"03d16ad3-66ad-45a2-af0e-696b4b8ff72c","subtype":"command","commandType":"auto","position":13.0,"command":"%md\n* Configure git to put your LookML files under source control (See https://discourse.looker.com/t/how-to-configure-git-in-looker-3-18/623)\n\n* Learn more about how to model your data with LookML (See http://www.looker.com/docs/data-modeling)\n\n* Explore and visualize your data using Looker (See http://www.looker.com/docs/exploring-data)\n\n* Discuss Looker in our community Discourse forum (See https://discourse.looker.com/)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"753d194e-f571-4477-9313-fbe1ddf52a46"}],"guid":"f7a58dbc-2954-4beb-8f2b-0572287e5d21","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"e2517799-b333-43e8-90d4-66638b823498","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>