<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Spark / Debugging - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":3890,"name":"Spark / Debugging","language":"python","commands":[{"version":"CommandV1","origId":3891,"guid":"8e397d7c-8514-4474-a955-94de6d5c5b90","subtype":"command","commandType":"auto","position":1.0,"command":"%md\n\n# **Debugging**\nThis notebook walks through how to look at the Spark UI pages for debugging.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.433824614573E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"vida","iPythonMetadata":null,"nuid":"d7f4b6ee-b969-414b-b37b-4f039d24493d"},{"version":"CommandV1","origId":3892,"guid":"c608331a-522a-464c-bdc2-0fa5333cc774","subtype":"command","commandType":"auto","position":1.5,"command":"%md\n\n### **Step 1:** To begin debugging a Spark cluster, click on its **Spark Driver** link.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.4341468515E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"matei","iPythonMetadata":null,"nuid":"9dfc86d2-a451-4a67-8e54-f134156ce5b9"},{"version":"CommandV1","origId":3893,"guid":"cb3a928c-a5dd-4e39-8a30-357cd5515249","subtype":"command","commandType":"auto","position":2.0,"command":"%md\n\nOne way to access the **Spark Driver** link is through the cluster menu in the upper left hand corner of your notebook.\n\n![Notebook Driver link](http://training.databricks.com/databricks_guide/notebook_driver_link.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.426212035977E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"d9c0d8d4-df0c-4bdd-935e-813e7fbc352e"},{"version":"CommandV1","origId":3894,"guid":"0693ae7b-ec10-476d-8e26-be384fdc8efd","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n\nAlternately, go to the clusters page, find your cluster, and click on the **Spark Driver** link there under the **Nodes** column.\n\n![Clusters Page](http://training.databricks.com/databricks_guide/cluster_spot_type.png)\n\n**NOTE: Restarting a cluster will clear out its history.**","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.434147012826E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"matei","iPythonMetadata":null,"nuid":"5dae5c95-aaf4-45f7-afa3-ff5dd74418cf"},{"version":"CommandV1","origId":3895,"guid":"52395acd-123f-4cac-b475-a690cba77fac","subtype":"command","commandType":"auto","position":4.0,"command":"%md\n\n### **Step 2:** On the Spark Driver UI or **Spark Jobs** page, you will see a list of **Active**, **Completed**, and **Failed Jobs**. \n\nA \"job\" in Spark is the set of parallel operations launched by an action, such as `collect()` or `count()`. It will consist of one or more *stages*, each of which will consist of multiple *tasks*. Jobs within a Spark application should not be confused with the [Jobs feature](../02 Product Overview/05 Jobs.html) in Databricks, which schedules entire applications.\n\n![Spark Driver UI Page](http://training.databricks.com/databricks_guide/spark_driver_ui.png)\n\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.434147078316E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"matei","iPythonMetadata":null,"nuid":"39312583-df5a-4ab4-b7ec-ac551281602b"},{"version":"CommandV1","origId":3896,"guid":"7a7615cd-b95f-4ae1-a998-1bd23ff2011f","subtype":"command","commandType":"auto","position":5.0,"command":"%md\n#### For each Spark job, you will see a table with the following fields:\n* **Job Id** - An ID that Spark will assign a job.\n* **Description** - The description field contains the command that was run, and a link to the Spark Job Details UI Page.\n* **Submitted** - The time the job was submitted.\n* **Duration** - The duration of the job.\n* **Stages** - The number of successful stages over the total number of stages.\n* **Tasks** - The total number of successful stages / Total.\n   * For **Active Jobs** The tasks column is an approximate progress bar of the job.\n   * For **Failed Jobs** The tasks column can tell you how many tasks failed for this job.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.434147088837E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"matei","iPythonMetadata":null,"nuid":"5bb2bf66-9b45-4ed8-a151-c4059a31e618"},{"version":"CommandV1","origId":3897,"guid":"9c1926a1-fb68-44ef-beae-5b901fb94036","subtype":"command","commandType":"auto","position":5.5,"command":"%md\n\nScroll down the list and find the job that is of interest.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.426212081439E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"733f4c4a-76b4-4052-aee0-da6873573b86"},{"version":"CommandV1","origId":3898,"guid":"d94af6b8-1061-4ab0-b092-37ae8dd0b843","subtype":"command","commandType":"auto","position":6.0,"command":"%md\n\n### **Step 3:** Follow the link in the **Description** column for the job to go to the **Job Details** page. \n* This page contains details about the stages for the job.\n\n![Spark Job UI Page](http://training.databricks.com/databricks_guide/job_stages_ui.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.433824637304E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"vida","iPythonMetadata":null,"nuid":"e0e64185-7acb-4b6d-83f3-511d9773bec0"},{"version":"CommandV1","origId":3899,"guid":"0e3648bb-27bb-4572-9bcd-ad73647d0a5f","subtype":"command","commandType":"auto","position":6.25,"command":"%md\n\nFind the stage that is of interest from the Active, Pending, or Completed list of stages.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.426212092548E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"1eccb1e0-b127-4e5d-a3be-3364033ef893"},{"version":"CommandV1","origId":3900,"guid":"39468c35-c5bf-43d3-9c07-2d63e723bc17","subtype":"command","commandType":"auto","position":6.5,"command":"%md ### **Step 4:** Click on the link on the **Description** column to view the **Stage Details** Page.\n\n![Spark Stage Details UI Page](http://training.databricks.com/databricks_guide/stage_details_ui.png)\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.426212096774E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"df962df4-76a7-4a30-9cd1-d135db0c0035"},{"version":"CommandV1","origId":3901,"guid":"d15a3128-deed-491b-8a82-f3239330f680","subtype":"command","commandType":"auto","position":6.75,"command":"%md \n\n#### **Summary Metrics** provides aggregate statistics about your tasks. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.426212216079E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"6891fb61-c011-4e70-b59c-fb429f42517a"},{"version":"CommandV1","origId":3902,"guid":"daedb80e-ac52-42a4-8299-0da1950f993d","subtype":"command","commandType":"auto","position":6.875,"command":"%md\n\n#### **Aggregated Metrics By Executor** provides aggregate statistics about tasks assigned on a given worker node.  \nThis is one way to detect if one of your worker nodes is causing problems for your Spark job.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.426212212312E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"d88e7bb7-4c93-4c19-9f55-7584f9c1b41c"},{"version":"CommandV1","origId":3903,"guid":"10ec1ea2-5cca-4959-aea4-d05c95d759fa","subtype":"command","commandType":"auto","position":6.9375,"command":"%md\n\n#### **Tasks** shows a breakdown of all individual tasks that have run or is currently runninng for your Spark Job.\n* Tasks that have not yet been scheduled will not appear in the list.\n* The **ID** column provides a unique ID for the task.\n   * Sometimes you will see the ID and **(speculative)** next to it.  This is to tell you that this task was launched more than once because the original task was taking much longer than the other tasks.\n* The **Status** column tells you if a task was successful, failed, or is currently running.\n* The **Duration** column tells you how long a task ran for or has currently been running for.\n   * Look at the duration column to tell if you have a task that is taking a particularly long time.\n* The **GC Time** column will let you know if your task are taking too long for GC.\n* The **Input** column lets you know how much input was given to this task.\n   * Use this column to detect if you have uneven sharding in your Spark job where some tasks get a disportionate amount of the input.\n* The **Shuffle Write** is another column to detect uneven sharding problems.\n* The **Error** column will contain the first part of the stack trace if your task fails.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.426212209197E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"eb556b1d-0617-4054-9922-58c7460155c5"},{"version":"CommandV1","origId":3904,"guid":"9ae6757a-1f8a-4b18-ac8b-7667c9a67271","subtype":"command","commandType":"auto","position":7.0,"command":"%md ### **Step 5:** It can also be useful to view the worker logs for more output. \n* Begin by expanding the nodes on the cluster page for your cluster of choice. \n* You can go directly to an individual **Worker** page.\n* Or you can click on the **Master** Page, which contains links to the individal worker pages. \n\n![Spark Cluster Node](http://training.databricks.com/databricks_guide/show_master_link.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.426212173618E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":null,"iPythonMetadata":null,"nuid":"4b3aaac8-c34d-4a15-88f8-c07b30a0dca0"},{"version":"CommandV1","origId":3905,"guid":"a2bf722a-24ab-46bf-9fe1-f9d41c0ee269","subtype":"command","commandType":"auto","position":8.0,"command":"%md\n\n#### From the **Spark Worker** page, there are links to the **log4j**, **stdout**, and **stderr** logs.\n* These logs often contain the most log information for debugging failed jobs.\n* If you use `print` statements or `log4j` in your application, this is where their output will appear.\n\n![Spark Worker Page](http://training.databricks.com/databricks_guide/worker_page.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.434147144013E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"matei","iPythonMetadata":null,"nuid":"f2a9f04a-4faa-47fa-814d-ba500ac133cc"}],"guid":"93717f14-6779-4c09-97c2-16c2ee57d434","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"317be72c-963b-4922-ac68-74cae387a2ae","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>