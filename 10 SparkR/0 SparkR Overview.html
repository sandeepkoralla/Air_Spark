<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>SparkR / SparkR Overview - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":2028,"name":"SparkR / SparkR Overview","language":"r","commands":[{"version":"CommandV1","origId":2029,"guid":"7f22d8f9-f810-44a5-a684-61a6656f6050","subtype":"command","commandType":"auto","position":1.0,"command":"%md #SparkR Overview\n\nSparkR is an R package that provides a light-weight frontend to use Apache Spark from R. In Spark 1.5.1, SparkR provides a distributed DataFrame implementation that supports operations like selection, filtering, aggregation etc. (similar to R data frames, dplyr) but on large datasets. SparkR also supports distributed machine learning using MLlib.\n\n####*Note*: Do not create a SparkContext or SQLContext yourself\nThe `SparkContext` and `SQLContext` is already created for you. Recreating one could cause unintended side effects.\n\nWe will cover the below topics in this overview:\n- Creating SparkR DataFrames\n  - From Local R Data Frames\n  - From Data Sources using Spark SQL\n    - Using Data Source Connectors with Spark Packages\n  - From Spark SQL Queries\n- DataFrame Operations\n  - Selecting Rows & Columns\n  - Grouping & Aggregation\n  - Column Operations\n- Machine Learning","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"d3d13424-87d6-4698-8f1b-0ccb418ca9b4"},{"version":"CommandV1","origId":2030,"guid":"f6655b4f-aa74-4b45-b9de-9e110488817c","subtype":"command","commandType":"auto","position":2.0,"command":"%md \n## Creating SparkR DataFrames\nWith a SQLContext, applications can create DataFrames from a local R data frame, from data sources, or using Spark SQL queries.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.445902032353E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"6381bcbb-c920-4f27-b45b-9b98d845550f"},{"version":"CommandV1","origId":2031,"guid":"4cfde8c2-de7b-46a3-ab09-ec7adaf08f83","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n### From Local R Data Frames\n\nThe simplest way to create a data frame is to convert a local R data frame into a SparkR DataFrame. Specifically we can use createDataFrame and pass in the local R data frame to create a SparkR DataFrame. As an example, the following creates a DataFrame using the faithful dataset from R.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"9300c0a6-41f8-4b2f-8604-beedc6439cc6"},{"version":"CommandV1","origId":2032,"guid":"378d1bf2-d0c6-432d-82e5-6718e60c357b","subtype":"command","commandType":"auto","position":4.0,"command":"df <- createDataFrame(sqlContext, faithful) \n\n# Displays the content of the DataFrame to stdout\nhead(df)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  eruptions waiting\n1     3.600      79\n2     1.800      54\n3     3.333      74\n4     2.283      62\n5     4.533      85\n6     2.883      55</pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.44590662727E12,"submitTime":1.445906624591E12,"finishTime":1.445906628134E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"279cae81-77ec-4aee-bf22-af5c5b233759"},{"version":"CommandV1","origId":2033,"guid":"0d2865d0-5a2b-48d4-8056-48f327482c5d","subtype":"command","commandType":"auto","position":5.0,"command":"%md\n### From Data Sources using Spark SQL\n\nThe general method for creating DataFrames from data sources is read.df. This method takes in the SQLContext, the path for the file to load and the type of data source. SparkR supports reading JSON and Parquet files natively and through Spark Packages you can find data source connectors for popular file formats like CSV and Avro.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"f343919e-766b-4546-bfa3-9cff4857d0ce"},{"version":"CommandV1","origId":2034,"guid":"5424855e-1056-4b15-98e6-3b035fab5520","subtype":"command","commandType":"auto","position":5.25,"command":"%fs rm dbfs:/tmp/people.json","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res24: Boolean = true\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445965665239E12,"submitTime":1.445965663993E12,"finishTime":1.445965665398E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"bb356afb-e288-4bce-896d-1f93508f2888"},{"version":"CommandV1","origId":2035,"guid":"9f93efa3-9814-4b4a-a629-031aaad953e6","subtype":"command","commandType":"auto","position":5.5,"command":"%fs put dbfs:/tmp/people.json \n'{\"age\": 10, \"name\": \"John\"}\n{\"age\": 20, \"name\": \"Jane\"}\n{\"age\": 30, \"name\": \"Andy\"}'","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Wrote 83 bytes.\nres25: Boolean = true\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:25: error: too many arguments for method put: (file: String, contents: String, overwrite: Boolean)Boolean\n              dbutils.fs.put(&quot;dbfs:/tmp/people.json&quot;, &quot;{age:&quot;, &quot;10,&quot;, &quot;name:&quot;, &quot;John}&quot;, &quot;{age:&quot;, &quot;20,&quot;, &quot;name:&quot;, &quot;Jane}&quot;, &quot;{age:&quot;, &quot;30,&quot;, &quot;name:&quot;, &quot;Andy}&quot;)\n                            ^\n</div>","error":null,"startTime":1.445965665805E12,"submitTime":1.445965664581E12,"finishTime":1.445965665946E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"78444cee-2591-4b0c-96a4-cc5f48592bca"},{"version":"CommandV1","origId":2036,"guid":"f050a52b-bbeb-4fb8-968e-1032ceb105ee","subtype":"command","commandType":"auto","position":6.0,"command":"people <- read.df(sqlContext, \"dbfs:/tmp/people.json\", source=\"json\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'></pre>","arguments":{},"plotOptions":null},"errorSummary":"<code style=\"font-size:10p\"> Error in invokeJava(isStatic = TRUE, className, methodName, ...) :  </code>","error":"<pre style=\"font-size:10p\">Error in invokeJava(isStatic = TRUE, className, methodName, ...) : \n  java.io.IOException: No input paths specified in job\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:156)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1091)\n\tat org.apache.spark.sql.execution.datasources.json.InferSchema$.apply(InferSchema.scala:58)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation$$anonfun$6.apply(JSONRelation.scala:105)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation$$anonfun$6.apply(JSONRelation.scala:100)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation.dataSchema$lzycompute(JSONRelation.scala:100)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation.dataSchema(JSONRelation.scala:99)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:561)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:560)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.&lt;init&gt;(LogicalRelation.scala:37)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:120)\n\tat org.apache.spark.sql.api.r.SQLUtils$.loadDF(SQLUtils.scala:156)\n\tat org.apache.spark.sql.api.r.SQLUtils.loadDF(SQLUtils.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:132)\n\tat org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:79)\n\tat org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)\n\tat java.lang.Thread.run(Thread.java:745)\n</pre>","startTime":1.445965666321E12,"submitTime":1.445965665096E12,"finishTime":1.445965666494E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"fada75ec-6436-4955-a303-b8318706e4b7"},{"version":"CommandV1","origId":2037,"guid":"ba9251c5-ef31-48fe-a287-e1b6fb0b5085","subtype":"command","commandType":"auto","position":6.0625,"command":"%md SparkR automatically infers the schema from the JSON file.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"d3d9a1f4-27ea-43d5-b542-7159ec9b0ee5"},{"version":"CommandV1","origId":2038,"guid":"e5045016-8e64-4547-b873-c464e2df1caf","subtype":"command","commandType":"auto","position":6.125,"command":"printSchema(people)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)</pre>","arguments":{},"plotOptions":null},"errorSummary":"<code style=\"font-size:10p\"> Error in show(collect(panda)) :  </code>","error":"<pre style=\"font-size:10p\">Error in show(collect(panda)) : \n  error in evaluating the argument 'object' in selecting a method for function 'show': Error in as.data.frame.default(x[[i]], optional = TRUE) : \n  cannot coerce class &quot;&quot;jobj&quot;&quot; to a data.frame\nCalls: collect ... data.frame -&gt; as.data.frame -&gt; as.data.frame.default\n</pre>","startTime":1.445965666845E12,"submitTime":1.445965665626E12,"finishTime":1.44596566685E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"7639e4dc-2301-4718-9435-9141b395b14e"},{"version":"CommandV1","origId":2039,"guid":"049a255c-8244-4297-95f3-87f002f44881","subtype":"command","commandType":"auto","position":6.234375,"command":"display(people)","commandVersion":0,"state":"finished","results":{"type":"table","data":[[10.0,"John"],[20.0,"Jane"],[30.0,"Andy"]],"arguments":{},"schema":[{"type":"double","name":"age"},{"type":"string","name":"name"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":null,"startTime":1.445965667376E12,"submitTime":1.445965666161E12,"finishTime":1.445965667574E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"036fe66e-ad20-40b6-be8e-95b2bcdbff9e"},{"version":"CommandV1","origId":2040,"guid":"b61e0c45-5b8a-425f-8e7f-8b36b9b7730b","subtype":"command","commandType":"auto","position":7.1171875,"command":"%md\n#### Using Data Source Connectors with Spark Packages\nAs an example, we will use the Spark CSV package to load a CSV file. You can find a [list of Databricks Spark Packages here](http://spark-packages.org/user/databricks).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"56656be4-8048-4fd2-96f0-6658b6937196"},{"version":"CommandV1","origId":2041,"guid":"935024b4-6e2e-445f-8803-04afbfe4c050","subtype":"command","commandType":"auto","position":7.55859375,"command":"Sys.setenv('SPARKR_SUBMIT_ARGS'='\"--packages\" \"com.databricks:spark-csv_2.10:1.2.0\" \"sparkr-shell\"') # Specify package\ndiamonds <- read.df(sqlContext, \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\",\n                    source = \"com.databricks.spark.csv\", header=\"true\", inferSchema = \"true\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'></pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445972164157E12,"submitTime":1.445972162608E12,"finishTime":1.445972167063E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"ae2866ee-4df4-4eab-ae93-ae8bbac50927"},{"version":"CommandV1","origId":2042,"guid":"5be03aa4-f3d4-4947-b08c-b883a71bbd0c","subtype":"command","commandType":"auto","position":7.779296875,"command":"head(diamonds)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  1:6 carat       cut color clarity depth table price    x    y    z\n1   1  0.23     Ideal     E     SI2  61.5    55   326 3.95 3.98 2.43\n2   2  0.21   Premium     E     SI1  59.8    61   326 3.89 3.84 2.31\n3   3  0.23      Good     E     VS1  56.9    65   327 4.05 4.07 2.31\n4   4  0.29   Premium     I     VS2  62.4    58   334 4.20 4.23 2.63\n5   5  0.31      Good     J     SI2  63.3    58   335 4.34 4.35 2.75\n6   6  0.24 Very Good     J    VVS2  62.8    57   336 3.94 3.96 2.48</pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445972173337E12,"submitTime":1.445972172026E12,"finishTime":1.445972173555E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"8a53f050-e4b5-481c-a6f6-234b927b9ec3"},{"version":"CommandV1","origId":2043,"guid":"ed4da21c-5686-411a-8015-2f3a2f02b185","subtype":"command","commandType":"auto","position":8.0,"command":"%md The data sources API can also be used to save out DataFrames into multiple file formats. For example we can save the DataFrame from the previous example to a Parquet file using write.df","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.445965751933E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"b0f7581c-6668-48f0-8db2-cbc8eb52e70b"},{"version":"CommandV1","origId":2044,"guid":"e27fc28c-cbed-4542-82b0-714a80bd7cbd","subtype":"command","commandType":"auto","position":8.5,"command":"%fs rm -r dbfs:/tmp/people.parquet","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res1: Boolean = true\n</div>","arguments":{},"plotOptions":null},"errorSummary":"java.io.IOException: Cannot delete /databricks-staging-storage-oregon/shard-dogfood/0/tmp/people.parquet as it is not an empty directory and recurse option is false","error":"<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:65)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:40)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.delete(DatabricksFileSystem.scala:171)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.rm(DBUtilsCore.scala:68)\n\tat com.databricks.dbutils_v1.package$fs$.rm(dbutils_v1.scala:54)\nCaused by: java.io.IOException: Cannot delete /databricks-staging-storage-oregon/shard-dogfood/0/tmp/people.parquet as it is not an empty directory and recurse option is false\n\tat com.databricks.backend.daemon.data.server.backend.S3FSBackend.delete(S3FSBackend.scala:84)\n\tat com.databricks.backend.daemon.data.server.backend.RootFileSystemBackend.delete(RootFileSystemBackend.scala:91)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.receive(FileSystemRequestHandler.scala:26)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext$$anonfun$queryHandlers$1.apply(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext$$anonfun$queryHandlers$1.apply(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:171)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:155)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n\tat com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.applyOrElse(ServerBackend.scala:44)\n\tat com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.applyOrElse(ServerBackend.scala:39)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:57)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:57)\n\tat scala.PartialFunction$OrElse.apply(PartialFunction.scala:162)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$7.apply(JettyServer.scala:263)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:263)\n\tat com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:206)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:152)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:143)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:143)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:71)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:69)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:83)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:98)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:83)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:142)\n\tat com.databricks.rpc.JettyServer$RequestManager.doGet(JettyServer.scala:98)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:735)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:430)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)</div>","startTime":1.446065842757E12,"submitTime":1.446065840561E12,"finishTime":1.446065844088E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"7d9c5a79-3940-4f63-abf7-91a434b978f7"},{"version":"CommandV1","origId":2045,"guid":"b5b21424-1c2e-4dde-a524-a29e74f56032","subtype":"command","commandType":"auto","position":9.0,"command":"write.df(people, path=\"dbfs:/tmp/people.parquet\", source=\"parquet\", mode=\"overwrite\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>NULL</pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445965860049E12,"submitTime":1.445965858702E12,"finishTime":1.445965862181E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"65d16d7f-da09-4986-b2fa-0c9efa4f12c3"},{"version":"CommandV1","origId":2046,"guid":"616e5a60-2c4f-49d1-b2ab-6fd85a89042f","subtype":"command","commandType":"auto","position":10.0,"command":"%fs ls dbfs:/tmp/people.parquet","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/tmp/people.parquet/_SUCCESS","_SUCCESS",0.0],["dbfs:/tmp/people.parquet/_common_metadata","_common_metadata",312.0],["dbfs:/tmp/people.parquet/_metadata","_metadata",778.0],["dbfs:/tmp/people.parquet/part-r-00000-9014e91f-a010-4d04-82ca-7841c8e4d3a7.gz.parquet","part-r-00000-9014e91f-a010-4d04-82ca-7841c8e4d3a7.gz.parquet",564.0],["dbfs:/tmp/people.parquet/part-r-00001-9014e91f-a010-4d04-82ca-7841c8e4d3a7.gz.parquet","part-r-00001-9014e91f-a010-4d04-82ca-7841c8e4d3a7.gz.parquet",556.0]],"arguments":{},"schema":[{"type":"string","name":"path"},{"type":"string","name":"name"},{"type":"bigint","name":"size"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":null,"startTime":1.445966105747E12,"submitTime":1.445966104446E12,"finishTime":1.445966105883E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"032c751b-cc38-4f2f-a7b1-cf04fdc89101"},{"version":"CommandV1","origId":2047,"guid":"90212848-5dc4-442d-a8ab-04a5b4523234","subtype":"command","commandType":"auto","position":11.0,"command":"%md\n### From Spark SQL Queries\nYou can also create SparkR DataFrames using Spark SQL queries.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"a6ee9ad9-a01c-4900-82aa-47977208f9dd"},{"version":"CommandV1","origId":2048,"guid":"3802d2cc-d95a-4b0d-b83f-8361e8015667","subtype":"command","commandType":"auto","position":11.25,"command":"# Register earlier df as temp table\nregisterTempTable(people, \"peopleTemp\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'></pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445971522459E12,"submitTime":1.445971521149E12,"finishTime":1.445971522463E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"bf144680-df00-4954-a182-6344ac8616d4"},{"version":"CommandV1","origId":2049,"guid":"e8c4e900-689b-43c5-9c77-1b78c27defce","subtype":"command","commandType":"auto","position":12.0,"command":"# Create a df consisting of only the 'age' column using a Spark SQL query\nage <- sql(sqlContext, \"SELECT age FROM peopleTemp\")\nhead(age)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  age\n1  10\n2  20\n3  30</pre>","arguments":{},"plotOptions":null},"errorSummary":"<code style=\"font-size:10p\"> Error in invokeJava(isStatic = FALSE, objId$id, methodName, ...) :  </code>","error":"<pre style=\"font-size:10p\">Error in invokeJava(isStatic = FALSE, objId$id, methodName, ...) : \n  org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask\n\tat org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:464)\n\tat org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:449)\n\tat org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:256)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:211)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:248)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.runHive(ClientWrapper.scala:449)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.runSqlHive(ClientWrapper.scala:439)\n\tat org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:558)\n\tat org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:33)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)\n\tat org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:933)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:933)\n\tat org.apache.spark.sql.DataFrame.&lt;init&gt;(DataFrame.scala:144)\n\tat org.apache.spark.sql.DataFrame.&lt;init&gt;(DataFrame.scala:129)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:725)\n\tat sun.reflect.GeneratedMethodAccessor121.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:132)\n\tat org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:79)\n\tat org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)\n\tat java.lang.Thread.run(Thread.java:745)\n</pre>","startTime":1.445971565597E12,"submitTime":1.445971564278E12,"finishTime":1.445971565953E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"5f23270c-16b5-4e05-a91e-48d18f296c76"},{"version":"CommandV1","origId":2050,"guid":"41457258-a33e-4bab-932d-528f09e6388f","subtype":"command","commandType":"auto","position":13.0,"command":"# Resulting df is a SparkR df\nstr(age)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>Formal class 'DataFrame' [package \"SparkR\"] with 2 slots\n  ..@ env:<environment: 0x54388f0> \n  ..@ sdf:Class 'jobj' <environment: 0x5434930> </pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445971584645E12,"submitTime":1.445971583346E12,"finishTime":1.445971584652E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"d8076861-f4e8-48b2-84a7-9c13fcda01b0"},{"version":"CommandV1","origId":2051,"guid":"febe48af-5ebe-4f98-ac40-59d881e2d61c","subtype":"command","commandType":"auto","position":14.0,"command":"%md ## DataFrame Operations\n\nSparkR DataFrames support a number of functions to do structured data processing. Here we include some basic examples and a complete list can be found in the [API docs](https://spark.apache.org/docs/latest/api/R/).\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"98573e16-038a-4f08-9474-55daa6b2ef3d"},{"version":"CommandV1","origId":2052,"guid":"d53f8fc5-17fa-4b29-bfc9-15210fd53f22","subtype":"command","commandType":"auto","position":14.5,"command":"%md ### Selecting Rows & Columns","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"0c3fcd39-9c1b-4b2e-9428-76f57ecd2340"},{"version":"CommandV1","origId":2053,"guid":"e0c32f6e-fd1e-4d9e-a044-868b78d95d3d","subtype":"command","commandType":"auto","position":15.0,"command":"# Create DataFrame\ndf <- createDataFrame(sqlContext, faithful)\ndf","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>DataFrame[eruptions:double, waiting:double]</pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.446057619576E12,"submitTime":1.446057616929E12,"finishTime":1.446057619757E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"1258787a-3057-4807-9a8a-6389c7fa098f"},{"version":"CommandV1","origId":2054,"guid":"ab5a2d2c-feca-445d-87c9-8616936d1818","subtype":"command","commandType":"auto","position":16.0,"command":"# Select only the \"eruptions\" column\nhead(select(df, df$eruptions))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  eruptions\n1     3.600\n2     1.800\n3     3.333\n4     2.283\n5     4.533\n6     2.883</pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445986547072E12,"submitTime":1.445986545691E12,"finishTime":1.445986549118E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"b7f0921b-5945-4ea3-a3b8-cb5a843ed1c1"},{"version":"CommandV1","origId":2055,"guid":"4c735c71-6a29-4040-ae01-9e897b0c355d","subtype":"command","commandType":"auto","position":17.0,"command":"# You can also pass in column name as strings\nhead(select(df, \"eruptions\"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  eruptions\n1     3.600\n2     1.800\n3     3.333\n4     2.283\n5     4.533\n6     2.883</pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445986549126E12,"submitTime":1.445986546127E12,"finishTime":1.4459865528E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"3d3b3021-abf0-4b65-965c-4b869e2cd57e"},{"version":"CommandV1","origId":2056,"guid":"2599879c-ab82-4e51-88cd-faca3151cc90","subtype":"command","commandType":"auto","position":18.0,"command":"# Filter the DataFrame to only retain rows with wait times shorter than 50 mins\nhead(filter(df, df$waiting < 50))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  eruptions waiting\n1     1.750      47\n2     1.750      47\n3     1.867      48\n4     1.750      48\n5     2.167      48\n6     2.100      49</pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445986552811E12,"submitTime":1.445986546631E12,"finishTime":1.445986552961E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"eaffc1d2-346b-41f4-9e73-b90c58a10001"},{"version":"CommandV1","origId":2057,"guid":"ee3d493d-e216-42b8-a410-a3a91fd87de9","subtype":"command","commandType":"auto","position":19.0,"command":"%md ### Grouping & Aggregation\n\nSparkR DataFrames support a number of commonly used functions to aggregate data after grouping. For example we can count the number of times each waiting time appears in the faithful dataset.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"90f116ce-60b0-48b7-b825-fc2c1ee30681"},{"version":"CommandV1","origId":2058,"guid":"60ed6442-a4bd-492e-a289-c708cc248a97","subtype":"command","commandType":"auto","position":20.0,"command":"head(count(groupBy(df, df$waiting)))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  waiting count\n1      81    13\n2      60     6\n3      93     2\n4      68     1\n5      47     4\n6      80     8</pre>","arguments":{},"plotOptions":null},"errorSummary":"<code style=\"font-size:10p\"> Error in x[seq_len(n)] : object of type 'S4' is not subsettable </code>","error":"<pre style=\"font-size:10p\">Error in x[seq_len(n)] : object of type 'S4' is not subsettable</pre>","startTime":1.445986552969E12,"submitTime":1.445986547623E12,"finishTime":1.445986554589E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"9af02b01-e362-4538-b680-b52784fa6973"},{"version":"CommandV1","origId":2059,"guid":"d9e25914-021e-4cf9-ba8e-9aea4a9f9587","subtype":"command","commandType":"auto","position":22.0,"command":"# We can also sort the output from the aggregation to get the most common waiting times\nwaiting_counts <- count(groupBy(df, df$waiting))\nhead(arrange(waiting_counts, desc(waiting_counts$count)))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  waiting count\n1      78    15\n2      83    14\n3      81    13\n4      77    12\n5      82    12\n6      84    10</pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445986554598E12,"submitTime":1.445986548117E12,"finishTime":1.44598655551E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"0e21d98a-ef5e-4e86-8e59-53825d09d8e1"},{"version":"CommandV1","origId":2060,"guid":"b2afabf9-8586-4873-a5c4-50c41f7cb557","subtype":"command","commandType":"auto","position":23.0,"command":"%md ### Column Operations\n\nSparkR provides a number of functions that can be directly applied to columns for data processing and aggregation. The example below shows the use of basic arithmetic functions.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"c7e31843-8fa0-4762-9a0f-c62e1b678d41"},{"version":"CommandV1","origId":2061,"guid":"1d54da70-474a-4f49-8ea7-7627bec82b77","subtype":"command","commandType":"auto","position":24.0,"command":"# Convert waiting time from hours to seconds.\n# Note that we can assign this to a new column in the same DataFrame\ndf$waiting_secs <- df$waiting * 60\nhead(df)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  eruptions waiting waiting_secs\n1     3.600      79         4740\n2     1.800      54         3240\n3     3.333      74         4440\n4     2.283      62         3720\n5     4.533      85         5100\n6     2.883      55         3300</pre>","arguments":{},"plotOptions":null},"errorSummary":"<code style=\"font-size:10p\"> Error in df$waiting : object of type 'closure' is not subsettable </code>","error":"<pre style=\"font-size:10p\">Error in df$waiting : object of type 'closure' is not subsettable</pre>","startTime":1.445986555518E12,"submitTime":1.445986549502E12,"finishTime":1.445986555675E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"a2405fe3-fce5-4723-a5a0-cb0c3891a89c"},{"version":"CommandV1","origId":2062,"guid":"13b29201-59f6-41a3-840b-6cca5bfdee87","subtype":"command","commandType":"auto","position":25.0,"command":"%md ## Machine Learning\n\nAs of Spark 1.5, SparkR allows the fitting of generalized linear models over SparkR DataFrames using the glm() function. Under the hood, SparkR uses MLlib to train a model of the specified family. We support a subset of the available R formula operators for model fitting, including ~, ., +, and -.\n\nUnder the hood, SparkR automatically performs one-hot encoding of categorical features so that it does not need to be done manually. Beyond String and Double type features, it is also possible to fit over MLlib Vector features, for compatibility with other MLlib components.\n\nThe example below shows the use of building a gaussian GLM model using SparkR. To run Linear Regression, set family to \"gaussian\". To run Logistic Regression, set family to \"binomial\".","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.445986573781E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"d5cbd7f5-0d8c-4010-bb8e-d8a8d3a54b74"},{"version":"CommandV1","origId":2063,"guid":"1b5647a2-e5b6-4af4-9130-a80fc3e87baa","subtype":"command","commandType":"auto","position":26.0,"command":"# Create the DataFrame\ndf <- createDataFrame(sqlContext, iris)\n\n# Fit a linear model over the dataset.\nmodel <- glm(Sepal_Length ~ Sepal_Width + Species, data = df, family = \"gaussian\")\n\n# Model coefficients are returned in a similar format to R's native glm().\nsummary(model)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>$coefficients\n                     Estimate\n(Intercept)         2.2513930\nSepal_Width         0.8035609\nSpecies__versicolor 1.4587432\nSpecies__virginica  1.9468169\n</pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445986657481E12,"submitTime":1.445986657181E12,"finishTime":1.445986660359E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"c8e06c0a-b4c2-4dd0-882d-49dd83111788"},{"version":"CommandV1","origId":2064,"guid":"f118d32c-5b79-438e-b9b0-f1f1cc4b62d1","subtype":"command","commandType":"auto","position":27.0,"command":"# Make predictions based on the model.\npredictions <- predict(model, newData = df)\nhead(select(predictions, \"Sepal_Length\", \"prediction\"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  Sepal_Length prediction\n1          5.1   5.063856\n2          4.9   4.662076\n3          4.7   4.822788\n4          4.6   4.742432\n5          5.0   5.144212\n6          5.4   5.385281</pre>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.445986671814E12,"submitTime":1.445986671446E12,"finishTime":1.445986671946E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"raela@databricks.com","iPythonMetadata":null,"nuid":"12629a5e-a56d-425c-bb88-84c0bfded336"}],"guid":"77651b5f-453d-4f87-9fec-7f9d34dc7470","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"3f939fb0-c473-4ce2-be96-884fa88f80c7","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>