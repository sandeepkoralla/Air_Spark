<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Accessing Data / DataSets / DBFS Hosted Datasets - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":754,"name":"Accessing Data / DataSets / DBFS Hosted Datasets","language":"python","commands":[{"version":"CommandV1","origId":755,"guid":"5b2c5548-3370-4494-be77-3cb38b92987b","subtype":"command","commandType":"auto","position":1.0,"command":"#Create HTML To Render From Dataset README Files in S3\nimport os\nfrom hoedown import HtmlRenderer, Markdown, EXT_TABLES, EXT_STRIKETHROUGH\nextensions = EXT_TABLES|EXT_STRIKETHROUGH\nmd = Markdown(HtmlRenderer(), extensions)\n\n#Create HTML Output for Global README\nmain_readme = md.render(open(\"/dbfs/databricks-datasets/README.md\").read()).replace(\"h2\",\"h3\")\n\n#Create HTML Output from all READMEs\ndatasets_location = \"/dbfs/databricks-datasets/\"\ndatasets_readmes = \"\"\n\nfor f in os.listdir(datasets_location):\n  if (f != \"README.md\"):\n    ds = \"%(datasets_location)s%(f)s\" % locals()\n    this_r = md.render(\"<hr/>\")\n    this_r += md.render(\"<pre>dbfs:/databricks-datasets/%(f)s</pre>\" % locals())\n    try:\n      r = md.render(open(\"%(ds)s/README.md\" % locals()).read().decode(\"utf-8\"))\n    except:\n      r = \"<p><em>Error processing readme</em></p>\"\n    this_r += r\n    datasets_readmes += this_r\n\ndatasets_readmes = datasets_readmes.replace(\"h2\",\"h3\").replace(\"h1\",\"h2\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":"<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> EOL while scanning string literal","error":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;ipython-input-23-ac4e7a9c3a74&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">11</span>\n<span class=\"ansiyellow\">    datasets_location = dbfs/databricks-datasets/&quot;</span>\n<span class=\"ansigrey\">                                                 ^</span>\n<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> EOL while scanning string literal\n\n</div>","startTime":1.44788247607E12,"submitTime":1.447882476363E12,"finishTime":1.447882480947E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"rlgarris@databricks.com","iPythonMetadata":null,"nuid":"ccd58390-95e1-4de8-9d22-4b86d8b1be3f"},{"version":"CommandV1","origId":756,"guid":"3797ec3c-5c7d-498c-9ac0-f2a00b77a500","subtype":"command","commandType":"auto","position":2.375,"command":"displayHTML(main_readme)","commandVersion":0,"state":"finished","results":{"type":"htmlSandbox","data":"<h1>Databricks Hosted Datasets</h1>\n\n<p>The data contained within this directory is hosted for users to build \ndata pipelines using Apache Spark and Databricks Cloud.</p>\n\n<h3>License</h3>\n\n<p>Unless otherwise noted (e.g. within the README for a given data set), the data \nis licensed under Creative Commons Attribution 4.0 International (CC BY 4.0),\nwhich can be viewed at the following url:\n<a href=\"http://creativecommons.org/licenses/by/4.0/legalcode\">http://creativecommons.org/licenses/by/4.0/legalcode</a></p>\n\n<h3>Contributions and Requests</h3>\n\n<p>To request or contribute new datasets to this repository, please send an email\nto: hosted-datasets@databricks.com.</p>\n\n<p>When making the request, include the README.md file you want to publish. Make\nsure the file includes information about the source of the data, the license, \nand how to get additional information. Please ensure the license for this \ndata allows it to be hosted by Databricks and consumed by the public.</p>\n","arguments":{},"plotOptions":null},"errorSummary":"Command skipped","error":null,"startTime":1.44788248095E12,"submitTime":1.447882476405E12,"finishTime":1.44788248098E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"rlgarris@databricks.com","iPythonMetadata":null,"nuid":"ebeac968-a97d-4c0d-9400-2f2a84c4d9fe"},{"version":"CommandV1","origId":757,"guid":"8fe561b1-174b-4021-a7ef-dbc6210145a0","subtype":"command","commandType":"auto","position":2.8125,"command":"%md\n## Datasets Available Now\n\nThe hosted datasets are automatically mounted and available through DBFS. You can explore them using the [dbutils](../../02 Product Overview/09 DB File System - py.html) command and looking in the `/databricks-datasets` mount point: \n\n    %fs ls /databricks-datasets","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":1.44788247647E12,"submitTime":1.44788247647E12,"finishTime":1.447882476518E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"rlgarris@databricks.com","iPythonMetadata":null,"nuid":"1fa14259-e4ba-4481-831e-bc8a9cc8dac4"},{"version":"CommandV1","origId":758,"guid":"533e8fcd-ed6f-4e4c-845a-0dc42d97da41","subtype":"command","commandType":"auto","position":2.84375,"command":"%fs ls /databricks-datasets","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/databricks-datasets/README.md","README.md",982.0],["dbfs:/databricks-datasets/Rdatasets/","Rdatasets/",0.0],["dbfs:/databricks-datasets/adult/","adult/",0.0],["dbfs:/databricks-datasets/airlines/","airlines/",0.0],["dbfs:/databricks-datasets/data-001/","data-001/",0.0],["dbfs:/databricks-datasets/learning-spark/","learning-spark/",0.0],["dbfs:/databricks-datasets/power-plant/","power-plant/",0.0],["dbfs:/databricks-datasets/sms_spam_collection/","sms_spam_collection/",0.0],["dbfs:/databricks-datasets/tpch/","tpch/",0.0],["dbfs:/databricks-datasets/wiki/","wiki/",0.0]],"arguments":{},"schema":[{"type":"string","name":"path"},{"type":"string","name":"name"},{"type":"bigint","name":"size"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"Command skipped","error":null,"startTime":1.447882480998E12,"submitTime":1.447882476554E12,"finishTime":1.447882482772E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"1181","height":"305","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"rlgarris@databricks.com","iPythonMetadata":null,"nuid":"2987ee2c-2a1d-48af-bf6a-dc9d2e6e63e9"},{"version":"CommandV1","origId":759,"guid":"c8df016d-793d-4fb1-aff4-2eaba66ab32d","subtype":"command","commandType":"auto","position":2.875,"command":"displayHTML(datasets_readmes)","commandVersion":0,"state":"finished","results":{"type":"htmlSandbox","data":"<hr/>\n<pre>dbfs:/databricks-datasets/Rdatasets</pre>\n<h2><a href=\"http://vincentarelbundock.github.io/Rdatasets/\">Rdatasets</a></h2>\n\n<p><code>Rdatasets</code> is a collection of 747 datasets that were originally distributed alongside the statistical software environment <code>R</code> and some of its add-on packages. The goal is to make these data more broadly accessible for teaching and statistical software development.</p>\n\n<p>The list of available datasets (csv and docs) is available here:</p>\n\n<ul>\n<li><a href=\"http://vincentarelbundock.github.com/Rdatasets/datasets.html\">HTML index</a></li>\n<li><a href=\"http://vincentarelbundock.github.com/Rdatasets/datasets.csv\">CSV index</a></li>\n</ul>\n\n<p>For more information, please see the README file within the latest <code>data</code> subdirectory</p>\n\n<h3>Versions</h3>\n\n<ul>\n<li>data-001 is from the git hash: aa0d6940a9</li>\n</ul>\n<hr/>\n<pre>dbfs:/databricks-datasets/adult</pre>\n<h2>Adult Dataset</h2>\n\n<p>This data was extracted from the <a href=\"http://www.census.gov/ftp/pub/DES/www/welcome.html\">census bureau database</a><br>\nDonor: Ronny Kohavi and Barry Becker,<br>\n     Data Mining and Visualization<br>\n     Silicon Graphics.<br>\n     e-mail: ronnyk@sgi.com for questions.<br>\nSplit into train-test using MLC++ GenCVFiles (2/3, 1/3 random).<br>\n48842 instances, mix of continuous and discrete    (train=32561, test=16281)<br>\n45222 if instances with unknown values are removed (train=30162, test=15060)<br>\nDuplicate or conflicting instances : 6<br>\nClass probabilities for adult.all file<br>\nProbability for the label &#39;&gt;50K&#39;  : 23.93% / 24.78% (without unknowns)<br>\nProbability for the label &#39;&lt;=50K&#39; : 76.07% / 75.22% (without unknowns)  </p>\n\n<p>Extraction was done by Barry Becker from the 1994 Census database.  A set of<br>\nreasonably clean records was extracted using the following conditions:<br>\n((AAGE&gt;16) &amp;&amp; (AGI&gt;100) &amp;&amp; (AFNLWGT&gt;1)&amp;&amp; (HRSWK&gt;0))  </p>\n\n<p>Prediction task is to determine whether a person makes over 50K a year.  </p>\n\n<p>First cited in:<br>\n@inproceedings{kohavi-nbtree,<br>\n author={Ron Kohavi},<br>\n title={Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid},<br>\n booktitle={Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},<br>\n year = 1996,<br>\n pages={to appear}}  </p>\n\n<p>Error Accuracy reported as follows, after removal of unknowns from train/test sets):<br>\n C4.5       : 84.46+-0.30<br>\n Naive-Bayes: 83.88+-0.30<br>\n NBTree     : 85.90+-0.28  </p>\n\n<p>Following algorithms were later run with the following error rates, all after removal of unknowns and using the original train/test split.<br>\nAll these numbers are straight runs using MLC++ with default values.  </p>\n\n<p>Algorithm|Error  </p>\n\n<ul>\n<li>1  C4.5                    15.54</li>\n<li>2  C4.5-auto               14.46</li>\n<li>3  C4.5 rules              14.94</li>\n<li>4  Voted ID3 (0.6)         15.64</li>\n<li>5  Voted ID3 (0.8)         16.47</li>\n<li>6  T2                      16.84</li>\n<li>7  1R                      19.54</li>\n<li>8  NBTree                  14.10</li>\n<li>9  CN2                     16.00</li>\n<li>10 HOODG                   14.82</li>\n<li>11 FSS Naive Bayes         14.05</li>\n<li>12 IDTM (Decision table)   14.46</li>\n<li>13 Naive-Bayes             16.12</li>\n<li>14 Nearest-neighbor (1)    21.42</li>\n<li>15 Nearest-neighbor (3)    20.35</li>\n<li>16 OC1                     15.04</li>\n<li>17 Pebls                   Crashed.  Unknown why (bounds WERE increased)</li>\n</ul>\n\n<p>Conversion of original data as follows:<br>\n1. Discretized agrossincome into two ranges with threshold 50,000.<br>\n2. Convert U.S. to US to avoid periods.<br>\n3. Convert Unknown to &quot;?&quot;<br>\n4. Run MLC++ GenCVFiles to generate data,test.  </p>\n\n<p>Description of fnlwgt (final weight)  </p>\n\n<p>The weights on the CPS files are controlled to independent estimates of the<br>\ncivilian noninstitutional population of the US.  These are prepared monthly<br>\nfor us by Population Division here at the Census Bureau.  We use 3 sets of<br>\ncontrols.<br>\nThese are:<br>\n       1.  A single cell estimate of the population 16+ for each state.<br>\n       2.  Controls for Hispanic Origin by age and sex.<br>\n       3.  Controls by Race, age and sex.  </p>\n\n<p>We use all three sets of controls in our weighting program and &quot;rake&quot; through them 6 times so that by the end we come back to all the controls we used.  </p>\n\n<p>The term estimate refers to population totals derived from CPS by creating &quot;weighted tallies&quot; of any specified socio-economic characteristics of the population.  </p>\n\n<p>People with similar demographic characteristics should have similar weights.  There is one important caveat to remember about this statement.  That is that since the CPS sample is actually a collection of 51 state samples, each with its own probability of selection, the statement only applies within state.  </p>\n\n<ul>\n<li><p>label: &gt;50K, &lt;=50K  </p></li>\n<li><p>age: continuous.</p></li>\n<li><p>workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.</p></li>\n<li><p>fnlwgt: continuous.</p></li>\n<li><p>education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.</p></li>\n<li><p>education-num: continuous.</p></li>\n<li><p>marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.</p></li>\n<li><p>occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.</p></li>\n<li><p>relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.</p></li>\n<li><p>race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.</p></li>\n<li><p>sex: Female, Male.</p></li>\n<li><p>capital-gain: continuous.</p></li>\n<li><p>capital-loss: continuous.</p></li>\n<li><p>hours-per-week: continuous.</p></li>\n<li><p>native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;Tobago, Peru, Hong, Holand-Netherlands.</p></li>\n</ul>\n<hr/>\n<pre>dbfs:/databricks-datasets/airlines</pre>\n<p><em>Error processing readme</em></p><hr/>\n<pre>dbfs:/databricks-datasets/data-001</pre>\n<p><em>Error processing readme</em></p><hr/>\n<pre>dbfs:/databricks-datasets/learning-spark</pre>\n<h2><a href=\"https://github.com/databricks/learning-spark/tree/master/files\">Learning Spark - Example Data From The Book</a> </h2>\n\n<p>This dataset holds the files for examples in the Learning Spark book. These examples are used throughout\nthe book.</p>\n\n<p>For more information, please see the \n<a href=\"https://github.com/databricks/learning-spark/blob/master/README.md\">README from the Learning Spark github project</a> </p>\n\n<h3>License</h3>\n\n<p>The files in the Learning Spark github project are licensed with the\nMIT license as defined in https://github.com/databricks/learning-spark/blob/master/LICENSE.md</p>\n\n<h3>Versions</h3>\n\n<ul>\n<li>data-001 is from the git hash: 13c39f22b1</li>\n</ul>\n<hr/>\n<pre>dbfs:/databricks-datasets/power-plant</pre>\n<p><em>Error processing readme</em></p><hr/>\n<pre>dbfs:/databricks-datasets/sms_spam_collection</pre>\n<h2>%md SMS Spam Collection v. 1</h2>\n\n<p>The SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-enconded messages, tagged according being legitimate (ham) or spam.</p>\n\n<h3>Composition</h3>\n\n<p>This corpus has been collected from free or free for research sources at the Internet:</p>\n\n<ul>\n<li>A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http://www.grumbletext.co.uk/.</li>\n<li>A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/.</li>\n<li>A list of 450 SMS ham messages collected from Caroline Tag&#39;s PhD Thesis available at http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf.\nFinally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available at: http://www.esp.uem.es/jmgomez/smsspamcorpus/.</li>\n</ul>\n\n<p>You can find more useful information about the SMS Spam Collection v.1 at the following page of the UCI Repository.</p>\n\n<p>http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</p>\n\n<h3>Usage</h3>\n\n<p>The collection is composed by just one file, where each line has the correct class (ham or spam) followed by the raw message.</p>\n\n<p><code>\nham   What you doing?how are you?\nham   Ok lar... Joking wif u oni...\nham   dun say so early hor... U c already then say...\nham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\nham   Siva is in hostel aha:-.\nham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\nspam  FreeMsg: Txt: CALL to No: 86888 &amp; claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\nspam  Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B\nspam  URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\n</code></p>\n\n<p>We would appreciate:</p>\n\n<p>If you find this collection useful, make a reference to the paper below and the web page: http://dcomp.sor.ufscar.br/talmeida/smspamcollection/.\nSend us a message either to talmeida &lt; AT &gt; ufscar.br  or jmgomezh <AT> yahoo.es in case you make use of the corpus.</p>\n\n<h3>Publication and More Information</h3>\n\n<p>We offer a comprehensive study of this corpus in the following papers. These works present a number of interesting statistics, studies and baseline results for many traditional machine learning methods.</p>\n\n<p>Almeida, T.A., Gómez Hidalgo, J.M., Yamakami, A. Contributions to the Study of SMS Spam Filtering: New Collection and Results.  Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG&#39;11), Mountain View, CA, USA, 2011. [preprint]</p>\n\n<p>Gómez Hidalgo, J.M., Almeida, T.A., Yamakami, A. On the Validity of a New SMS Spam Collection.  Proceedings of the 11th IEEE International Conference on Machine Learning and Applications (ICMLA&#39;12), Boca Raton, FL, USA, 2012. [preprint]</p>\n\n<p>Almeida, T.A., Gómez Hidalgo, J.M., Silva, T.P.  Towards SMS Spam Filtering: Results under a New Dataset.   International Journal of Information Security Science (IJISS), 2(1), 1-18, 2013. [Invited paper - full version]</p>\n\n<h3>About</h3>\n\n<p>The SMS Spam Collection has been created by Tiago A. Almeida and José María Gómez Hidalgo.</p>\n\n<p>We would like to thank Min-Yen Kan and his team for making the NUS SMS Corpus available.</p>\n<hr/>\n<pre>dbfs:/databricks-datasets/tpch</pre>\n<h2><a href=\"http://www.tpc.org/tpch/\">TPC-H Data</a></h2>\n\n<p>The data in this directory was generated to run the TPC-H benchmark.</p>\n\n<p>The TPC Benchmark™H (TPC-H) is a decision support benchmark. It consists of a suite of business oriented\nad-hoc queries and concurrent data modifications. The queries and the data populating the database have\nbeen chosen to have broad industry-wide relevance. This benchmark illustrates decision support systems \nthat examine large volumes of data, execute queries with a high degree of complexity, and give answers\nto critical business questions.</p>\n\n<p>For more information, refer to the Transaction Processing Performance Council&#39;s\n<a href=\"http://www.tpc.org/tpch/\">TPC-H page</a></p>\n\n<h3>Versions</h3>\n\n<ul>\n<li>data-001 is a ~10GB TPC-H dataset and was generated by Parviz Deyhim <a href=\"mailto:parviz@databricks.com\">parviz@databricks.com</a></li>\n</ul>\n<hr/>\n<pre>dbfs:/databricks-datasets/wiki</pre>\n<p><em>Error processing readme</em></p>","arguments":{},"plotOptions":null},"errorSummary":"Command skipped","error":null,"startTime":1.447882482776E12,"submitTime":1.447882476602E12,"finishTime":1.447882482854E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"rlgarris@databricks.com","iPythonMetadata":null,"nuid":"f20cb3de-a634-471d-a028-6ad8d59a4766"},{"version":"CommandV1","origId":760,"guid":"42719b8c-888e-4a7b-a2aa-3f4d892cd9ad","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n## FAQs\n---\n\n### What is the best way to get started?\n\nFor users of Databricks Cloud, there is a DBFS mount available to use out of the box. For users outside of Databricks cloud, you can directly mount the S3 buckets.\n\n### What is the data useful for?\n\nWhether building examples, delivering training courses, or training models on public data, every data pipeline starts with data. By making these datasets widely available, it will be easier to share notebooks and collaborate.\n\n### Where can the data be obtained?\n\nThe data is available in S3 buckets in many AWS regions, and also conveniently mounted into the Databricks File System for accessing within Databricks Cloud.\n\n\n * s3://databricks-datasets-california\n * s3://databricks-datasets-frankfurt\n * s3://databricks-datasets-ireland\n * s3://databricks-datasets-oregon\n * s3://databricks-datasets-saopaulo\n * s3://databricks-datasets-singapore\n * s3://databricks-datasets-sydney\n * s3://databricks-datasets-tokyo\n * s3://databricks-datasets-virginia\n\n\n### How is the data transfer billed?\n\nUsers of Databricks Cloud have free access to these datasets through their Databricks file system mount. For direct S3 access, the requester pays the transfer cost, but note that buckets are available in many different regions and reading from a local bucket should minimize costs.\n\n### What license is the data released under?\n\nSeveral of these datasets are copied from other sources with their own licenses, which are available for viewing within  the dataset buckets themselves. In cases where the license is not already listed, the license is creative commons attribution 3.0 license.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":1.447882476653E12,"submitTime":1.447882476653E12,"finishTime":1.44788247671E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":null,"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"rlgarris@databricks.com","iPythonMetadata":null,"nuid":"91ce2188-b489-48be-b8c3-511fbaa668f5"}],"guid":"03aedd2b-330c-465f-9d20-461e3d16a221","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"e4ad3857-cf28-4e2e-a759-81da6b6be906","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>