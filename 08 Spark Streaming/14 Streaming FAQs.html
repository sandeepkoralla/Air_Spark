<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Spark Streaming / Streaming FAQs - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":230,"name":"Spark Streaming / Streaming FAQs","language":"scala","commands":[{"version":"CommandV1","origId":231,"guid":"397b8ab6-78fa-4a0f-9ee8-95d8e3b467e7","subtype":"command","commandType":"auto","position":1.0,"command":"%md ### Streaming FAQs & Best Practices\n\nFor a good understanding on using Spark Streaming, see here: http://spark.apache.org/docs/latest/streaming-programming-guide.html\n\nFor help on debugging a Spark Streaming application in Databricks, see [this notebook](../08 Spark Streaming/02 Debugging Spark Streaming Application.html)","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"813f48af-6cdb-4568-80da-d9e40e2826f1"},{"version":"CommandV1","origId":232,"guid":"d0857e81-4517-40b7-a83b-4303d10a2afc","subtype":"command","commandType":"auto","position":1.5,"command":"%md ** Can I have more than one Streaming job in a cluster? **\n\nNo. There can be only one streaming context in a cluster which implies only one streaming job.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"fb327693-379d-4595-a644-ec3db3a8f635"},{"version":"CommandV1","origId":233,"guid":"0bda9301-fafd-4bc2-be9f-013584c38953","subtype":"command","commandType":"auto","position":1.75,"command":"%md ** Can I have more than one stream in a job? **\n\nYes. You can create multiple streams and join or union them too. For example:\n\n    val stream1 = ssc.textFileStream(\"dbfs://path1\")\n    val stream2 = ssc.textFileStream(\"dbfs://path2\")\n    val unionedStream = stream1.union(stream)\n    unionStream.foreachRDD()","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.446676768342E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"b9d8eb35-a4bc-45ce-aa25-0698d64cb1ed"},{"version":"CommandV1","origId":234,"guid":"10fdf555-1471-42f4-b1dd-433c57fd2ff5","subtype":"command","commandType":"auto","position":1.8125,"command":"%md #### Can I run batch workloads along with a streaming job?\n\nYes, you can. But it is highly advised against. A streaming job has lot of micro-batches being submitted to the scheduler. A batch workload can use up the resources delaying the execution of some micro-batches affecting the overall performance of the streaming job.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"865b0968-3c3d-40ff-8299-938de766705f"},{"version":"CommandV1","origId":235,"guid":"1e43900f-f17a-4403-98cd-2d485ed5c320","subtype":"command","commandType":"auto","position":1.875,"command":"%md ** How can I verify if my streaming job is currently running and if I'm receiving data? **\n\nWhen you do `ssc.start()` in a notebook, it starts the streaming job in the background. You can go to the Spark UI of the streaming cluster. If a streaming job is running, you will see a 'Streaming' tab. That page has all the information about the running streaming job - how many events are being received, what is the processing time, etc. If there is no 'Streaming' tab, then your job was not started successfully. You can check the driver logs for exception. See the [Debugging Spark Streaming Application notebook](../08 Spark Streaming/02 Debugging Spark Streaming Application.html) for more information.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"cbeb6d51-9656-4b78-b9f9-02a32c63960c"},{"version":"CommandV1","origId":236,"guid":"b8df0079-c9a8-497d-bc3e-9d793a38553b","subtype":"command","commandType":"auto","position":1.9375,"command":"%md ** If I detach my notebook from the cluster, what happens to the streaming job? **\n\nThe Streaming job runs in the background thread of the cluster. Just detaching the notebook from the cluster will not stop the streaming job. The streamig job **may** keep running in the background. If a notebook is detached from the cluster, the REPL is cleared and along with it all the temporary classes and anon functions are gone; so there is a high chance that the streaming job may fail too if it refers to variables declared in the REPL.\n\nSo the preferred approach is to **not** detach the notebook from the cluster. If you have accidentally detached, then check the cluster's Spark UI page to check if the streaming tab is present. If the tab is present, then the streaming job is still running. If you want to stop the job, then you can do `StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }` to stop the streaming context.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"bf1cc84f-5fb8-47d9-853e-eb873d548c97"},{"version":"CommandV1","origId":237,"guid":"bc0a22f0-474e-40e1-a98b-2b936f7ffca5","subtype":"command","commandType":"auto","position":1.96875,"command":"%md ** How can I get an alert if my streaming job failed or stopped? **\n\nOnce you have a notebook that has all the streaming logic, you can run this notebook through the jobs feature. The jobs feature allows you to do two things:\n\n* Send alerts if the job fails.\n* Automatically restart the job.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"a8c22822-1d44-4c1f-8618-695fc4912429"},{"version":"CommandV1","origId":238,"guid":"1008843f-2131-4d97-887e-06daaa84f59c","subtype":"command","commandType":"auto","position":1.984375,"command":"%md ** What is the difference between batchInterval and blockInterval? **\n\nSpark Streaming batches all your input events and then processes it. batchInterval defines this batch duration. For example, providing a batchInterval of 1 second implies that spark streaming will batch events every 1 second and then process them. \n\nDuring that 1 second batching, spark streaming internally creates blocks/partitions. blockInterval defines the interval for which a block is created. The default value of blockInterval is 200ms. So, for every 200ms, spark streaming creates 1 block. If a batchInterval of 1 second is provided, then for every batch, you have an RDD with 5 blocks/partitions. This parameter is not configurable in Databricks. If there is a necessity to change this parameter, open a ticket with Databricks.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"5ec0d497-270a-479c-8532-e3faadb4a2f9"},{"version":"CommandV1","origId":239,"guid":"e52e1adc-e8a0-4715-be2a-e25d5ffb3fdd","subtype":"command","commandType":"auto","position":1.984619140625,"command":"%md ** Where does the receiver(s) run? **\n\nEach input stream corresponds to a receiver. The receivers run on the Spark executors in the worker nodes. Each receiver occupies one core in the worker. So if you are running a streaming job on a 1-node cluster (4 cores) with 1 receiver, then you have 3 cores available for processing and 1 core is used for receiving the data.\n\nIf you have multiple receivers, Spark Streaming allocates 1 receiver per executor to ensure that the received data is distributed in the cluster. For this reason, as a general rule of thumb for achieving good throughput, have as many executors as receivers you plan to have.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"c07b0c90-caa1-42fc-aedf-b4be2d37681d"},{"version":"CommandV1","origId":240,"guid":"133a8bd6-1b61-46ad-836b-4d7627092048","subtype":"command","commandType":"auto","position":1.986328125,"command":"%md ** I have a high volume input stream. How can I ensure that I can ingest the data into Spark Streaming at a desired rate? **\n\nThe best possible way to increase parallelism while receiving data would be to have multiple receivers creating multiple input streams. You can then union the streams together and process them. Union is a cheap transformation and so it is efficient to just union multiple input streams. See here for more details: http://spark.apache.org/docs/latest/streaming-programming-guide.html#level-of-parallelism-in-data-receiving\n","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"8bbcee80-8c96-4e64-9e4a-31fbd80b30e7"},{"version":"CommandV1","origId":241,"guid":"8d7e89ad-5990-468a-ac73-10c046f0c6e0","subtype":"command","commandType":"auto","position":1.998046875,"command":"%md ** How can I check the contents of a DStream? **\n\nThe easiest and quickest way is to do a print on the dstream and then look at the logs. In scala, you can do `dstream.print()` and its python equivalent would be `dstream.pprint()`. Another option would be to convert your dstream to a dataframe and then register it as a temporary table and then you can keep querying it. See the DB guides for examples on this.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"db7fa05d-2fc7-4146-aec6-abd4cd24d366"},{"version":"CommandV1","origId":242,"guid":"9f3d20e8-5f79-43ab-a9c4-11a7b810b2e1","subtype":"command","commandType":"auto","position":1.9990234375,"command":"%md ** What is the difference between `StreamingContext.checkpoint()` and `dstream.checkpoint()` ? **\n\n`StreamingContext.checkpoint(directory)` will checkpoint all the metadata about the streaming job. For example, information of the queued batches, etc.\n\n`dstream.checkpoint(interval)` will checkpoint the actual data in dstream. This is an expensive operation as data in the dstream is being written out to disk.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"f6698307-dc30-47c3-91b1-ed39c79d2933"},{"version":"CommandV1","origId":243,"guid":"9828823a-9b23-462a-897d-b4de669ac867","subtype":"command","commandType":"auto","position":1.999267578125,"command":"%md ** When should I use `StreamingContext.checkpoint()` ? **\n\nIt is useful if you want recovery from failures.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"e9fa8419-2307-4c89-a677-68b4003e977f"},{"version":"CommandV1","origId":244,"guid":"5446f90b-5224-44b9-9ccc-62bbc6004f41","subtype":"command","commandType":"auto","position":1.99951171875,"command":"%md ** When should I use the `dstream.checkpoint()` and what is the optimum checkpoint interval? **\n\nSpark Streaming automatically checkpoints dstream where required. So there is no need to explicitly checkpoint the dstreams. You can override the default checkpoint interval by calling `dstream.checkpoint(interval)`. A good rule of thumb would be to set the interval as 5-10x the sliding interval and adjust it from there based on the performance.\n\nIf you are curious on when dstream checkpointing is required, read on:\n\nDStream checkpointing is required if you are using stateful transformations in your streaming job:\n* `updateStateByKey()` \n* `reduceByKeyAndWindow()` with an inverse function.\n\nIn the above cases, the lineage for the spark transformation will keep growing with every batch as spark streaming needs to process the previous batch data as well for every current batch. You must checkpoint the dstream so that spark can internally use the checkpoint rather than traversing all the way back to very old batches. (See the streaming DB guide examples on [Window Aggregations](../08 Spark Streaming/09 Window Aggregations.html) and [Global Aggregations](../08 Spark Streaming/10 Global Aggregations.html).)\n\nFrequent checkpointing can reduce the throughput of the streaming pipeline because lot of rdds need to be persisted. Infrequent checkpointing can cause the lineage to grow for every batch resulting in poor performance.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"9b8a038e-a3f7-4719-b47f-cff6af4f41f4"},{"version":"CommandV1","origId":245,"guid":"e6f19464-baab-446a-9086-2cbf7a4d8376","subtype":"command","commandType":"auto","position":1.999755859375,"command":"%md ** In pyspark streaming, I'm not able to see the output in the logs. Where can I find the logs? **\n\nPyspark logs are available only when a cell in a notebook is active. Since the streaming job runs in the background thread, the logs are not available by default. This is an unfortunate limitation in our current setup. For now, the workaround to see the log output would be to do `ssc.awaitTerminationOrTimeout(x)` in a cell. This makes a cell in the notebook active for 'x' seconds. After 'x' seconds, you will see the output of the streaming job prints and exceptions during that 'x' seconds. ","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"c478191d-5ffb-4071-bfe4-ec68ccec7aa7"},{"version":"CommandV1","origId":246,"guid":"3caa74ce-03ad-4d72-ae3b-69831272276e","subtype":"command","commandType":"auto","position":1.9998779296875,"command":"%md ** If S3 is my source, the Spark UI does not show the # of input events. How can I verify if my new files are being picked up properly? **\n\nThe streaming UI does not show the # of events for S3. You can click on a completed batch and you can see the list of the files processed in that batch. If no file information is present, then there was no file processed during that batch.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"c5883355-4593-45a9-8a5c-90c41fddd045"},{"version":"CommandV1","origId":247,"guid":"eaf54922-d183-4123-9ba6-40d7b16b8c94","subtype":"command","commandType":"auto","position":1.99993896484375,"command":"%md ** When I convert my dstream to a dataframe and register as a temp table, I don't see anything in the temp table. Why would that be? **\n\nFirst thing to verify would be if you are receiving any input events in the streaming tab of the Spark UI. The next thing to verify would be to do a print on a dstream and check its contents on the logs. One common reason is that the streaming job converts the current batch dstream to a dataframe and then overwrites it in the temp table. Now, if your current batch did not have any data, then your table will be empty.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"f99436b0-92d5-4886-8a67-4a2085b94e31"}],"guid":"fcaed3f1-298d-4e2c-8352-b3cd0dc3a383","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"1137c537-c338-4330-ae63-b26457d514bc","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>