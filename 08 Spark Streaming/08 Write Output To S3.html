<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Spark Streaming / Write Output To S3 - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":195,"name":"Spark Streaming / Write Output To S3","language":"scala","commands":[{"version":"CommandV1","origId":196,"guid":"2dbf86d3-1e21-458d-b1eb-fc1032fb2937","subtype":"command","commandType":"auto","position":0.25,"command":"%md ## Streaming Data Ingestion to S3\n\nThis notebook provides examples on how to ingest data from a stream to S3 using Spark Streaming. The example does 2 things:\n\n* For every 10 seconds, it does a simple action. In this case, prints the # of events to the logs.\n* For every 60 seconds, it batches the last 60 seconds data and writes them into S3 in Parquet format.\n\n** Things to Remember **\n\n* To write data to S3, even for writing small data, the IO latency will be around 2 seconds. Based on the amount of data you are writing, you might want to have a batch interval > 2 seconds.\n* Ensure that the final DStream has the right # of partitions to control the # of files being written. Too many small files will not only affect the write performance of the streaming job but also the read performance of the subsequent jobs that will load the data being written.\n* **_The dataframe's write performance (in append mode) is proportional to the # of files already present in the output directory. This means that your write latency will keep increasing with time._** Along with this streaming ingestion notebook, you need to have some hourly ETL job that will read the files in the output directory to a more permanent location or insert into a Spark SQL table and also compacting the # of files along the way.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"93cb86bb-0a46-4fb2-8e41-e4d283412e6f"},{"version":"CommandV1","origId":197,"guid":"69213adb-969d-41dd-a805-aad43b2e4172","subtype":"command","commandType":"auto","position":0.5,"command":"%md #### Configuration","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"4d7796a2-89ec-4721-ab0c-f4c5034469bd"},{"version":"CommandV1","origId":198,"guid":"18fa3d3d-1834-4eb0-92b7-8577520d0a68","subtype":"command","commandType":"auto","position":1.0,"command":"val stopActiveContext = true\t \n// \"true\"  = stop if any existing StreamingContext is running;              \n// \"false\" = dont stop, and let it run undisturbed, but your latest code may not be used\n\nval eventsPerSecond = 1000    // For the dummy source\n\n// Verify that the attached Spark cluster is 1.4.0+\nrequire(sc.version.replace(\".\", \"\").toInt >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">stopActiveContext: Boolean = true\neventsPerSecond: Int = 1000\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.447219895393E12,"submitTime":1.447219894675E12,"finishTime":1.447219901465E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"66a9f95e-682b-43db-8531-9f5b3f74e4d1"},{"version":"CommandV1","origId":199,"guid":"4536ad0b-1838-47e5-8e9f-64809a884070","subtype":"command","commandType":"auto","position":1.75,"command":"val batchIntervalSeconds = 10\nval outputDirectory = \"OUTPUT_PATH_HERE\"\n\n// Consolidate data for the last 60 seconds and write them out in one shot. You could have chosen a batch interval of 60 seconds too. If your streaming job needs to do some other processing at a lower granularity of time, then you can have a smaller batch interval and then use the window and sliding interval to write only every N seconds to ensure that there is less write overhead and also only less # of files are written.\nval writeEveryNSeconds = 60\n\n// # of files that needs to be written out for every batch (writeEveryNSeconds).\nval numberOfFilesPerBatch = 2","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">batchIntervalSeconds: Int = 10\noutputDirectory: String = /mnt/prakash/DataIngestTest\nwriteEveryNSeconds: Int = 60\nnumberOfFilesPerBatch: Int = 2\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.447219921608E12,"submitTime":1.447219921087E12,"finishTime":1.447219921747E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"eaafd880-1551-4cbd-b50a-a8b0ecbe95fa"},{"version":"CommandV1","origId":200,"guid":"944bd9b4-7b32-4ee7-8081-63e53b1c2e05","subtype":"command","commandType":"auto","position":2.0,"command":"import org.apache.spark._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming._","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming._\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.447219930845E12,"submitTime":1.447219930432E12,"finishTime":1.447219930951E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"bf52bcb0-4b9c-40a1-a060-c3722bd76d03"},{"version":"CommandV1","origId":201,"guid":"7f977b5e-c035-43de-a734-94014106c0f0","subtype":"command","commandType":"auto","position":2.5,"command":"%md #### Custom Reciever\n\nThis in-memory receiver produces events each of which has 100 fields and are comma separated. Each field is a random string.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"8ea5be6a-22f9-4ef4-8990-b25b8826bfc8"},{"version":"CommandV1","origId":202,"guid":"11800846-cc82-447a-89b6-b08f9c5f650d","subtype":"command","commandType":"auto","position":3.0,"command":"import scala.util.Random\nimport org.apache.spark.streaming.receiver._\n\nclass CustomReceiver(ratePerSec: Int) extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) {\n\n  def generateData(): String = {\n    (1 to 100).map(i => \"Random\" + i + Random.nextInt(100000)).mkString(\",\")\n  }\n  \n  def onStart() {\n    // Start the thread that receives data over a connection\n    new Thread(\"Dummy Source\") {\n      override def run() { receive() }\n    }.start()\n  }\n\n  def onStop() {\n   // There is nothing much to do as the thread calling receive()\n   // is designed to stop by itself isStopped() returns false\n  }\n\n  /** Create a socket connection and receive data until receiver is stopped */\n  private def receive() {\n    while(!isStopped()) {      \n      store(generateData())\n      Thread.sleep((1000.toDouble / ratePerSec).toInt)\n    }\n  }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import scala.util.Random\nimport org.apache.spark.streaming.receiver._\ndefined class CustomReceiver\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.447219934722E12,"submitTime":1.447219934236E12,"finishTime":1.447219935874E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"02dba607-9a85-442b-bdc9-94ab5edfb4b1"},{"version":"CommandV1","origId":203,"guid":"85897b61-4432-4b96-9929-9bedc1e86c74","subtype":"command","commandType":"auto","position":3.5,"command":"%md #### Core Streaming Logic\n\nThe streaming logic to print the count and dump the incoming data to S3 in parquet format.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"32ac824a-8c57-4978-9395-ba6abe393044"},{"version":"CommandV1","origId":204,"guid":"d98ad4bc-5a39-49f7-910f-5b8c8b6ee5a3","subtype":"command","commandType":"auto","position":4.0,"command":"import org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport java.text.SimpleDateFormat\nimport java.util.Date\n\nvar newContextCreated = false      // Flag to detect whether new context was created or not\n\n// Function to create a new StreamingContext and set it up\ndef creatingFunc(): StreamingContext = {\n    \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n  \n  // Create a stream that generates 1000 lines per second\n  val stream = ssc.receiverStream(new CustomReceiver(eventsPerSecond))  \n  \n  val schemaString = (1 to 100).map(i => \"C\" + i).mkString(\",\")\n  val schema = StructType(\n                  schemaString.split(\",\").map(fieldName => StructField(fieldName, StringType, true))\n                )\n  \n  // Split the event into fields and print the # of events\n  val rowStream = stream.map(event => Row.fromSeq(event.split(\",\")))\n  rowStream.foreachRDD { rdd => println(rdd.count) }\n\n  val sdf = new SimpleDateFormat(\"yyyyMMddhh\")\n  \n  // Write to S3 in Parquet format for every N seconds.\n  rowStream.window(Seconds(writeEveryNSeconds), Seconds(writeEveryNSeconds)).foreachRDD { (rdd, time) => \n    val df = sqlContext.createDataFrame(rdd.coalesce(numberOfFilesPerBatch), schema)\n    val datehour = sdf.format(new Date(time.milliseconds))    // UTC time as shown in Spark Streaming UI.\n    val finalDf = df.withColumn(\"datehr\", lit(datehour))\n    finalDf.write.mode(SaveMode.Append).partitionBy(\"datehr\").save(outputDirectory)\n  }\n\n  ssc.remember(Minutes(1))\n  \n  println(\"Creating function called to create new StreamingContext\")\n  newContextCreated = true  \n  ssc\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport java.text.SimpleDateFormat\nimport java.util.Date\nnewContextCreated: Boolean = false\ncreatingFunc: ()org.apache.spark.streaming.StreamingContext\n</div>","arguments":{},"plotOptions":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:83: error: not found: value writeAsSparkSQLTable\n           if( writeAsSparkSQLTable )\n               ^\n</div>","error":null,"startTime":1.447220073635E12,"submitTime":1.44722007318E12,"finishTime":1.447220074674E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"a7902887-8939-4225-86d2-670194c33c4d"},{"version":"CommandV1","origId":205,"guid":"2f8c1508-4d95-4a14-ac97-c5978306fa0b","subtype":"command","commandType":"auto","position":4.5,"command":"%md #### Start the streaming job","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"f9a8b404-c069-4faa-b124-4260a2550a73"},{"version":"CommandV1","origId":206,"guid":"be8e0227-60cc-4f44-bf4c-092e7823d724","subtype":"command","commandType":"auto","position":5.0,"command":"// Stop any existing StreamingContext \nif (stopActiveContext) {\t\n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n} \n\n// Get or create a streaming context\nval ssc = StreamingContext.getActiveOrCreate(creatingFunc)\nif (newContextCreated) {\n  println(\"New context created from currently defined creating function\") \n} else {\n  println(\"Existing context running or recovered from checkpoint, may not be running currently defined creating function\")\n}\n\n// Start the streaming context in the background.\nssc.start()\n\n// This is to ensure that we wait for some time before the background streaming job starts. This will put this cell on hold for 2 times the batchIntervalSeconds.\nssc.awaitTerminationOrTimeout(batchIntervalSeconds * 2 * 1000)\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Creating function called to create new StreamingContext\nNew context created from currently defined creating function\n6501\n8723\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@1f0d3897\nres5: Boolean = false\n</div>","arguments":{},"plotOptions":null},"errorSummary":"java.lang.Exception: The window duration of windowed DStream (60 ms) must be a multiple of the slide duration of parent DStream (10000 ms)","error":"<div class=\"ansiout\">\tat org.apache.spark.streaming.dstream.WindowedDStream.&lt;init&gt;(WindowedDStream.scala:35)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$window$1.apply(DStream.scala:754)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$window$1.apply(DStream.scala:754)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:712)\n\tat org.apache.spark.streaming.StreamingContext.withScope(StreamingContext.scala:266)\n\tat org.apache.spark.streaming.dstream.DStream.window(DStream.scala:753)\n\tat Notebook.creatingFunc(&lt;console&gt;:134)\n\tat Notebook$$anonfun$2.apply(&lt;console&gt;:130)\n\tat Notebook$$anonfun$2.apply(&lt;console&gt;:130)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.streaming.StreamingContext$.getActiveOrCreate(StreamingContext.scala:800)</div>","startTime":1.447220262156E12,"submitTime":1.447220261713E12,"finishTime":1.447220282527E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"b8ffc1db-1190-45e3-b4f1-edf0b1ea051a"},{"version":"CommandV1","origId":207,"guid":"c9c5af53-1d6f-46ad-b261-dc1b48992303","subtype":"command","commandType":"auto","position":5.03125,"command":"%md #### Take a peek at the output directory to ensure that the data files are being written.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"ef9fc75f-9dca-495a-8c22-a156f4cb6a38"},{"version":"CommandV1","origId":208,"guid":"993bc40c-8f77-4ebf-883b-9ab06a60ad6e","subtype":"command","commandType":"auto","position":5.0625,"command":"display(dbutils.fs.ls(outputDirectory))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/mnt/prakash/DataIngestTest/_SUCCESS","_SUCCESS",0.0],["dbfs:/mnt/prakash/DataIngestTest/_common_metadata","_common_metadata",7454.0],["dbfs:/mnt/prakash/DataIngestTest/_metadata","_metadata",37724.0],["dbfs:/mnt/prakash/DataIngestTest/datehr=2015111105/","datehr=2015111105/",0.0]],"arguments":{},"schema":[{"type":"string","name":"path"},{"type":"string","name":"name"},{"type":"bigint","name":"size"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":"java.io.FileNotFoundException: /mnt/prakash/DataIngestTest","error":"<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:63)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:40)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:179)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:58)\n\tat com.databricks.dbutils_v1.package$fs$.ls(dbutils_v1.scala:40)</div>","startTime":1.447220371784E12,"submitTime":1.447220371307E12,"finishTime":1.447220372399E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"1f8afba8-4385-4244-97e3-936539b7d2e5"},{"version":"CommandV1","origId":209,"guid":"e074edde-fae4-4756-93fa-d3d4a5060fb2","subtype":"command","commandType":"auto","position":5.25,"command":"%md #### Stop the streaming job\n\nUncomment and run the following command","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"556fd196-7f18-41af-aec3-14c0159a85d3"},{"version":"CommandV1","origId":210,"guid":"5fe8a89d-d2bc-422e-bba1-f4566f886d9e","subtype":"command","commandType":"auto","position":5.5,"command":"//StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:25: error: not found: value StreamingContext\n              StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n              ^\n</div>","error":null,"startTime":1.447220380288E12,"submitTime":1.447220379722E12,"finishTime":1.447220382766E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d04cacae-51f8-4eca-806b-708425c62e09"},{"version":"CommandV1","origId":211,"guid":"0720e178-bef8-44e4-80dc-ff0b1a7b3902","subtype":"command","commandType":"auto","position":6.25,"command":"%md #### Cleanup the directory","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"c061b6c2-7276-43b2-a199-af0eb4b6f920"},{"version":"CommandV1","origId":212,"guid":"ce7292e1-d5f6-4782-ae68-d877efe3d23b","subtype":"command","commandType":"auto","position":7.0,"command":"dbutils.fs.rm(outputDirectory, true)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res8: Boolean = true\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.447220391315E12,"submitTime":1.447220390896E12,"finishTime":1.447220391805E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"ed3d430c-172c-49b9-a827-f1eb078da919"}],"guid":"36a20566-7dd4-4200-93ff-9e3c5a8eda66","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"a67b8d1f-e982-4976-a55f-450f05ed5e97","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>