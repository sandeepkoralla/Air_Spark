<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Spark Streaming / Global Aggregations - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":445,"name":"Spark Streaming / Global Aggregations","language":"scala","commands":[{"version":"CommandV1","origId":446,"guid":"a048d21e-4bb3-436d-8a3d-0ed269ddfab4","subtype":"command","commandType":"auto","position":1.0,"command":"%md ### Global Aggregation in Streaming\n\nThe [window aggregations example](../08 Spark Streaming/09 Window Aggregations.html) using reduceByKeyAndWindow has 3 limitations:\n* The fold/aggregation function should be associative \n* The fold/aggregation is limited to the window\n* The type of the state is limited to the type of the input DStream values\n\nIf your use case needs to overcome any of the above limitations, then updateStateByKey transformation can be used. It maintains a global state for all the keys and provides the ability to define any custom state and a custom function to update the state.\n\nTo do a word count, an example update function would look like:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438057170705E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"a524c6cc-b422-44cd-a8d4-8aee773c24e7"},{"version":"CommandV1","origId":447,"guid":"bc98a4c3-ce1e-410d-a69d-64474f341183","subtype":"command","commandType":"auto","position":3.0,"command":"def updateFunc(values: Seq[Int], state: Option[Int]): Option[Int] = {\n    val currentCount = values.sum\n    val previousCount = state.getOrElse(0)\n    Some(currentCount + previousCount)  \n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">updateFunc: (values: Seq[Int], state: Option[Int])Option[Int]\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438057206068E12,"submitTime":1.438057205974E12,"finishTime":1.438057206178E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"863a4066-0775-46da-8b52-f81dd353c948"},{"version":"CommandV1","origId":448,"guid":"3cb6417e-17f0-4775-8e36-141c4f517772","subtype":"command","commandType":"auto","position":4.0,"command":"%md The function above takes in the new set of values, a previous state (in this case, the previous count) and returns the new state. Now you can use the updateStateByKey interface:\n\n`val globalCountStream = wordStream.updateStateByKey(updateFunc)`\n\nThe globalCountStream is a DStream that has all the cumulative word counts.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.4380590894E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"7f47167e-8c81-4f7a-bd6f-060de2d06483"},{"version":"CommandV1","origId":449,"guid":"1c34ec0b-8cba-4710-b1b7-5a89f6df0fa1","subtype":"command","commandType":"auto","position":5.0,"command":"%md #### Global Aggregation with Pruning\n\nOne limitation with the above updateStateByKey interface is that there is no way to remove keys from the state. In order to keep only relevant keys or control the key management in the state, you can use another overloaded method of updateStateByKey. \n\nThe rest of the notebook walks through an example of doing global cumulative word counts efficiently with the updateStateByKey interface.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438057554579E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"483d7f58-9044-470d-9e13-2f433535e580"},{"version":"CommandV1","origId":450,"guid":"76a8015e-9245-49de-b1e7-a5602c03102f","subtype":"command","commandType":"auto","position":5.5,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time}\nimport org.apache.spark.streaming.dstream._\nimport org.apache.spark.streaming.kafka._\nimport kafka.serializer.StringDecoder","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time}\nimport org.apache.spark.streaming.dstream._\nimport org.apache.spark.streaming.kafka._\nimport kafka.serializer.StringDecoder\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.43805772364E12,"submitTime":1.438057723599E12,"finishTime":1.438057723723E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"6ccdd840-9c2b-4d94-a1e6-1689e889f413"},{"version":"CommandV1","origId":451,"guid":"dcc8b8c9-61f8-4705-b4f3-7f620414ced4","subtype":"command","commandType":"auto","position":5.8125,"command":"%md An utility function to create kafka direct stream","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438058018829E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d667fcb2-0220-4c5e-afc9-01f706beac94"},{"version":"CommandV1","origId":452,"guid":"b407e65b-d642-489b-8880-4ee60391f5b4","subtype":"command","commandType":"auto","position":5.875,"command":"val kafkaTopics = \"YOUR_KAFKA_HOST_1,YOUR_KAFKA_HOST_2\"    // command separated list of topics\nval kafkaBrokers = \"YOUR_BROKER_1:HOST_1,YOUR_BROKER_2:HOST_2\"   // comma separated list of broker:host\n\ndef createKafkaStream(ssc: StreamingContext): DStream[(String, String)] = {\n  val topicsSet = kafkaTopics.split(\",\").toSet\n  val kafkaParams = Map[String, String](\"metadata.broker.list\" -> kafkaBrokers)\n  KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n                                   ssc, kafkaParams, topicsSet)\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">kafkaTopics: String = webapp,usage\nkafkaBrokers: String = 52.25.255.200:9092,52.25.255.200:9093\ncreateKafkaStream: (ssc: org.apache.spark.streaming.StreamingContext)org.apache.spark.streaming.dstream.DStream[(String, String)]\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438057770512E12,"submitTime":1.43805777043E12,"finishTime":1.438057770867E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"f7ba1f4f-26cf-40af-a89e-05cc50dde291"},{"version":"CommandV1","origId":453,"guid":"ce4f04cc-6303-48c3-8289-569999632403","subtype":"command","commandType":"auto","position":6.0,"command":"%md ** Update function **\n\nNow, lets define the update function. The function takes in the new values and the previous state (count) for each key as an iterator and returns the new state which is an iterator of keys and their counts.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438057826432E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"bc077e3e-5299-4454-a93d-9cdf87218b34"},{"version":"CommandV1","origId":454,"guid":"35924641-1058-486e-9f49-2e810bcd3fc2","subtype":"script","commandType":"auto","position":6.083333333333333,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.dstream._\n\nrequire(sc.version.replace(\".\", \"\").toInt >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.dstream._\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438057742051E12,"submitTime":1.437516036498E12,"finishTime":1.438057742439E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["98673ac6-603c-4a36-b5aa-eefc71079874"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d9ee8e1a-998b-4788-863e-b6ce505a01d6"},{"version":"CommandV1","origId":455,"guid":"fc9fcb7d-edfb-4c5a-b4d1-3140d824faf9","subtype":"script","commandType":"auto","position":6.416666666666667,"command":"def createStreamingContext(\n                    batchIntervalSeconds: Int = 1, \n                    checkpointDir: String = \"dbfs:/streaming/checkpoint/100\", \n                    rememberSeconds: Int = 60): StreamingContext = {\n  \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n  \n  // To make sure data is not deleted by the time we query it interactively\n  ssc.remember(Seconds(rememberSeconds))       \n  \n  // For saving checkpoint info so that it can recover from failed clusters\n  ssc.checkpoint(checkpointDir)  \n  \n  ssc\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">createStreamingContext: (batchIntervalSeconds: Int, checkpointDir: String, rememberSeconds: Int)org.apache.spark.streaming.StreamingContext\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438057742444E12,"submitTime":1.437502226626E12,"finishTime":1.438057742931E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["98673ac6-603c-4a36-b5aa-eefc71079874"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"f7c4cae0-e28c-46e4-a7b4-b43d53581013"},{"version":"CommandV1","origId":456,"guid":"fc607180-e253-469a-b47f-4b0499c8803f","subtype":"command","commandType":"auto","position":7.0,"command":"import scala.collection.mutable.ListBuffer\n\ndef updateStateForKey(values: Seq[Int], state: Option[Int]): Option[Int] = {\n    val currentCount = values.sum\n    val previousCount = state.getOrElse(0)\n    Some(currentCount + previousCount)  \n}\n\n// The custom update function that takes in an iterator of key, new values for the key, previous state for the key and returns a new iterator of key, value\ndef updateFunc(iter: Iterator[(String, Seq[Int], Option[Int])]): Iterator[(String, Int)] = {\n    val list = ListBuffer[(String, Int)]()\n    while(iter.hasNext) {\n      val record = iter.next\n      val state = updateStateForKey(record._2, record._3)\n      val value = state.getOrElse(0)\n      if( value != 0 ) {\n        list += ((record._1, value))              // Add only keys with positive counts\n      }\n    }\n    list.toIterator\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import scala.collection.mutable.ListBuffer\nupdateStateForKey: (values: Seq[Int], state: Option[Int])Option[Int]\nupdateFunc: (iter: Iterator[(String, Seq[Int], Option[Int])])Iterator[(String, Int)]\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":"<div class=\"ansiout\">&lt;console&gt;:31: error: not found: value ListBuffer\n           val list = ListBuffer[(String, Int)]()\n                      ^\n</div>","startTime":1.43805769055E12,"submitTime":1.438057690469E12,"finishTime":1.438057690936E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"7e19887b-3397-4c7a-8614-1b3aa3438da0"},{"version":"CommandV1","origId":457,"guid":"bb819e25-710e-434e-b852-fbf75c997e4a","subtype":"command","commandType":"auto","position":7.5,"command":"%md The core function that is passed to the streaming context. We create a kafka direct DStream and then call updateStateByKey with our previous defined update function.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438058097066E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d491ca4a-df86-4f72-bbe1-43f6dc065ee5"},{"version":"CommandV1","origId":458,"guid":"3e699ca9-2a34-4b99-9528-64b590b36126","subtype":"command","commandType":"auto","position":8.0,"command":"import org.apache.spark.HashPartitioner\n\nval windowSize = Duration(10000L)          // 10 seconds\nval slidingInterval = Duration(2000L)      // 2 seconds\n\nval checkpointDir = \"dbfs:/streaming/checkpoint/100\"\n\n// Function to create a new StreamingContext and set it up\ndef creatingFunc(): StreamingContext = {\n    \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n\n  // Get the word stream from the source\n  val wordStream = createKafkaStream(ssc).flatMap { event => event._2.split(\" \") }.map(x => (x, 1))\n  \n  val runningCountStream = wordStream.updateStateByKey[Int](updateFunc _, new HashPartitioner(10), false)\n  \n  // Create temp table at every batch interval\n  runningCountStream.foreachRDD { rdd => \n    val sqlContext = SQLContext.getOrCreate(SparkContext.getOrCreate())\n    sqlContext.createDataFrame(rdd).toDF(\"word\", \"count\").registerTempTable(\"batch_word_count\")\n    \n    /* Trigger an action so that the DAG gets executed. This triggering of action will ensure that \n       the checkpointing is invoked. Otherwise checkpointing will not be invoked and if somebody queries\n       the table after 'n' minutes, it will result in processing a big lineage of rdds.\n    */\n    rdd.take(1)\n  }\n  \n  // To make sure data is not deleted by the time we query it interactively\n  ssc.remember(Minutes(1))\n  \n  ssc.checkpoint(checkpointDir)\n  \n  println(\"Creating function called to create new StreamingContext\")\n  ssc\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark.HashPartitioner\nwindowSize: org.apache.spark.streaming.Duration = 10000 ms\nslidingInterval: org.apache.spark.streaming.Duration = 2000 ms\nregisterTempTable: (dstream: org.apache.spark.streaming.dstream.DStream[(String, Int)])Unit\ncreatingFunc: ()org.apache.spark.streaming.StreamingContext\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":"<div class=\"ansiout\">&lt;console&gt;:96: error: not found: value wordCountStream\n         registerTempTable(wordCountStream)\n                           ^\n</div>","startTime":1.438058208946E12,"submitTime":1.438058208784E12,"finishTime":1.438058209491E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"5f9de1d6-06b6-4cca-8e7b-a4043a21aef9"},{"version":"CommandV1","origId":459,"guid":"1297bec7-0318-43a6-86e0-9ad161e8b287","subtype":"command","commandType":"auto","position":8.5,"command":"%md #### Start the streaming job","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438063872559E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"a7a7011c-fddc-48e7-9a9a-0a3be48813cb"},{"version":"CommandV1","origId":460,"guid":"a72830ab-eb77-406c-a34b-63b967e1bd2f","subtype":"command","commandType":"auto","position":9.0,"command":"// Stop any existing StreamingContext \nval stopActiveContext = true\nif (stopActiveContext) {\t\n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n} \n\n// Get or create a streaming context.\nval ssc = StreamingContext.getActiveOrCreate(creatingFunc)\n\n// This starts the streaming context in the background. \nssc.start()\n\n// This is to ensure that we wait for some time before the background streaming job starts. This will put this cell on hold for 5 times the batchIntervalSeconds.\nssc.awaitTerminationOrTimeout(batchIntervalSeconds * 5 * 1000)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":1.438058222464E12,"submitTime":1.438058222464E12,"finishTime":1.438058224479E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"b948cdcd-9b50-48bb-880b-29609e7cb095"},{"version":"CommandV1","origId":461,"guid":"358bbca5-a4f1-4acd-9e3d-6865c1e17ee9","subtype":"script","commandType":"auto","position":9.25,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.dstream._\nimport kafka.serializer.StringDecoder\n\n// Verify that the attached Spark cluster is 1.4.0+\nrequire(sc.version.replace(\".\", \"\").toInt >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\")\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.dstream._\nimport kafka.serializer.StringDecoder\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438058222712E12,"submitTime":1.437508030773E12,"finishTime":1.438058222958E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["2b73ee20-4196-48e7-b7a7-e51e5982b157"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"ee66836f-5489-47d4-92fa-8fa547bb0269"},{"version":"CommandV1","origId":462,"guid":"71de72a5-890c-4ae0-86b0-166cf2babd86","subtype":"script","commandType":"auto","position":9.5,"command":"/*\n  The function that starts a spark stream. It takes the following params:\n    * creatingFunc - the core function that does all the processing and returns a streaming context. This context is used only based on other params\n    * checkpointDir - checkpoint directory where the driver's write-ahead-logs are stored\n    * batchIntervalSeconds - batch interval in seconds\n    * stopActiveContext - if a streaming context is already running, then should they be stopped.\n    * stopGracefully - while stopping existing active streaming contexts, should we wait for all the processing in those streaming contexts to be completed before stopping.\n    * restartIfCheckpointExists - if set to false, the checkpoint directory is cleared everytime to ensure that a fresh stream is started everytime. In prod, you might want to set it to true to recover from previous failures.\n    * waitForContextTermination - if set to true, the execution waits until the streaming context is stopped or there is an exception. So ensure this is set to false while running from notebook so that the notebook that calls the startStream() doesn't get hung forever. While running as a job, this flag needs to be set \n*/\n\ndef startStream(creatingFunc: () => StreamingContext,\n                checkpointDir: String = \"dbfs:/streaming/checkpoint/100\",\n                batchIntervalSeconds: Int = 1,\n                stopActiveContext: Boolean = true,\n                stopGracefully: Boolean = false,\n                restartIfCheckpointExists: Boolean = false,\n                waitForContextTermination: Boolean = false) = {\n  // Stop any existing StreamingContext \n  if (stopActiveContext) {\t\n    StreamingContext.getActive.foreach { _.stop(stopSparkContext = false, stopGracefully = stopGracefully) }\n  } \n\n  // Delete checkpoint information to ensure context is not restarted\n  if (!restartIfCheckpointExists) {\n    dbutils.fs.rm(checkpointDir, true)\n  }  \n\n  // Get either the active context, or recover the context from checkpoint or create new one\n  val ssc = StreamingContext.getActiveOrCreate(checkpointDir, creatingFunc)\n  ssc.start()\n\n  // ssc.awaitTermination will terminate only if the context stopped from another notebook, or the context encounters an exception.\n  if (waitForContextTermination) {\n    ssc.awaitTermination()  \n  } else {\n    ssc.awaitTerminationOrTimeout(batchIntervalSeconds * 5 * 1000)\n  }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">startStream: (creatingFunc: () =&gt; org.apache.spark.streaming.StreamingContext, checkpointDir: String, batchIntervalSeconds: Int, stopActiveContext: Boolean, stopGracefully: Boolean, restartIfCheckpointExists: Boolean, waitForContextTermination: Boolean)AnyVal\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438058222962E12,"submitTime":1.437508030797E12,"finishTime":1.438058223628E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["2b73ee20-4196-48e7-b7a7-e51e5982b157"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"70a90126-9297-4bbc-a384-1ba5c6bf7c04"},{"version":"CommandV1","origId":463,"guid":"a426e5f3-2b85-4be2-8482-d93de814813d","subtype":"script","commandType":"auto","position":9.75,"command":"def stopAllStreams() = {\n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">stopAllStreams: ()Unit\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438058223632E12,"submitTime":1.437508030814E12,"finishTime":1.438058223919E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["2b73ee20-4196-48e7-b7a7-e51e5982b157"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"dd06f4a7-da73-467d-b71a-561ec42934c4"},{"version":"CommandV1","origId":464,"guid":"6c7c2fcf-7fcf-4abf-bac0-20345f6a9159","subtype":"command","commandType":"auto","position":11.0,"command":"%md #### Interactive Querying\n\nNow let's try querying the table. You can run this command again and again, you will find the numbers changing while data is being sent.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438058319149E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"788eff6e-2401-45b8-bdbc-ecf3e56cbe14"},{"version":"CommandV1","origId":465,"guid":"684cee4b-83f5-4984-b2d8-e4a9cf1dbd82","subtype":"command","commandType":"auto","position":12.0,"command":"%sql select * from batch_word_count","commandVersion":0,"state":"finished","results":{"type":"table","data":[["my",52382.0],["are",51774.0],["spark",52212.0],["father",52573.0],["you",52059.0]],"arguments":{},"schema":[{"type":"string","name":"word"},{"type":"int","name":"count"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":null,"startTime":1.438058717557E12,"submitTime":1.438058717518E12,"finishTime":1.438058717752E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"da7fff42-eb11-4e91-8ea7-f65e7db39d4d"},{"version":"CommandV1","origId":466,"guid":"5d3fad39-aebd-4de4-aade-663f8d5e8755","subtype":"command","commandType":"auto","position":13.0,"command":"%md #### Finally, if you want stop the StreamingContext, you can uncomment and execute the following","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438058356205E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"46c86097-926e-4782-a34d-38536c493e3f"},{"version":"CommandV1","origId":467,"guid":"bdcb86cc-c7a1-40ed-9039-4c1682b8dbcd","subtype":"command","commandType":"auto","position":14.0,"command":"//StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"57b2eae5-6cf7-4d03-9563-f2eb18b0a3dc"}],"guid":"ced7c292-3f15-43e8-9b16-c11a4270e3c5","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"f22ce13d-7d88-41d3-97fa-3936318da981","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>