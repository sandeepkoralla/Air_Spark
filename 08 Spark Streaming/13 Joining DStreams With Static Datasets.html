<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Spark Streaming / Joining DStreams With Static Datasets - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":384,"name":"Spark Streaming / Joining DStreams With Static Datasets","language":"scala","commands":[{"version":"CommandV1","origId":385,"guid":"dc4150ff-bf1f-4651-87d8-1a2fa1259661","subtype":"command","commandType":"auto","position":1.0,"command":"%md ### Joining DStreams with Static Datasets\n\nThis notebook provides an example to show how to join DStreams with dataframes or RDDs that does not change during the streaming application's run. The notebook builds on the previous example of [joining DStreams](../08 Spark Streaming/11 Joining DStreams.html).\n\nThis notebook receives 2 input streams - ad clicks and ad impressions - from a Kafka cluster based on the data produced by [Kafka Ads Data Producer](../08 Spark Streaming/Streaming Producers/3 Kafka Ads Data Producer.html). The streaming job then counts the ad clicks and impressions over a window and joins these 2 streams to compute the Click-Through Rate (CTR) of all the ads for every window. The streams are then joined with a dataset to tag some advertiser information. Finally, it stores the last 1 minute of CTR in a temporary table in memory that can be used to setup a dashboard which refreshes periodically.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438114188002E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"a817d60a-5d41-4250-9b4e-05e6c54238ee"},{"version":"CommandV1","origId":386,"guid":"b08fba17-f9fe-4326-86a8-a0ec38bfc967","subtype":"command","commandType":"auto","position":1.5,"command":"%md #### Import some utility functions required","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112110425E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"3f015fe7-41f5-407d-b700-b5bb686bd3b2"},{"version":"CommandV1","origId":387,"guid":"a8fa0f52-9b7a-4a54-89ac-8fec2d4964bb","subtype":"command","commandType":"auto","position":2.0,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.dstream._\nimport kafka.serializer.StringDecoder","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.dstream._\nimport kafka.serializer.StringDecoder\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":"java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext\n\tat org.apache.spark.SparkContext.org$apache$spark$SparkContext$$assertNotStopped(SparkContext.scala:103)\n\tat org.apache.spark.SparkContext.getSchedulingMode(SparkContext.scala:1501)\n\tat org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:2005)\n\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1607)\n\tat org.apache.spark.sql.hive.execution.AddJar.run(commands.scala:106)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)\n\tat org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:955)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:955)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:144)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:128)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:755)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$1.apply(DriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$1.apply(DriverLocal.scala:129)\n\tat scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:153)\n\tat scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:306)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:129)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:481)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:383)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:194)\n\tat java.lang.Thread.run(Thread.java:745)\n","startTime":1.438109021851E12,"submitTime":1.438109021732E12,"finishTime":1.43810902224E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"9e3a13dd-2322-4e01-972c-865018e1886a"},{"version":"CommandV1","origId":388,"guid":"b39cd623-820a-4db5-aaa0-9d3600188e91","subtype":"script","commandType":"auto","position":3.5833333333333335,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.dstream._\n\nrequire(sc.version.replace(\".\", \"\").toInt >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.dstream._\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438109024515E12,"submitTime":1.437516036498E12,"finishTime":1.438109025528E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["58949047-39c7-4833-b926-fcfe8cde6500"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d9ee8e1a-998b-4788-863e-b6ce505a01d6"},{"version":"CommandV1","origId":389,"guid":"5d9a2e1c-1b1b-429b-9c97-b913527ab44b","subtype":"script","commandType":"auto","position":3.9166666666666665,"command":"def createStreamingContext(\n                    batchIntervalSeconds: Int = 1, \n                    checkpointDir: String = \"dbfs:/streaming/checkpoint/100\", \n                    rememberSeconds: Int = 60): StreamingContext = {\n  \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n  \n  // To make sure data is not deleted by the time we query it interactively\n  ssc.remember(Seconds(rememberSeconds))       \n  \n  // For saving checkpoint info so that it can recover from failed clusters\n  ssc.checkpoint(checkpointDir)  \n  \n  ssc\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">createStreamingContext: (batchIntervalSeconds: Int, checkpointDir: String, rememberSeconds: Int)org.apache.spark.streaming.StreamingContext\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438109025532E12,"submitTime":1.437502226626E12,"finishTime":1.438109026655E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["58949047-39c7-4833-b926-fcfe8cde6500"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"f7c4cae0-e28c-46e4-a7b4-b43d53581013"},{"version":"CommandV1","origId":390,"guid":"dacb64b0-1e98-487b-978f-54b399e065fa","subtype":"command","commandType":"auto","position":4.125,"command":"def createKafkaStream(ssc: StreamingContext, kafkaTopics: String, brokers: String): DStream[(String, String)] = {\n  val topicsSet = kafkaTopics.split(\",\").toSet\n  val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n  KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n                                   ssc, kafkaParams, topicsSet)\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">createKafkaStream: (ssc: org.apache.spark.streaming.StreamingContext, kafkaTopics: String, brokers: String)org.apache.spark.streaming.dstream.DStream[(String, String)]\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.43810902925E12,"submitTime":1.438109029136E12,"finishTime":1.438109029858E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"bd63c0a0-2646-4fcc-af2b-45c2ac777f62"},{"version":"CommandV1","origId":391,"guid":"fe462449-943f-41a6-bb27-e15b9968b396","subtype":"command","commandType":"auto","position":4.34375,"command":"%md #### Load the advertiser information table and cache it so that it can be joined with streams","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112142915E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"24b418d5-14f0-41d8-a3c6-5f29aee88615"},{"version":"CommandV1","origId":392,"guid":"58f595d8-81ba-4139-8aef-1e0fcff41a78","subtype":"command","commandType":"auto","position":4.5625,"command":"import org.apache.spark.HashPartitioner\n\nval partitioner = new HashPartitioner(2)\n\nval advertiserInfo = sqlContext.table(\"AdvertiserInfo\").rdd.map(row => (row(0).toString(), (row(1).toString(), row(2).toString()))).partitionBy(partitioner).cache()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark.HashPartitioner\npartitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@2\nadvertiserInfo: org.apache.spark.rdd.RDD[(String, (String, String))] = ShuffledRDD[87933] at start at &lt;console&gt;:223\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":"<div class=\"ansiout\">&lt;console&gt;:114: error: type mismatch;\n found   : org.apache.spark.HashPartitioner\n required: Int\n       val advertiserInfo = sqlContext.table(&quot;AdvertiserInfo&quot;).rdd.map(row =&gt; (row(0).toString(), (row(1).toString(), row(2).toString()))).repartition(partitioner).cache()\n                                                                                                                                                       ^\n</div>","startTime":1.438113931449E12,"submitTime":1.438113931331E12,"finishTime":1.438113932129E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"67b5da30-c837-42bf-95be-a35012a52ca0"},{"version":"CommandV1","origId":393,"guid":"6631719a-0073-4111-b574-544a68c9a142","subtype":"command","commandType":"auto","position":4.78125,"command":"%md #### The core function that is run by the streaming job\nIt does the following:\n\n* Receives 2 input streams - ad impressions and ad clicks\n* Calculates the impression and click counts over a window of 10 seconds\n* Joins the impression and click counts and computes the CTR for every ad over the 10 second window\n* Joins the calculated CTR information with an advertiser table to tag some advertiser information\n* Finally registers the CTR and the tagged information as a table so that a dashboard can be setup to query it periodically","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112185519E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"518599b5-83ce-44cd-a608-000f0f9d6e8e"},{"version":"CommandV1","origId":394,"guid":"60db45de-118c-42e6-95c8-0737a03e2a42","subtype":"command","commandType":"auto","position":5.0,"command":"val kafkaTopic1 = \"adclicks\"                                    \nval kafkaTopic2 = \"adviews\"\nval kafkaBrokers = \"YOUR-KAFKA-BROKER-1:PORT1,YOUR-KAFKA-BROKER-2:PORT2\"   // comma separated list of broker:host\n\nval windowSize = Duration(10000L)          // 10 seconds\nval slidingInterval = Duration(2000L)      // 2 seconds\nval checkpointInterval = Duration(40000L)  // 40 seconds\n\nval batchIntervalSeconds = 1\n\n// Function to create a new StreamingContext and set it up\ndef creatingFunc(): StreamingContext = {\n    \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n\n  // Get the input stream from the source\n  val clickStream = createKafkaStream(ssc, kafkaTopic1, kafkaBrokers)\n  val impressionStream = createKafkaStream(ssc, kafkaTopic2, kafkaBrokers)\n  \n  // Create a paired dstream\n  val clickPair = clickStream.map(event => {\n                      val fields = event._2.split(\",\")\n                      ((fields(0), fields(1)), 1)\n                  })\n  val impressionPair = impressionStream.map(event => {\n                      val fields = event._2.split(\",\")\n                      ((fields(0), fields(1)), 1)\n                      })\n\n  // Calculate the click and impression counts during the window\n  val clickCounts = clickPair.reduceByKeyAndWindow(\n                                        (x: Int, y: Int) => x+y, \n                                        (x: Int, y: Int) => x-y,                // Remove old counts\n                                        windowSize, slidingInterval, 2, \n                                        (x: ((String, String), Int)) => x._2 != 0)      // Filter all keys with zero counts\n  clickCounts.checkpoint(checkpointInterval)\n  \n  val impressionCounts = impressionPair.reduceByKeyAndWindow(\n                                        (x: Int, y: Int) => x+y, \n                                        (x: Int, y: Int) => x-y,                        // Remove old counts\n                                        windowSize, slidingInterval, 2, \n                                        (x: ((String, String), Int)) => x._2 != 0)      // Filter all keys with zero counts\n  impressionCounts.checkpoint(checkpointInterval)\n  \n  // Join the 2 streams and calculate the CTR for every ad campaign\n  val ctr = clickCounts.join(impressionCounts).map(adInfo => {\n                val adId = adInfo._1._1\n                val advertiserId = adInfo._1._2\n                val clicks = adInfo._2._1\n                val impressions = adInfo._2._2\n                val time = System.currentTimeMillis()\n                if( impressions != 0 ) {\n                  val ctr = (clicks.toFloat/impressions.toFloat) * 100.0\n                  (advertiserId, (adId, clicks, impressions, ctr, time))\n                } else \n                  (advertiserId, (adId, clicks, impressions, 0.0, time))\n            })\n\n  // Join the CTR stream with advertiser table to tag some advertiser info in the stream.\n  val ctrWithAdvertiserInfo = ctr.transform(rdd => advertiserInfo.join(rdd, partitioner).map(record => {\n    val advertiserId = record._1\n    val ctrTuple = record._2._2\n    val advertiserTuple = record._2._1\n    (ctrTuple._1, advertiserId, advertiserTuple._1, advertiserTuple._2, ctrTuple._2, ctrTuple._3, ctrTuple._4, ctrTuple._5)\n  }))\n  \n  // Register the final tagged CTR stream at every batch interval so that it can be queried separately\n  ctrWithAdvertiserInfo.window(Duration(30000)).foreachRDD { rdd => \n    val sqlContext = SQLContext.getOrCreate(SparkContext.getOrCreate())\n    sqlContext.createDataFrame(rdd).toDF(\"adId\", \"advertiserId\", \"advertiserName\", \"domain\", \"clicks\", \"impressions\", \"CTR\", \"Time\").registerTempTable(\"ctr\")   \n    rdd.take(1)\n  }\n  \n  // To make sure data is not deleted by the time we query it interactively\n  ssc.remember(Minutes(1))\n  \n  println(\"Creating function called to create new StreamingContext\")\n  ssc\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">kafkaTopic1: String = usage\nkafkaTopic2: String = webapp\nkafkaBrokers: String = 52.25.255.200:9092,52.25.255.200:9093\nwindowSize: org.apache.spark.streaming.Duration = 10000 ms\nslidingInterval: org.apache.spark.streaming.Duration = 2000 ms\ncheckpointInterval: org.apache.spark.streaming.Duration = 40000 ms\nbatchIntervalSeconds: Int = 1\ncreatingFunc: ()org.apache.spark.streaming.StreamingContext\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":"<div class=\"ansiout\">&lt;console&gt;:128: error: type mismatch;\n found   : org.apache.spark.rdd.RDD[(Any, org.apache.spark.sql.Row)]\n required: org.apache.spark.rdd.RDD[(String, ?)]\nNote: (Any, org.apache.spark.sql.Row) &gt;: (String, ?), but class RDD is invariant in type T.\nYou may wish to define T as -T instead. (SLS 4.5)\n         val ctrWithAdvertiserInfo = ctr.transform(rdd =&gt; rdd.join(advertiserInfo).map(record =&gt; {\n                                                                   ^\n</div>","startTime":1.438113939216E12,"submitTime":1.438113939093E12,"finishTime":1.438113940373E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"baf79a8d-f54b-40eb-8859-7d9a90e92fc0"},{"version":"CommandV1","origId":395,"guid":"1b7a6bef-9067-455e-ab39-56ed67c5f097","subtype":"command","commandType":"auto","position":5.5,"command":"%md #### Start the stream","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112211863E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"b1279918-28b9-4695-ad18-51404442b545"},{"version":"CommandV1","origId":396,"guid":"22fce619-0e4b-454b-82ef-2bc835e10d9d","subtype":"command","commandType":"auto","position":5.75,"command":"// Stop any existing StreamingContext \nval stopActiveContext = true\nif (stopActiveContext) {\t\n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n} \n\n// Get or create a streaming context.\nval ssc = StreamingContext.getActiveOrCreate(creatingFunc)\n\n// This starts the streaming context in the background. \nssc.start()\n\n// This is to ensure that we wait for some time before the background streaming job starts. This will put this cell on hold for 5 times the batchIntervalSeconds.\nssc.awaitTerminationOrTimeout(batchIntervalSeconds * 5 * 1000)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":1.438109054109E12,"submitTime":1.438109054109E12,"finishTime":1.438109057075E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"10154faf-ef53-430a-b51a-2c6fe4149a96"},{"version":"CommandV1","origId":397,"guid":"37ab860f-03ea-412d-93b8-14bd0f3a2500","subtype":"command","commandType":"auto","position":5.90625,"command":"%md #### Interactively query the CTR information","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112225895E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"293d79b5-a2f6-4eba-8707-5578f80ace56"},{"version":"CommandV1","origId":398,"guid":"fd2696e7-28b1-4489-b61f-bf4cc7539e42","subtype":"command","commandType":"auto","position":5.9375,"command":"%sql select * from ctr where adId = 5","commandVersion":0,"state":"finished","results":{"type":"table","data":[["5","5","T-Mobile","Wireless",1.0,94.0,1.0638297535479069,1.438113952743E12],["5","5","T-Mobile","Wireless",1.0,174.0,0.5747126415371895,1.438113954172E12],["5","5","T-Mobile","Wireless",3.0,260.0,1.1538461782038212,1.438113956169E12],["5","5","T-Mobile","Wireless",3.0,357.0,0.8403361774981022,1.438113958135E12],["5","5","T-Mobile","Wireless",3.0,482.0,0.6224066484719515,1.438113960179E12],["5","5","T-Mobile","Wireless",3.0,459.0,0.6535947788506746,1.438113962172E12],["5","5","T-Mobile","Wireless",5.0,461.0,1.0845987126231194,1.438113964131E12],["5","5","T-Mobile","Wireless",4.0,464.0,0.8620689623057842,1.43811396614E12],["5","5","T-Mobile","Wireless",6.0,467.0,1.2847965583205223,1.438113968178E12],["5","5","T-Mobile","Wireless",8.0,450.0,1.7777778208255768,1.438113970155E12],["5","5","T-Mobile","Wireless",7.0,482.0,1.4522821642458439,1.438113972164E12]],"arguments":{},"schema":[{"type":"string","name":"adId"},{"type":"string","name":"advertiserId"},{"type":"string","name":"advertiserName"},{"type":"string","name":"domain"},{"type":"int","name":"clicks"},{"type":"int","name":"impressions"},{"type":"double","name":"CTR"},{"type":"bigint","name":"Time"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":"Error in SQL statement: com.databricks.backend.daemon.driver.DriverLocal$SQLExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4991.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4991.0 (TID 3879, 10.0.188.66): java.io.FileNotFoundException: /user/hive/warehouse/advertiserinfo/part-r-00002-2cca2f03-4962-4279-8368-f1b81c32eef6.gz.parquet\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:63)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:40)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:187)\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:381)\n\tat parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:155)\n\tat parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:138)\n\tat org.apache.spark.sql.sources.SqlNewHadoopRDD$$anon$1.<init>(SqlNewHadoopRDD.scala:153)\n\tat org.apache.spark.sql.sources.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:124)\n\tat org.apache.spark.sql.sources.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:66)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:303)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:151)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:481)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:383)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:194)\n\tat java.lang.Thread.run(Thread.java:745)\n","startTime":1.438113972746E12,"submitTime":1.438113972362E12,"finishTime":1.438113972883E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"lineChart","width":"956","height":"271","xColumns":["Time"],"yColumns":["CTR"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"b24c5859-142b-4682-a876-9794f3232365"},{"version":"CommandV1","origId":399,"guid":"dd2b9396-d24c-4bce-a774-e57c551903b2","subtype":"script","commandType":"auto","position":6.0,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.dstream._\nimport kafka.serializer.StringDecoder\n\n// Verify that the attached Spark cluster is 1.4.0+\nrequire(sc.version.replace(\".\", \"\").toInt >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\")\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.dstream._\nimport kafka.serializer.StringDecoder\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438109054551E12,"submitTime":1.437508030773E12,"finishTime":1.438109055126E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["362ceb35-cc56-4df4-8af2-853c69bc18fb"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"ee66836f-5489-47d4-92fa-8fa547bb0269"},{"version":"CommandV1","origId":400,"guid":"be4d7fe4-4de1-4051-a83e-498f2ad73b2d","subtype":"command","commandType":"auto","position":6.0,"command":"%sql select advertiserId, advertiserName, Time, AVG(ctr) from ctr where domain = \"Wireless\" group by advertiserId, advertiserName, Time","commandVersion":0,"state":"finished","results":{"type":"table","data":[["3","AT&T",1.438113970154E12,6.055548042058945],["3","AT&T",1.438113952739E12,6.194690242409706],["3","AT&T",1.438113976155E12,6.879759367023196],["3","AT&T",1.438113970155E12,6.53851218521595],["3","AT&T",1.438113952737E12,6.889073674877484],["4","Verizon",1.438113974143E12,2.551276423037052],["3","AT&T",1.438113954172E12,6.9622959569096565],["3","AT&T",1.438113956168E12,6.884088180959225],["3","AT&T",1.438113956169E12,6.01479634642601],["5","T-Mobile",1.438113974144E12,1.3572950226565201],["4","Verizon",1.438113952743E12,2.9602693393826485],["3","AT&T",1.438113972164E12,7.442751775185267],["3","AT&T",1.43811396614E12,4.408914782106876],["5","T-Mobile",1.438113974145E12,1.2069205132623513],["4","Verizon",1.43811395274E12,3.883495181798935],["4","Verizon",1.438113976156E12,2.5590552017092705],["3","AT&T",1.438113972163E12,6.024734463010516],["3","AT&T",1.438113966139E12,6.085887877270579],["4","Verizon",1.438113970154E12,2.6398375164717436],["4","Verizon",1.438113970155E12,3.2036499430735907],["4","Verizon",1.438113952739E12,4.1018822230398655],["4","Verizon",1.438113976155E12,2.542902518891626],["4","Verizon",1.438113952737E12,1.9999999552965164],["5","T-Mobile",1.438113960178E12,1.141186859458685],["2","Comcast",1.438113952743E12,11.8665162473917],["5","T-Mobile",1.438113960179E12,1.2793744634836912],["2","Comcast",1.43811395274E12,13.789473474025726],["2","Comcast",1.438113976156E12,11.573070846498013],["4","Verizon",1.438113954172E12,3.0681105703115463],["2","Comcast",1.438113970154E12,12.106027733534575],["2","Comcast",1.438113976155E12,12.102591122190157],["2","Comcast",1.438113952739E12,12.207792326807976],["2","Comcast",1.438113970155E12,12.368717789649963],["2","Comcast",1.438113952737E12,13.330451771616936],["4","Verizon",1.438113956168E12,3.153213827560345],["4","Verizon",1.438113956169E12,2.761002304032445],["4","Verizon",1.43811396614E12,3.5309450700879097],["4","Verizon",1.438113972164E12,3.1682657077908516],["4","Verizon",1.438113966139E12,2.680378220975399],["4","Verizon",1.438113972163E12,2.5546329095959663],["3","AT&T",1.438113962172E12,6.088420711457729],["2","Comcast",1.438113954172E12,11.691432942946753],["2","Comcast",1.438113954173E12,13.02083283662796],["2","Comcast",1.438113956168E12,11.54797805680169],["2","Comcast",1.438113956169E12,14.0625],["2","Comcast",1.438113972164E12,12.77819573879242],["2","Comcast",1.43811396614E12,11.82527206838131],["2","Comcast",1.438113972163E12,12.056129518896341],["5","T-Mobile",1.438113974143E12,1.1320754885673523],["2","Comcast",1.438113966139E12,12.759169470518827],["4","Verizon",1.438113962172E12,3.089966749151548],["3","AT&T",1.438113968178E12,5.771589279174805],["5","T-Mobile",1.438113952743E12,2.0319148432463408],["5","T-Mobile",1.438113976156E12,1.6396816819906235],["5","T-Mobile",1.43811395274E12,1.0416666977107525],["4","Verizon",1.438113962171E12,3.094777651131153],["5","T-Mobile",1.438113970154E12,1.4545224939606018],["5","T-Mobile",1.438113976155E12,1.376981008797884],["5","T-Mobile",1.438113970155E12,1.9696051254868507],["5","T-Mobile",1.438113952739E12,0.9598887215058008],["5","T-Mobile",1.438113952737E12,0.9636666315297285],["2","Comcast",1.438113962172E12,12.173938304185867],["5","T-Mobile",1.438113954172E12,1.0515916103031486],["5","T-Mobile",1.438113954173E12,0.48664393834769726],["3","AT&T",1.438113958134E12,6.552235409617424],["3","AT&T",1.438113958135E12,6.537530571222305],["5","T-Mobile",1.438113956168E12,0.9486272552749142],["5","T-Mobile",1.438113956169E12,1.846764376387],["3","AT&T",1.438113958133E12,7.117512449622154],["3","AT&T",1.43811396413E12,5.847321610365595],["4","Verizon",1.438113968178E12,3.206407632678747],["3","AT&T",1.438113964131E12,5.803572138150533],["5","T-Mobile",1.43811396614E12,1.5945911097029846],["5","T-Mobile",1.438113972164E12,1.50524340569973],["5","T-Mobile",1.438113966139E12,1.4256447087973356],["5","T-Mobile",1.438113972163E12,1.4256248117557593],["2","Comcast",1.438113968178E12,12.350607067346573],["4","Verizon",1.438113958134E12,2.816273341886699],["4","Verizon",1.438113958135E12,3.2098766416311264],["4","Verizon",1.438113958133E12,3.1630169600248337],["4","Verizon",1.43811396413E12,3.1990309112838338],["4","Verizon",1.438113964131E12,3.1604300563534102],["5","T-Mobile",1.438113962172E12,1.3823608309030533],["3","AT&T",1.438113974144E12,6.663211435079575],["3","AT&T",1.438113974145E12,8.075268566608429],["2","Comcast",1.438113958134E12,13.230126649141312],["2","Comcast",1.438113958133E12,10.943788141012192],["2","Comcast",1.43811396413E12,12.3326962813735],["2","Comcast",1.438113964131E12,13.660810515284538],["3","AT&T",1.438113960178E12,6.746562644839287],["3","AT&T",1.438113960179E12,5.926673337817192],["4","Verizon",1.438113974144E12,2.3448199033737183],["5","T-Mobile",1.438113968178E12,1.5189284272491932],["4","Verizon",1.438113974145E12,2.7226161832610765],["4","Verizon",1.438113960178E12,3.04393470287323],["4","Verizon",1.438113960179E12,2.9003376673374857],["2","Comcast",1.438113974144E12,11.609487235546112],["2","Comcast",1.438113974145E12,12.741411477327347],["5","T-Mobile",1.438113958134E12,0.9790439903736115],["5","T-Mobile",1.438113958135E12,1.657791854813695],["5","T-Mobile",1.438113958133E12,1.2134996398041646],["5","T-Mobile",1.43811396413E12,2.0002514123916626],["5","T-Mobile",1.438113964131E12,1.3388290729684134],["2","Comcast",1.438113960178E12,13.486002571880817],["2","Comcast",1.438113960179E12,11.180434376001358],["3","AT&T",1.438113952743E12,6.771402309338252],["3","AT&T",1.438113976156E12,7.5364430745442705],["3","AT&T",1.43811395274E12,8.56270988782247]],"arguments":{},"schema":[{"type":"string","name":"advertiserId"},{"type":"string","name":"advertiserName"},{"type":"bigint","name":"Time"},{"type":"double","name":"_c3"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":"Error in SQL statement: com.databricks.backend.daemon.driver.DriverLocal$SQLExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4448.0 failed 4 times, most recent failure: Lost task 2.3 in stage 4448.0 (TID 3562, 10.0.188.66): java.io.FileNotFoundException: /user/hive/warehouse/advertiserinfo/part-r-00003-2cca2f03-4962-4279-8368-f1b81c32eef6.gz.parquet\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:63)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:40)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:187)\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:381)\n\tat parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:155)\n\tat parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:138)\n\tat org.apache.spark.sql.sources.SqlNewHadoopRDD$$anon$1.<init>(SqlNewHadoopRDD.scala:153)\n\tat org.apache.spark.sql.sources.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:124)\n\tat org.apache.spark.sql.sources.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:66)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:303)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:151)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:481)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:383)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:194)\n\tat java.lang.Thread.run(Thread.java:745)\n","startTime":1.438113976288E12,"submitTime":1.43811397613E12,"finishTime":1.438113977087E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"lineChart","width":"1007","height":"265","xColumns":["Time"],"yColumns":["_c3"],"pivotColumns":["advertiserName"],"pivotAggregation":"sum","customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"8a3c6ebf-8f67-4722-b977-a21be1c16b42"},{"version":"CommandV1","origId":401,"guid":"a78c6266-7bbe-4904-9295-faa3f1d00eb9","subtype":"script","commandType":"auto","position":6.25,"command":"/*\n  The function that starts a spark stream. It takes the following params:\n    * creatingFunc - the core function that does all the processing and returns a streaming context. This context is used only based on other params\n    * checkpointDir - checkpoint directory where the driver's write-ahead-logs are stored\n    * batchIntervalSeconds - batch interval in seconds\n    * stopActiveContext - if a streaming context is already running, then should they be stopped.\n    * stopGracefully - while stopping existing active streaming contexts, should we wait for all the processing in those streaming contexts to be completed before stopping.\n    * restartIfCheckpointExists - if set to false, the checkpoint directory is cleared everytime to ensure that a fresh stream is started everytime. In prod, you might want to set it to true to recover from previous failures.\n    * waitForContextTermination - if set to true, the execution waits until the streaming context is stopped or there is an exception. So ensure this is set to false while running from notebook so that the notebook that calls the startStream() doesn't get hung forever. While running as a job, this flag needs to be set \n*/\n\ndef startStream(creatingFunc: () => StreamingContext,\n                checkpointDir: String = \"dbfs:/streaming/checkpoint/100\",\n                batchIntervalSeconds: Int = 1,\n                stopActiveContext: Boolean = true,\n                stopGracefully: Boolean = false,\n                restartIfCheckpointExists: Boolean = false,\n                waitForContextTermination: Boolean = false) = {\n  // Stop any existing StreamingContext \n  if (stopActiveContext) {\t\n    StreamingContext.getActive.foreach { _.stop(stopSparkContext = false, stopGracefully = stopGracefully) }\n  } \n\n  // Delete checkpoint information to ensure context is not restarted\n  if (!restartIfCheckpointExists) {\n    dbutils.fs.rm(checkpointDir, true)\n  }  \n\n  // Get either the active context, or recover the context from checkpoint or create new one\n  val ssc = StreamingContext.getActiveOrCreate(checkpointDir, creatingFunc)\n  ssc.start()\n\n  // ssc.awaitTermination will terminate only if the context stopped from another notebook, or the context encounters an exception.\n  if (waitForContextTermination) {\n    ssc.awaitTermination()  \n  } else {\n    ssc.awaitTerminationOrTimeout(batchIntervalSeconds * 5 * 1000)\n  }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">startStream: (creatingFunc: () =&gt; org.apache.spark.streaming.StreamingContext, checkpointDir: String, batchIntervalSeconds: Int, stopActiveContext: Boolean, stopGracefully: Boolean, restartIfCheckpointExists: Boolean, waitForContextTermination: Boolean)AnyVal\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438109055131E12,"submitTime":1.437508030797E12,"finishTime":1.438109056417E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["362ceb35-cc56-4df4-8af2-853c69bc18fb"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"70a90126-9297-4bbc-a384-1ba5c6bf7c04"},{"version":"CommandV1","origId":402,"guid":"efe26d47-cc50-46ea-9e0c-322f0170c6bd","subtype":"command","commandType":"auto","position":6.25,"command":"%md #### Stop the stream","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112236093E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"3189e33d-d707-437f-b082-66c2b39c8284"},{"version":"CommandV1","origId":403,"guid":"c4ffe72b-b61b-49d8-b7ae-8b30b8661e65","subtype":"script","commandType":"auto","position":6.5,"command":"def stopAllStreams() = {\n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">stopAllStreams: ()Unit\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438109056421E12,"submitTime":1.437508030814E12,"finishTime":1.438109057017E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["362ceb35-cc56-4df4-8af2-853c69bc18fb"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"dd06f4a7-da73-467d-b71a-561ec42934c4"},{"version":"CommandV1","origId":404,"guid":"6cedd4d1-b9ba-422b-8a79-56b8351eb7ba","subtype":"command","commandType":"auto","position":6.5,"command":"//StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438113990063E12,"submitTime":1.438113989977E12,"finishTime":1.438113991298E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"228be60b-4db5-40ed-b9d2-e02afcedb706"}],"guid":"9ac1196a-9fca-4ba8-badf-2d752e156307","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"60831b3a-ebe4-49e1-988d-cb9d42694a9a","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>