<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Spark Streaming / Joining DStreams - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":213,"name":"Spark Streaming / Joining DStreams","language":"scala","commands":[{"version":"CommandV1","origId":214,"guid":"7121a9f4-4c37-4ffb-a196-91ce7f93a157","subtype":"command","commandType":"auto","position":0.5,"command":"%md ### Joining DStreams\n\nThis notebook demonstrates how to join two simple DStreams. The streaming job receives 2 input streams - ad clicks and ad impressions - from the [Kafka Ads Data Producer](../08 Spark Streaming/Streaming Producers/3 Kafka Ads Data Producer.html) (You can run that notebook to start pumping the data to kafka). The streaming job counts the ad clicks and impressions over a window and joins these 2 streams to compute the Click-Through Rate (CTR) of all the ads for every window. The job then registers the last 1 minute of data in a temp table with timestamp so that the CTR for ads can be visualized in a dashboard.\n\nImport the utility functions required for the example:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112794596E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"39759bde-f930-4f5c-9dc0-a9a1746750de"},{"version":"CommandV1","origId":215,"guid":"0f224959-4f97-4470-84ac-b65f1348e7df","subtype":"command","commandType":"auto","position":0.6875,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time}\nimport org.apache.spark.streaming.dstream._\nimport org.apache.spark.streaming.kafka._\nimport kafka.serializer.StringDecoder","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time}\nimport org.apache.spark.streaming.dstream._\nimport org.apache.spark.streaming.kafka._\nimport kafka.serializer.StringDecoder\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":"Command skipped","startTime":1.438112930323E12,"submitTime":1.43811293023E12,"finishTime":1.438112931221E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"b05348a5-b3f3-408e-b12f-469d82ddf9db"},{"version":"CommandV1","origId":216,"guid":"16a71ff4-75fa-498c-b384-44d3e2a49920","subtype":"script","commandType":"auto","position":1.2083333333333333,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.dstream._\n\nrequire(sc.version.replace(\".\", \"\").toInt >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.dstream._\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438112935071E12,"submitTime":1.437516036498E12,"finishTime":1.438112935472E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["da77c1f5-86bd-4e78-9afe-123c782e31ad"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d9ee8e1a-998b-4788-863e-b6ce505a01d6"},{"version":"CommandV1","origId":217,"guid":"beba19c8-c4c1-4c39-81e2-d04e7365010c","subtype":"script","commandType":"auto","position":1.5416666666666665,"command":"def createStreamingContext(\n                    batchIntervalSeconds: Int = 1, \n                    checkpointDir: String = \"dbfs:/streaming/checkpoint/100\", \n                    rememberSeconds: Int = 60): StreamingContext = {\n  \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n  \n  // To make sure data is not deleted by the time we query it interactively\n  ssc.remember(Seconds(rememberSeconds))       \n  \n  // For saving checkpoint info so that it can recover from failed clusters\n  ssc.checkpoint(checkpointDir)  \n  \n  ssc\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">createStreamingContext: (batchIntervalSeconds: Int, checkpointDir: String, rememberSeconds: Int)org.apache.spark.streaming.StreamingContext\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438112935476E12,"submitTime":1.437502226626E12,"finishTime":1.438112935803E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["da77c1f5-86bd-4e78-9afe-123c782e31ad"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"f7c4cae0-e28c-46e4-a7b4-b43d53581013"},{"version":"CommandV1","origId":218,"guid":"e792fbd5-0d80-4b05-88b0-d794daa11d62","subtype":"command","commandType":"auto","position":4.875,"command":"%md ** The core function that does all the streaming computation **\n* It computes the # of ad clicks and impressions over a window\n* It joins these 2 streams to compute the CTR for every window","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112938157E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"e8c8b7c0-79e2-4da6-956a-e1dfa033b727"},{"version":"CommandV1","origId":219,"guid":"c5f5c0b5-96eb-4490-b62e-a33f67e0d32d","subtype":"command","commandType":"auto","position":4.9375,"command":"def createKafkaStream(ssc: StreamingContext, kafkaTopics: String, brokers: String): DStream[(String, String)] = {\n  val topicsSet = kafkaTopics.split(\",\").toSet\n  val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n  KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n                                   ssc, kafkaParams, topicsSet)\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">createKafkaStream: (ssc: org.apache.spark.streaming.StreamingContext, kafkaTopics: String, brokers: String)org.apache.spark.streaming.dstream.DStream[(String, String)]\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438112940594E12,"submitTime":1.438112940513E12,"finishTime":1.438112940955E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"993d2d4a-5e80-4402-a44c-3efb3612179b"},{"version":"CommandV1","origId":220,"guid":"9b6b277b-6305-435b-a216-076f416db9c3","subtype":"command","commandType":"auto","position":5.0,"command":"val kafkaTopic1 = \"adclicks\"                                    \nval kafkaTopic2 = \"adviews\"\nval kafkaBrokers = \"YOUR-BROKER-HOST1:PORT1,YOUR-BROKER-HOST2:PORT2\"   // comma separated list of broker:host\n\nval windowSize = Duration(10000L)          // 10 seconds\nval slidingInterval = Duration(2000L)      // 2 seconds\nval checkpointInterval = Duration(40000L)  // 20 seconds\n\nval batchIntervalSeconds = 1\n\n// Function to create a new StreamingContext and set it up\ndef creatingFunc(): StreamingContext = {\n    \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n\n  // Get the input stream from the source\n  val clickStream = createKafkaStream(ssc, kafkaTopic1, kafkaBrokers).map(event => (event._2.split(\",\")(0), 1))\n  val impressionStream = createKafkaStream(ssc, kafkaTopic2, kafkaBrokers).map(event => (event._2.split(\",\")(0), 1))\n\n  // Calculate the click and impression counts during the window\n  val clickCounts = clickStream.reduceByKeyAndWindow(\n                                        (x: Int, y: Int) => x+y, \n                                        (x: Int, y: Int) => x-y,                // Remove old counts\n                                        windowSize, slidingInterval, 2, \n                                        (x: (String, Int)) => x._2 != 0)        // Filter all keys with zero counts\n  clickCounts.checkpoint(checkpointInterval)\n  \n  val impressionCounts = impressionStream.reduceByKeyAndWindow(\n                                        (x: Int, y: Int) => x+y, \n                                        (x: Int, y: Int) => x-y,                // Remove old counts\n                                        windowSize, slidingInterval, 2, \n                                        (x: (String, Int)) => x._2 != 0)        // Filter all keys with zero counts\n  impressionCounts.checkpoint(checkpointInterval)\n  \n  // Join the 2 streams and calculate the CTR for every ad campaign\n  val ctr = clickCounts.join(impressionCounts).map(adInfo => {\n                val adId = adInfo._1\n                val clicks = adInfo._2._1\n                val impressions = adInfo._2._2\n                val time = System.currentTimeMillis()\n                if( impressions != 0 ) {\n                  val ctr = (clicks.toFloat/impressions.toFloat) * 100.0\n                  (adId, clicks, impressions, ctr, time)\n                } else \n                  (adId, clicks, impressions, 0.0, time)\n            })\n  \n  // Register a temp table at every batch interval so that it can be queried separately\n  ctr.window(Duration(60000)).foreachRDD { rdd => \n    val sqlContext = SQLContext.getOrCreate(SparkContext.getOrCreate())\n    sqlContext.createDataFrame(rdd).toDF(\"adId\", \"clicks\", \"impressions\", \"CTR\", \"Time\").registerTempTable(\"ctr\")   \n    rdd.take(1)\n  }\n  \n  // To make sure data is not deleted by the time we query it interactively\n  ssc.remember(Minutes(1))\n  \n  println(\"Creating function called to create new StreamingContext\")\n  ssc\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">kafkaTopic1: String = usage\nkafkaTopic2: String = webapp\nkafkaBrokers: String = 52.25.255.200:9092,52.25.255.200:9093\nwindowSize: org.apache.spark.streaming.Duration = 10000 ms\nslidingInterval: org.apache.spark.streaming.Duration = 2000 ms\ncheckpointInterval: org.apache.spark.streaming.Duration = 40000 ms\nbatchIntervalSeconds: Int = 1\ncreatingFunc: ()org.apache.spark.streaming.StreamingContext\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":"<div class=\"ansiout\">&lt;console&gt;:93: error: not found: value createKafkaStream\n         val clickStream = createKafkaStream(ssc, kafkaTopic1, kafkaBrokers).map(event =&gt; (event._2.split(&quot;,&quot;)(0), 1))\n                           ^\n&lt;console&gt;:94: error: not found: value createKafkaStream\n         val impressionStream = createKafkaStream(ssc, kafkaTopic2, kafkaBrokers).map(event =&gt; (event._2.split(&quot;,&quot;)(0), 1))\n                                ^\n</div>","startTime":1.438112947766E12,"submitTime":1.438112947675E12,"finishTime":1.438112948371E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"eed8794e-0358-488f-95d5-3c8a1b76a70f"},{"version":"CommandV1","origId":221,"guid":"13b82647-ea30-4d90-ab43-1b81fe00ebff","subtype":"command","commandType":"auto","position":5.5,"command":"%md **Start the streaming application**","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112828759E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"432016ea-f4ab-479d-ae65-9480c6c2c1ef"},{"version":"CommandV1","origId":222,"guid":"b1ccf7ee-7b0f-4d41-83d2-ebafb67f6a5c","subtype":"command","commandType":"auto","position":5.875,"command":"// Stop any existing StreamingContext \nval stopActiveContext = true\nif (stopActiveContext) {\t\n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n} \n\n// Get or create a streaming context.\nval ssc = StreamingContext.getActiveOrCreate(creatingFunc)\n\n// This starts the streaming context in the background. \nssc.start()\n\n// This is to ensure that we wait for some time before the background streaming job starts. This will put this cell on hold for 5 times the batchIntervalSeconds.\nssc.awaitTerminationOrTimeout(batchIntervalSeconds * 5 * 1000)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":"The execution of this command did not finish successfully","startTime":1.438112967528E12,"submitTime":1.438112967528E12,"finishTime":1.4381129689E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"ff794eac-14a2-49d6-ab59-890feee8c5d5"},{"version":"CommandV1","origId":223,"guid":"04a2fc0d-1401-48d9-998c-a6426b88b7c0","subtype":"script","commandType":"auto","position":6.125,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.dstream._\nimport kafka.serializer.StringDecoder\n\n// Verify that the attached Spark cluster is 1.4.0+\nrequire(sc.version.replace(\".\", \"\").toInt >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\")\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.dstream._\nimport kafka.serializer.StringDecoder\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438112967969E12,"submitTime":1.437508030773E12,"finishTime":1.438112968133E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["1d618e52-847f-405c-97e9-d377cf47dfb2"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"ee66836f-5489-47d4-92fa-8fa547bb0269"},{"version":"CommandV1","origId":224,"guid":"82132b58-a754-42f9-8908-24f24f8960ff","subtype":"script","commandType":"auto","position":6.375,"command":"/*\n  The function that starts a spark stream. It takes the following params:\n    * creatingFunc - the core function that does all the processing and returns a streaming context. This context is used only based on other params\n    * checkpointDir - checkpoint directory where the driver's write-ahead-logs are stored\n    * batchIntervalSeconds - batch interval in seconds\n    * stopActiveContext - if a streaming context is already running, then should they be stopped.\n    * stopGracefully - while stopping existing active streaming contexts, should we wait for all the processing in those streaming contexts to be completed before stopping.\n    * restartIfCheckpointExists - if set to false, the checkpoint directory is cleared everytime to ensure that a fresh stream is started everytime. In prod, you might want to set it to true to recover from previous failures.\n    * waitForContextTermination - if set to true, the execution waits until the streaming context is stopped or there is an exception. So ensure this is set to false while running from notebook so that the notebook that calls the startStream() doesn't get hung forever. While running as a job, this flag needs to be set \n*/\n\ndef startStream(creatingFunc: () => StreamingContext,\n                checkpointDir: String = \"dbfs:/streaming/checkpoint/100\",\n                batchIntervalSeconds: Int = 1,\n                stopActiveContext: Boolean = true,\n                stopGracefully: Boolean = false,\n                restartIfCheckpointExists: Boolean = false,\n                waitForContextTermination: Boolean = false) = {\n  // Stop any existing StreamingContext \n  if (stopActiveContext) {\t\n    StreamingContext.getActive.foreach { _.stop(stopSparkContext = false, stopGracefully = stopGracefully) }\n  } \n\n  // Delete checkpoint information to ensure context is not restarted\n  if (!restartIfCheckpointExists) {\n    dbutils.fs.rm(checkpointDir, true)\n  }  \n\n  // Get either the active context, or recover the context from checkpoint or create new one\n  val ssc = StreamingContext.getActiveOrCreate(checkpointDir, creatingFunc)\n  ssc.start()\n\n  // ssc.awaitTermination will terminate only if the context stopped from another notebook, or the context encounters an exception.\n  if (waitForContextTermination) {\n    ssc.awaitTermination()  \n  } else {\n    ssc.awaitTerminationOrTimeout(batchIntervalSeconds * 5 * 1000)\n  }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">startStream: (creatingFunc: () =&gt; org.apache.spark.streaming.StreamingContext, checkpointDir: String, batchIntervalSeconds: Int, stopActiveContext: Boolean, stopGracefully: Boolean, restartIfCheckpointExists: Boolean, waitForContextTermination: Boolean)AnyVal\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438112968137E12,"submitTime":1.437508030797E12,"finishTime":1.438112968614E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["1d618e52-847f-405c-97e9-d377cf47dfb2"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"70a90126-9297-4bbc-a384-1ba5c6bf7c04"},{"version":"CommandV1","origId":225,"guid":"b4a646b6-d181-4ae8-9e77-95e15b81ee93","subtype":"script","commandType":"auto","position":6.625,"command":"def stopAllStreams() = {\n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">stopAllStreams: ()Unit\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438112968617E12,"submitTime":1.437508030814E12,"finishTime":1.43811296882E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["1d618e52-847f-405c-97e9-d377cf47dfb2"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"dd06f4a7-da73-467d-b71a-561ec42934c4"},{"version":"CommandV1","origId":226,"guid":"5ac40c00-151d-4469-9c2b-0f685f192f38","subtype":"command","commandType":"auto","position":7.0,"command":"%md ** Execute the below cell repeatedly to see how the CTR is changing over time for a given ad id.**","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112848587E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"c3ccff17-8e17-4739-b497-3d46b0233f52"},{"version":"CommandV1","origId":227,"guid":"bfbd6ab5-0436-4c1e-b2f5-8e4eb51b2531","subtype":"command","commandType":"auto","position":8.0,"command":"%sql select * from ctr where adId in (1, 5, 8)","commandVersion":0,"state":"finished","results":{"type":"table","data":[["8",1.0,471.0,0.2123142359778285,1.438113031607E12],["5",1.0,488.0,0.20491802133619785,1.438113031607E12],["1",102.0,472.0,21.610169112682343,1.438113031607E12],["8",1.0,487.0,0.2053388161584735,1.438113031654E12],["1",101.0,481.0,20.997920632362366,1.438113031654E12],["8",1.0,482.0,0.20746889058500528,1.438113039894E12],["5",3.0,486.0,0.6172839552164078,1.438113039894E12],["1",106.0,495.0,21.414141356945038,1.438113039894E12],["5",4.0,468.0,0.8547008968889713,1.438113039941E12],["1",112.0,493.0,22.718052566051483,1.438113039941E12],["5",5.0,485.0,1.0309278033673763,1.438113039989E12],["1",117.0,499.0,23.4468936920166,1.438113039989E12],["5",5.0,484.0,1.0330578312277794,1.438113040033E12],["1",116.0,489.0,23.721881210803986,1.438113040034E12],["5",5.0,471.0,1.0615711100399494,1.438113044461E12],["1",123.0,501.0,24.550898373126984,1.438113044462E12],["8",2.0,506.0,0.3952569328248501,1.438113044506E12],["5",2.0,485.0,0.41237114928662777,1.438113044506E12],["1",113.0,465.0,24.30107593536377,1.438113044506E12],["8",2.0,515.0,0.38834952283650637,1.43811304455E12],["5",4.0,511.0,0.7827788591384888,1.43811304455E12],["1",104.0,460.0,22.608695924282074,1.438113044551E12],["8",2.0,513.0,0.3898635506629944,1.438113053996E12],["5",5.0,498.0,1.004016026854515,1.438113053996E12],["1",109.0,458.0,23.799127340316772,1.438113053996E12],["8",2.0,493.0,0.4056795034557581,1.438113054041E12],["5",7.0,494.0,1.4170040376484394,1.438113054041E12],["1",116.0,464.0,25.0,1.438113054042E12],["8",3.0,466.0,0.643776822835207,1.438113054085E12],["5",11.0,503.0,2.1868787705898285,1.438113054085E12],["1",113.0,451.0,25.055432319641113,1.438113054086E12],["8",1.0,478.0,0.20920501556247473,1.438113054129E12],["5",11.0,501.0,2.195608802139759,1.438113054129E12],["1",113.0,470.0,24.042552709579468,1.43811305413E12],["8",1.0,472.0,0.21186440717428923,1.438113066125E12],["5",9.0,480.0,1.875000074505806,1.438113066125E12],["1",112.0,466.0,24.03433471918106,1.438113066125E12],["8",2.0,498.0,0.40160641074180603,1.438113066169E12],["5",11.0,475.0,2.3157894611358643,1.438113066169E12],["1",105.0,462.0,22.727273404598236,1.438113066169E12],["8",2.0,515.0,0.38834952283650637,1.438113066213E12],["5",11.0,483.0,2.277432754635811,1.438113066213E12],["1",95.0,466.0,20.386266708374023,1.438113066213E12],["8",1.0,520.0,0.1923076924867928,1.438113066257E12],["5",9.0,487.0,1.8480492755770683,1.438113066257E12],["1",99.0,458.0,21.61571979522705,1.438113066257E12],["8",1.0,483.0,0.20703934133052826,1.438113066301E12],["5",12.0,478.0,2.510460279881954,1.438113066301E12],["1",104.0,444.0,23.423422873020172,1.438113066301E12],["8",1.0,379.0,0.2638522535562515,1.438113066349E12],["5",11.0,389.0,2.827763557434082,1.438113066349E12],["1",88.0,363.0,24.242424964904785,1.438113066349E12],["5",7.0,297.0,2.3569023236632347,1.438113075745E12],["1",66.0,271.0,24.354243278503418,1.438113075745E12],["5",5.0,196.0,2.551020495593548,1.438113075789E12],["1",46.0,171.0,26.900583505630493,1.438113075789E12],["5",3.0,88.0,3.4090910106897354,1.438113075833E12],["1",21.0,81.0,25.925925374031067,1.438113075833E12]],"arguments":{},"schema":[{"type":"string","name":"adId"},{"type":"int","name":"clicks"},{"type":"int","name":"impressions"},{"type":"double","name":"CTR"},{"type":"bigint","name":"Time"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":"Cancelled","startTime":1.438113086889E12,"submitTime":1.438113086785E12,"finishTime":1.438113087281E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"lineChart","width":"1142","height":"309","xColumns":["Time"],"yColumns":["CTR"],"pivotColumns":["adId"],"pivotAggregation":"sum","customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d0f19dde-884b-4866-b0d2-f2836ec6bdf6"},{"version":"CommandV1","origId":228,"guid":"0a86a725-8ee4-40fe-a606-a2f68ac4228e","subtype":"command","commandType":"auto","position":8.5,"command":"%md ** Uncomment the below cell and execute it to stop the streaming context **","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438112867468E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"b5957799-6a10-4a0d-b1fd-cf082258b400"},{"version":"CommandV1","origId":229,"guid":"87a03fd7-424c-40a0-a483-554963f317b3","subtype":"command","commandType":"auto","position":9.0,"command":"//StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":"Command skipped","startTime":1.438113102145E12,"submitTime":1.43811310209E12,"finishTime":1.438113112389E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"e973350e-6fe7-46f8-8c3d-a18fd9119caf"}],"guid":"929402a1-47b7-4891-a923-8f3881c40edf","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"9da4e8ca-80b8-498a-a6c6-457797c192bf","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>