<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Spark Streaming / Window Aggregations - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":false,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-U094163cf51-S47b89c350f-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-U094163cf51-S2f95f6c227-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-U094163cf51-S3436f2ea50-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.8.1","local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-U094163cf51-S336f76a5be-2015-12-03-00:16:18.916275","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"d3dde7ae-fd45-4989-86b8-e91415a5ebcf","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"094163cf51fcd4717c3ea96799d1008723ae8985","userFullname":"Suresh Jayaram","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100017,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"surjayaram@paypal.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":468,"name":"Spark Streaming / Window Aggregations","language":"scala","commands":[{"version":"CommandV1","origId":469,"guid":"1e122bfd-8679-4c5e-b2c5-cd784c5a29ad","subtype":"command","commandType":"auto","position":1.0,"command":"%md ### Window Aggregations in Streaming\n\n  The notebook provides some examples of the different window aggregations available in Spark Streaming and when and how to use them. Some terminologies:\n  * Batch Interval - The interval for which a DStream is created. This is provided while creating the Streaming Context.\n  * Window Duration/Size - The window duration over which a certain fold operation needs to be performed. Should be a multiple of batch interval\n  * Sliding Interval - The interval over which the window should be slided. Should be a multiple of batch interval.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438060193612E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"762a6bbc-1929-4828-876e-1b7f32dfae5e"},{"version":"CommandV1","origId":470,"guid":"0a686b30-375c-49a3-954b-bd0cc6495210","subtype":"command","commandType":"auto","position":2.0,"command":"%md #### Associative Aggregation over a Window\n\nIn order to do a simple associative aggregation (like word count) over a window, you can use the following interface:\n\n`wordStream.reduceByKeyAndWindow((x: Int, y: Int) => x+y, \n                                    windowSize, \n                                    slidingInterval)`","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438063955671E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"35c4ca30-1a49-45e4-8a1a-db224195be33"},{"version":"CommandV1","origId":471,"guid":"75c6dab0-8b01-40e4-bf24-ec2b46593f25","subtype":"command","commandType":"auto","position":2.5,"command":"%md #### Incremental Associative Aggregation over a Window\n\nFor larger window sizes with small sliding intervals, computing the aggregation over every large window can become a bottleneck. To avoid these bottlenecks, the aggregation can be performed more efficiently by incrementally performing the aggregation from the previous windows. There are 2 requirements to do this:\n\n* **Inverse Reduce Function**: The reduce function should be injective (i.e.) the user should be able to provide a inverse reduce function to account for old data going out of window. Example for word count - the inverse reduce function would be to just subtract the counts.\n* **DStream Checkpointing**: Since the lineage for this stateful transformation can grow quickly, it *requires* checkpointing of the dstreams. The checkpointing interval can be configured. Frequent checkpointing can reduce the throughput of the streaming pipeline because lot of rdds need to be persisted. Infrequent checkpointing can cause the lineage to grow for every batch resulting in poor performance. A good rule of thumb would be to set the interval as 5-10x the sliding interval and adjust it from there based on the performance.\n\n```wordStream.reduceByKeyAndWindow((x: Int, y: Int) => x+y,\n                                   (x: Int, y: Int) => x-y, \n                                    windowSize, slidingInterval)```","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438063973384E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"30a1994a-364c-4d73-8ebf-9c4639e2c6d4"},{"version":"CommandV1","origId":472,"guid":"95032ac7-7b9a-4ee7-9a2b-c5c539c0e488","subtype":"command","commandType":"auto","position":4.0,"command":"%md #### Incremental Associative Aggregation with Pruning\n\nA major side effect of using the above incremental aggregation is that the key space can keep growing over time since the inverse reduce function helps only in updating the values but not removing the keys. For example, in the word count case, there are lot of words that might not appear in the last few windows and their counts will be 0 and there can be lot of such unnecessary words in the dstream over time degrading the performance of aggregation. In order to prune the state and keep only the relevant keys, a filter function can be provided to the reduceByKeyAndWindow function that will filter irrelevant keys.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438060193813E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"fcfc721c-6cbb-4c08-9b37-b131c20da0e0"},{"version":"CommandV1","origId":473,"guid":"8b42c739-059c-4085-93fe-0c71208ddff3","subtype":"command","commandType":"auto","position":4.5,"command":"%md The rest of this notebook provides an example where a word count is performed incrementally over previous windows and also the dstream is pruned to keep only relevant keys","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.43806019388E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"ed250914-9939-4e74-92a2-3cfcf5ebc6ba"},{"version":"CommandV1","origId":474,"guid":"49d54c09-3f53-4249-b210-b6c38e7843b1","subtype":"command","commandType":"auto","position":5.0,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time}\nimport org.apache.spark.streaming.dstream._\nimport org.apache.spark.streaming.kafka._\nimport kafka.serializer.StringDecoder","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time}\nimport org.apache.spark.streaming.dstream._\nimport org.apache.spark.streaming.kafka._\nimport kafka.serializer.StringDecoder\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438060195245E12,"submitTime":1.438060193944E12,"finishTime":1.438060195504E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d4313496-3a00-4663-aa75-94686e007082"},{"version":"CommandV1","origId":475,"guid":"a731b709-cbdb-444f-bd0c-55fda9ea60f7","subtype":"script","commandType":"auto","position":6.333333333333333,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.dstream._\n\nrequire(sc.version.replace(\".\", \"\").toInt >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.dstream._\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438060195508E12,"submitTime":1.437516036498E12,"finishTime":1.438060196419E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["e31765ae-f7b5-4450-95de-b26646c073e0"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d9ee8e1a-998b-4788-863e-b6ce505a01d6"},{"version":"CommandV1","origId":476,"guid":"1c06f376-e291-4b64-8792-5f8e88838ffb","subtype":"script","commandType":"auto","position":6.666666666666667,"command":"def createStreamingContext(\n                    batchIntervalSeconds: Int = 1, \n                    checkpointDir: String = \"dbfs:/streaming/checkpoint/100\", \n                    rememberSeconds: Int = 60): StreamingContext = {\n  \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n  \n  // To make sure data is not deleted by the time we query it interactively\n  ssc.remember(Seconds(rememberSeconds))       \n  \n  // For saving checkpoint info so that it can recover from failed clusters\n  ssc.checkpoint(checkpointDir)  \n  \n  ssc\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">createStreamingContext: (batchIntervalSeconds: Int, checkpointDir: String, rememberSeconds: Int)org.apache.spark.streaming.StreamingContext\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438060196423E12,"submitTime":1.437502226626E12,"finishTime":1.438060197314E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["e31765ae-f7b5-4450-95de-b26646c073e0"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"f7c4cae0-e28c-46e4-a7b4-b43d53581013"},{"version":"CommandV1","origId":477,"guid":"7e6299a0-63f6-40da-ae74-39e8619e773f","subtype":"command","commandType":"auto","position":7.0,"command":"val kafkaTopics = \"YOUR_KAFKA_TOPIC_1,YOUR_KAFKA_TOPIC_2\"    // command separated list of topics\nval kafkaBrokers = \"YOUR_KAFKA_BROKER_1:HOST1,YOUR_KAFKA_BROKER_2:HOST2\"   // comma separated list of broker:host\n\ndef createKafkaStream(ssc: StreamingContext): DStream[(String, String)] = {\n  val topicsSet = kafkaTopics.split(\",\").toSet\n  val kafkaParams = Map[String, String](\"metadata.broker.list\" -> kafkaBrokers)\n  KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n                                   ssc, kafkaParams, topicsSet)\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">kafkaTopics: String = webapp,usage\nkafkaBrokers: String = 52.25.255.200:9092,52.25.255.200:9093\ncreateKafkaStream: (ssc: org.apache.spark.streaming.StreamingContext)org.apache.spark.streaming.dstream.DStream[(String, String)]\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438060197318E12,"submitTime":1.438060194349E12,"finishTime":1.438060197794E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"696e6088-6a6b-455b-bb58-5459e639c2ef"},{"version":"CommandV1","origId":478,"guid":"ec1bbcf1-69af-4dd9-9deb-9b7a1494ff2a","subtype":"command","commandType":"auto","position":7.5,"command":"%md The core function that's run by the streaming job. It does the following key things:\n* Uses the reduceByKeyAndWindow to incrementally do the association with pruning\n* Checkpoints the RDD\n* Triggers a dummy action with take(1) to ensure that the rdds are actually checkpointed at periodic intervals","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438062894173E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"8da36f7a-e53c-4695-89b5-78dea3fd9803"},{"version":"CommandV1","origId":479,"guid":"f0f21cd5-4423-4ebb-938a-7b35721d5b3f","subtype":"command","commandType":"auto","position":8.0,"command":"val windowSize = Duration(10000L)          // 10 seconds\nval slidingInterval = Duration(2000L)      // 2 seconds\nval checkpointInterval = Duration(20000L)  // 20 seconds\n\nval checkpointDir = \"dbfs://streaming/checkpoint/100\"\n\n// Function to create a new StreamingContext and set it up\ndef creatingFunc(): StreamingContext = {\n    \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n\n  // Get the word stream from the source\n  val wordStream = createKafkaStream(ssc).flatMap { event => event._2.split(\" \") }.map(x => (x, 1))\n  \n  val runningCountStream = wordStream.reduceByKeyAndWindow(\n                                        (x: Int, y: Int) => x+y, \n                                        (x: Int, y: Int) => x-y,                // Subtract counts going out of window\n                                        windowSize, slidingInterval, 2, \n                                        (x: (String, Int)) => x._2 != 0)        // Filter all keys with zero counts\n  \n  // Checkpoint the dstream so that it can be persisted every 20 seconds. If the stream is not checkpointed, the performance will deteriorate significantly over time and eventually crash.\n  runningCountStream.checkpoint(checkpointInterval)\n  \n  // Create temp table at every batch interval\n  runningCountStream.foreachRDD { rdd => \n    val sqlContext = SQLContext.getOrCreate(SparkContext.getOrCreate())\n    sqlContext.createDataFrame(rdd).toDF(\"word\", \"count\").registerTempTable(\"batch_word_count\")\n    \n    /* Trigger a dummy action to execute the DAG. This triggering of action will ensure that \n       the checkpointing is invoked. If there is no action on the DAG, then checkpointing will not \n       be invoked and if somebody queries the table after 'n' minutes, it will result in processing \n       a big lineage of rdds.\n    */\n    rdd.take(1)\n  }\n  \n  // To make sure data is not deleted by the time we query it interactively\n  ssc.remember(Minutes(1))\n  \n  ssc.checkpoint(checkpointDir)\n  \n  println(\"Creating function called to create new StreamingContext\")\n  ssc\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">windowSize: org.apache.spark.streaming.Duration = 10000 ms\nslidingInterval: org.apache.spark.streaming.Duration = 2000 ms\ncheckpointInterval: org.apache.spark.streaming.Duration = 20000 ms\ncreatingFunc: ()org.apache.spark.streaming.StreamingContext\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":"<div class=\"ansiout\">&lt;console&gt;:179: error: value take is not a member of org.apache.spark.streaming.dstream.DStream[(String, Int)]\n         runningCountStream.take(1)\n                            ^\n</div>","startTime":1.438062531923E12,"submitTime":1.438062531846E12,"finishTime":1.438062532615E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"fd4e29c7-f1b1-41f2-8ea5-58070e7e971a"},{"version":"CommandV1","origId":480,"guid":"a25836cf-7b34-4773-bfc3-6e2939bae354","subtype":"command","commandType":"auto","position":8.5,"command":"%md #### Start the Streaming job","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438064004611E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d71a7284-1d39-41ab-820b-f1a6a06c5d49"},{"version":"CommandV1","origId":481,"guid":"0c1b2b66-82ec-4b9c-befe-d4549b776d45","subtype":"command","commandType":"auto","position":9.0,"command":"// Stop any existing StreamingContext \nval stopActiveContext = true\nif (stopActiveContext) {\t\n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n} \n\n// Get or create a streaming context.\nval ssc = StreamingContext.getActiveOrCreate(creatingFunc)\n\n// This starts the streaming context in the background. \nssc.start()\n\n// This is to ensure that we wait for some time before the background streaming job starts. This will put this cell on hold for 5 times the batchIntervalSeconds.\nssc.awaitTerminationOrTimeout(batchIntervalSeconds * 5 * 1000)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":1.438060194465E12,"submitTime":1.438060194465E12,"finishTime":1.438060200705E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"46ca4eaa-8b06-4359-a6a9-02e6288bf9a6"},{"version":"CommandV1","origId":482,"guid":"38d96ef1-684e-4cfd-a74e-3dde847b500f","subtype":"script","commandType":"auto","position":9.25,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.dstream._\nimport kafka.serializer.StringDecoder\n\n// Verify that the attached Spark cluster is 1.4.0+\nrequire(sc.version.replace(\".\", \"\").toInt >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\")\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.nio.ByteBuffer\nimport scala.util.Random\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time, Duration}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.dstream._\nimport kafka.serializer.StringDecoder\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438060198482E12,"submitTime":1.437508030773E12,"finishTime":1.438060198922E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["d4572f18-278f-4956-b39a-ec089cea1d3b"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"ee66836f-5489-47d4-92fa-8fa547bb0269"},{"version":"CommandV1","origId":483,"guid":"a5337c63-d86a-47f9-a188-753da0f373ec","subtype":"script","commandType":"auto","position":9.5,"command":"/*\n  The function that starts a spark stream. It takes the following params:\n    * creatingFunc - the core function that does all the processing and returns a streaming context. This context is used only based on other params\n    * checkpointDir - checkpoint directory where the driver's write-ahead-logs are stored\n    * batchIntervalSeconds - batch interval in seconds\n    * stopActiveContext - if a streaming context is already running, then should they be stopped.\n    * stopGracefully - while stopping existing active streaming contexts, should we wait for all the processing in those streaming contexts to be completed before stopping.\n    * restartIfCheckpointExists - if set to false, the checkpoint directory is cleared everytime to ensure that a fresh stream is started everytime. In prod, you might want to set it to true to recover from previous failures.\n    * waitForContextTermination - if set to true, the execution waits until the streaming context is stopped or there is an exception. So ensure this is set to false while running from notebook so that the notebook that calls the startStream() doesn't get hung forever. While running as a job, this flag needs to be set \n*/\n\ndef startStream(creatingFunc: () => StreamingContext,\n                checkpointDir: String = \"dbfs:/streaming/checkpoint/100\",\n                batchIntervalSeconds: Int = 1,\n                stopActiveContext: Boolean = true,\n                stopGracefully: Boolean = false,\n                restartIfCheckpointExists: Boolean = false,\n                waitForContextTermination: Boolean = false) = {\n  // Stop any existing StreamingContext \n  if (stopActiveContext) {\t\n    StreamingContext.getActive.foreach { _.stop(stopSparkContext = false, stopGracefully = stopGracefully) }\n  } \n\n  // Delete checkpoint information to ensure context is not restarted\n  if (!restartIfCheckpointExists) {\n    dbutils.fs.rm(checkpointDir, true)\n  }  \n\n  // Get either the active context, or recover the context from checkpoint or create new one\n  val ssc = StreamingContext.getActiveOrCreate(checkpointDir, creatingFunc)\n  ssc.start()\n\n  // ssc.awaitTermination will terminate only if the context stopped from another notebook, or the context encounters an exception.\n  if (waitForContextTermination) {\n    ssc.awaitTermination()  \n  } else {\n    ssc.awaitTerminationOrTimeout(batchIntervalSeconds * 5 * 1000)\n  }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">startStream: (creatingFunc: () =&gt; org.apache.spark.streaming.StreamingContext, checkpointDir: String, batchIntervalSeconds: Int, stopActiveContext: Boolean, stopGracefully: Boolean, restartIfCheckpointExists: Boolean, waitForContextTermination: Boolean)AnyVal\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438060198926E12,"submitTime":1.437508030797E12,"finishTime":1.43806019995E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["d4572f18-278f-4956-b39a-ec089cea1d3b"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"70a90126-9297-4bbc-a384-1ba5c6bf7c04"},{"version":"CommandV1","origId":484,"guid":"68119ceb-c129-4b9a-b8a4-2302c82920c1","subtype":"script","commandType":"auto","position":9.75,"command":"def stopAllStreams() = {\n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">stopAllStreams: ()Unit\n</div>","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":1.438060199954E12,"submitTime":1.437508030814E12,"finishTime":1.43806020044E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["d4572f18-278f-4956-b39a-ec089cea1d3b"],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"dd06f4a7-da73-467d-b71a-561ec42934c4"},{"version":"CommandV1","origId":485,"guid":"f2b9400b-458b-4e48-b2af-1aa3a5f2a561","subtype":"command","commandType":"auto","position":11.0,"command":"%md #### Interactive Querying\n\nNow let's try querying the table. You can run this command again and again, you will find the numbers changing while data is being sent.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438060902391E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"5f3f7214-21a5-4e7c-b01d-37a648286281"},{"version":"CommandV1","origId":486,"guid":"169e3e03-e8ab-40c9-aab8-7cb6548e933b","subtype":"command","commandType":"auto","position":12.0,"command":"%sql select * from batch_word_count","commandVersion":0,"state":"finished","results":{"type":"table","data":[],"arguments":{},"schema":[{"type":"string","name":"word"},{"type":"int","name":"count"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":false},"errorSummary":null,"error":"Error in SQL statement: com.databricks.backend.daemon.driver.DriverLocal$SQLExecutionException: org.apache.spark.sql.AnalysisException: no such table batch_word_count; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:229)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:221)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:227)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:212)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:229)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:61)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:59)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:59)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:51)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:51)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:938)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:938)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:936)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:755)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4.apply(DriverLocal.scala:306)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4.apply(DriverLocal.scala:286)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:286)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:160)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:481)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:383)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:194)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:320)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:160)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:481)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:383)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:194)\n\tat java.lang.Thread.run(Thread.java:745)\n","startTime":1.438061902974E12,"submitTime":1.438061902859E12,"finishTime":1.438061903036E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"e84e912d-de24-4d30-82ac-b5a22bde48da"},{"version":"CommandV1","origId":487,"guid":"844fa8ea-6b20-4c43-9a22-e132d02196f0","subtype":"command","commandType":"auto","position":13.0,"command":"%md #### Finally, if you want stop the StreamingContext, you can uncomment and execute the following\n\n`StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }`","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.438060195007E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"d5c9a11d-c23e-496a-8cf0-14c7c48d97e8"}],"guid":"2cce0b51-a1cc-4952-a228-b90929d6638d","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"0a12f703-6ef4-4590-95a1-24816537c1bb","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512022229240000-094163cf51fcd4717c3ea96799d1008723ae8985/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>